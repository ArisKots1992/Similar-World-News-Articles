<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<site name="sciencedaily">
    <Article id="0">
        <date>Fri May 01 15:17:44 EEST 2015</date>
        <title>Heritage destruction in conflict zones provides archaeological opportunities</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/OF0iMDVK20c/150501081744.htm</Link>
        <Description>An international archaeological team is investigating an historic site devastated by conflict in Lebanon. They have demonstrated it is possible to obtain original and important information from heritage sites that have been devastated by conflict.</Description>
        <MainBody>Heritage destruction in conflict zones provides archaeological opportunities
Date:
University of Leicester
Summary:
An international archaeological team is investigating an historic site devastated by conflict in Lebanon. They have demonstrated it is possible to obtain original and important information from heritage sites that have been devastated by conflict.
Share:
Total shares: 
FULL STORY
The standing remains of the large 2nd century CE Graeco-Roman temple at Hosn Niha, unchanged since first recorded in the early 19th century CE. The walls still stand to a height of 10 metres.
Credit: Image courtesy of University of Leicester
The standing remains of the large 2nd century CE Graeco-Roman temple at Hosn Niha, unchanged since first recorded in the early 19th century CE. The walls still stand to a height of 10 metres.
Credit: Image courtesy of University of Leicester
Close
An international archaeological team is investigating an historic site devastated by conflict in Lebanon. They have demonstrated it is possible to obtain original and important information from heritage sites that have been devastated by conflict.
Working at the Graeco-Roman temple and village site of Hosn Niha, high in the central Biqa' Valley of Lebanon, the team led by Dr Paul Newson (Department of History and Archaeology, American University of Beirut) and Dr Ruth Young (School of Archaeology and Ancient History, University of Leicester) have described the value of exploring conflict damaged sites in the archaeological journal Antiquity.
Dr Newson said: "Shocking recent footage showing apparent damage to world heritage and archaeological sites at Hatra and Nimrud in Iraq include scenes of the bulldozing of irreplaceable buildings. Aerial photographs of living ancient cities such as Homs and Aleppo in Syria taken before the war have been compared to images from the last few months, and the extent of damage to houses, mosques, and heritage structures is brutal and widespread.
"Of course the human cost in any conflict is the first and highest priority; however, archaeology and heritage are extremely vulnerable to attack and damage during conflict and conflict continues to inflict damage on numerous sites, both large and small, around the world today."
Dr Young added: "Rather than simply ignoring sites that have been badly damaged by conflict, we have taken on the challenge of investigating a site previously considered too badly damaged by conflict to warrant systematic archaeological investigation.
"Our research at the Graeco-Roman temple and village site of Hosn Niha in Lebanon has shown that with the right methods and questions, it is possible to obtain a great deal of original and important information from sites that have suffered badly through conflict.
"Using a range of up-to-date surface survey methods we were able to answer some important questions about the site. The first of these was an accurate assessment of site damage, what had been done and where, and the effects of various actions, be it bulldozing or clandestine looting of the site. Through this exercise, we learned that bulldozing and other damage actions had effectively erased the heart of the settlement, but significantly sized sections of settlement beyond remained quite well preserved. From recording and collecting surface finds from across the settlement area as a whole we were able to begin to understand both the morphology and development history of the settlement."
The authors suggest the settlement was firmly established by the 1st century CE with a dense core area and more dispersed courtyard dwellings on the periphery. By the early Islamic period the settlement appears less robust and permanent occupation may have ended for a time. Surprisingly, they also recovered some evidence for an early medieval re-occupation of the site, perhaps a fortified farmhouse. They acknowledge the initial results are preliminary and that more research and analysis of the results is on-going.
Hosn Niha, along with many other sites in Lebanon was severely damaged as a consequence of decades of civil war and the associated unruliness and accelerated looting that went with this.
The authors state: "Sites that have been badly damaged by various causes may be disregarded by professionals who consider that their archaeological or heritage potential has been too badly affected to warrant any investigation. Instead, as demonstrated by the Hosn Niha project, the opposite should become automatic: archaeologists should view conflict-damaged sites as opportunities to gain information and explore sites and regions with new agendas.
"Conflict is impacting the lives of many millions of people, and the archaeology and heritage of many nations. All conflict-damaged archaeology and heritage can play a vital role as resources to help re-build damaged communities and offer hope of employment and reintegration to those impacted by war. Being able to offer ways of thinking of how to deal with damaged sites, gain as much information from them, and consider them a valuable resource rather than an inevitable casualty of war is critical to moving forward, and regaining control over land and identity."
The Central Biqa' Archaeological Project is based at the American University of Beirut, Lebanon (AUB). The project has been supported by the Department of Antiquities, Lebanon and the University of Leicester, and is funded by the American University of Beirut through its University Research Board (URB).
Story Source:
The above story is based on materials provided by University of Leicester . Note: Materials may be edited for content and length.
Journal Reference:
Paul Newson, Ruth Young. The archaeology of conflict-damaged sites: Hosn Niha in the Biqa Valley, Lebanon. Antiquity, 2015; 89 (344): 449 DOI: 10.15184/aqy.2015.4
Cite This Page:
</MainBody>
    </Article>
    <Article id="1">
        <date>Fri May 01 15:17:42 EEST 2015</date>
        <title>Dissolvable surgical clip, 5 mm in size, made of a magnesium alloy</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/EKK7swe7BFc/150501081742.htm</Link>
        <Description>A safe surgical clip that dissolves, which is absorbed by the body after a certain period of time, has been created by researchers. Clinical use of this clip is expected because it can reduce the rate of postoperative complications and minimize problems associated with diagnostic imaging.</Description>
        <MainBody>Dissolvable surgical clip, 5 mm in size, made of a magnesium alloy
Date:
Kobe University
Summary:
A safe surgical clip that dissolves, which is absorbed by the body after a certain period of time, has been created by researchers. Clinical use of this clip is expected because it can reduce the rate of postoperative complications and minimize problems associated with diagnostic imaging.
Share:
Credit: Image courtesy of Kobe University
The surgical clip.
Credit: Image courtesy of Kobe University
Close
Kobe University has developed a safe surgical clip that dissolves and be absorbed by the body after a certain period of time. Clinical use of this clip is expected because it can reduce the rate of postoperative complications and minimize problems associated with diagnostic imaging.
The clip was developed as a collaboration between the Division of Mechanics and Physics of Materials at the Kobe University Graduate School of Engineering and the Division of Hepato-Biliary-Pancreatic Surgery at the Kobe University Graduate School of Medicine. Most surgical clips are currently made of titanium, and as many as 30 to 40 clips may be used during a single surgical procedure. They remain inside the patient's body after the wounds are healed. Retained clips lead to diminished quality of CT (computed tomography) and MRI (magnetic resonance imaging) images around the wound and may cause complications. The newly developed clip is 5 mm in size and made of a magnesium alloy. The alloy also contains calcium and zinc to improve its microstructure, ensuring fastening ability and formability, qualities required of materials to make clips.
The safety and functionality of the clip were evaluated in vivo studies. To evaluate the safety, an implantation study was conducted in a subcutaneous mouse model. Very little gas was produced as the clip dissolved and there was no inflammation of the surrounding tissues after 1 to 12 weeks. These results suggest that the clip is associated with very few adverse effects. Blood testing revealed that levels of magnesium and other substances in the blood were in the normal range after 12 weeks. The volume of the implanted clip was reduced by almost half after 12 weeks. Therefore, the clip is likely to dissolve and exit the body within 1 year.
To evaluate its functionality, it was tested in a rat model in which the biliary duct, portal vein, hepatic artery, and hepatic vein were occluded with the clip and a partial liver was removed. The rat had no problems during a monitoring period of 8 weeks, suggesting that the clip functioned properly. Micro CT scanning of the mouse and rat revealed that the quality of images was not degraded and organs can be observed.
Professor MUKAI Toshiji (Department of Mechanical Engineering, Graduate School of Engineering), who was involved in developing the clip, expressed his hopes: "We will conduct further in vivo studies and a clinical study within 2 to 3 years. Kobe University works toward the development of new medical devices. We will continue to promote collaboration between the Graduate Schools of Medicine and Engineering."
Story Source:
The above story is based on materials provided by Kobe University . Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="2">
        <date>Fri May 01 15:17:05 EEST 2015</date>
        <title>Surgery for terminal cancer patients still common</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/KMqRtB6M-l4/150501081705.htm</Link>
        <Description>The number of surgeries performed on terminally ill cancer patients has not dropped in recent years, despite more attention to the importance of less invasive care for these patients to relieve symptoms and improve quality of life.</Description>
        <MainBody>Surgery for terminal cancer patients still common
Date:
UC Davis Comprehensive Cancer Center
Summary:
The number of surgeries performed on terminally ill cancer patients has not dropped in recent years, despite more attention to the importance of less invasive care for these patients to relieve symptoms and improve quality of life.
Share:
Total shares: 
FULL STORY
The number of surgeries performed on terminally ill cancer patients has not dropped in recent years , despite more attention to the importance of less invasive care for these patients to relieve symptoms and improve quality of life.
But new research from UC Davis also finds that the morbidity and mortality among patients with terminal cancer has declined because surgeons are selecting to operate on healthier patients.
The study, "Current Perioperative Outcomes for Patients with Disseminated Care Undergoing Surgery" was published online in the Journal of Surgical Research.
"Surgeons are becoming wiser," said study lead author Sarah Bateni, a UC Davis resident surgeon. "Our research suggests that surgeons may be operating on healthier patients who are more likely to recover well from an operation. These are patients who can perform activities of daily living without assistance, for example."
Bateni's interest in the appropriate surgical care of people with late-stage cancer grew from observing terminally ill patients whose acute problems were addressed through surgery, and who then suffered complications resulting in lengthy stays in intensive care units, and even in death.
"It is common that patients end up dying in the intensive care unit instead of being managed with medication with hopes of returning home with their families, including with hospice care," she said.
For the study, Bateni used the American College of Surgeons National Surgical Quality Improvement Program between 2006 and 2010 to identify 21,755 patients with stage IV cancer, meaning that the disease had metastasized, or spread, beyond the primary tumor site.
Over the five years in the study period, surgical interventions declined just slightly, from 1.9 percent to 1.6 percent of all procedures. The most frequent operations were surgeries to alleviate bowel obstructions among cancer patients with metastatic disease.
Also over time, the patients undergoing surgery were more independent and fewer had experienced dramatic weight loss or sepsis, a serious blood infection. These characteristics are generally associated with poorer surgical outcomes.
The patients' rate of morbidity, a measure of illness, significantly decreased, from 33.7 percent in 2006 to 26.6 percent in 2010. Mortality declined as well, although more modestly, from 10. 4 percent to 9.3 percent over the study period.
Why surgeons continue to operate on patients at such high risk for complications and death is due to several factors, Bateni said.
"Some of it has to do with the patients and families," she said. "If the patient is uncomfortable, the family wants a solution. In some cases, the surgeon also may be too optimistic about what the surgical outcome will be."
What Bateni also found was that just 3 percent of the patients with terminal cancer had Do Not Resuscitate (DNR) directives in place at the time of their surgery. DNRs, part of advanced directives used in end-of-life planning, direct physicians to withhold advanced life support if the patient stops breathing or their heart stops beating.
Bateni said the study results imply that patients, families and care providers, including surgeons, are often delaying discussions about the goals of the care and the priorities at the end of life.
She cautioned that delaying end-of-life discussions can have serious consequences because it can lead to delayed referrals for palliative care and hospice. In addition, the patient risks undergoing multiple invasive, uncomfortable procedures in an attempt to prolong life, despite being against the patient's goals of care and how they wish to spend their final days of life.
"It's really important that the doctor has an end-of-life, goals-of-care discussion prior to the time that the patient comes into the hospital with an acute illness," she said. "Patients should be referred to a palliative care counselor or have a comprehensive end-of-life discussion to ensure that their goals are respected as soon as they are diagnosed with cancer, especially those with cancers that have a high mortality rate."
Story Source:
The above story is based on materials provided by UC Davis Comprehensive Cancer Center . Note: Materials may be edited for content and length.
Journal Reference:
Sarah B. Bateni, Frederick J. Meyers, Richard J. Bold, Robert J. Canter. Current perioperative outcomes for patients with disseminated cancer. Journal of Surgical Research, 2015; DOI: 10.1016/j.jss.2015.03.063
Cite This Page:
</MainBody>
    </Article>
    <Article id="3">
        <date>Fri May 01 05:57:03 EEST 2015</date>
        <title>Regions at greatest risk for species extinction the least studied</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/oOl_J_r1hSY/150430225703.htm</Link>
        <Description>Scientists have crunched the numbers and the results are clear. For every degree that global temperatures rise, more species will become extinct. Overall, the study predicts a nearly 3 percent species extinction rate based on current conditions. If the earth warms another 3°C, the extinction risk rises to 8.5 percent. And if climate change continues on that trajectory, the world would experience a 4.3°C rise in temperature by the year 2100 -- meaning a 16 percent extinction rate.</Description>
        <MainBody>Regions at greatest risk for species extinction the least studied
Date:
University of Connecticut
Summary:
Scientists have crunched the numbers and the results are clear. For every degree that global temperatures rise, more species will become extinct. Overall, the study predicts a nearly 3 percent species extinction rate based on current conditions. If the earth warms another 3C, the extinction risk rises to 8.5 percent. And if climate change continues on that trajectory, the world would experience a 4.3C rise in temperature by the year 2100 -- meaning a 16 percent extinction rate.
Share:
Total shares: 
FULL STORY
Emperor penguins, a species native to Anarctica, are being increasingly affected by habitat loss. Global warming not only reduces the amount of pack ice surrounding the continent but also causes it to melt earlier in the year.
Credit: Michael Van Woert, NOAA NESDIS, ORA
Emperor penguins, a species native to Anarctica, are being increasingly affected by habitat loss. Global warming not only reduces the amount of pack ice surrounding the continent but also causes it to melt earlier in the year.
Credit: Michael Van Woert, NOAA NESDIS, ORA
Close
Mark Urban has crunched the numbers and the results are clear. For every degree that global temperatures rise, more species will become extinct.
And the risk of species loss is most acute for those continents that have unique climate ranges, with native species that can survive only in a limited range. Yet those regions are the least studied.
In a meta-analysis based on data from previously published studies, Urban, a UConn professor of ecology and evolutionary biology, reports that rises in future global temperatures will threaten up to one in six species if current climate policies are not modified.
His study, titled "Accelerating Extinction Risk from Climate Change," appears in the May 1 issue of Science.
While there have been numerous studies on how individual species may be affected by climate change, Urban's research is the first to take a holistic approach.
"We can look across all the studies and use the wisdom of many scientists," Urban says. "When we put it all together we can account for the uncertainty in each approach, and look for common patterns and understand how the moderators in each type of study affect outcomes."
Overall, the study predicts a nearly 3 percent species extinction rate based on current conditions. If the earth warms another 3C, the extinction risk rises to 8.5 percent. And if climate change continues on that trajectory, the world would experience a 4.3C rise in temperature by the year 2100 -- meaning a 16 percent extinction rate.
Urban took a global approach with his analysis because there are inherent difficulties in comparing previous studies by various authors. Studies differed in significant ways, including assumptions, methods, species examined, and geographic regions. Findings were inconsistent and difficult to compare across species.
Further, about 60 percent of studies about the effects of climate change have centered on North America and Europe. Yet South America, Australia, and New Zealand are at greatest risk for species loss, says Urban.
The risk in South America, Australia and New Zealand is particularly troublesome because those continents have unique climate ranges and many of their native species have a limited range in which they can survive. Some of the native species with smaller ranges, such as amphibians and reptiles, face a 6 percent greater risk of extinction than do non-native species currently sharing their space.
"With Australia and New Zealand, we're also looking at land masses that are relatively small and isolated, so that the possibility of a species shifting to a new habitat simply doesn't exist," he adds.
Among the discoveries he made is that extinction risks did not vary significantly by taxonomic group, a finding he describes as unexpected.
"We have generally thought that certain groups were more at risk than others, but our results show that all taxonomic groups will be affected as the climate changes.
While all species affected by climate change will not become extinct, there will undoubtedly be unwanted changes to contend with.
Even species not threatened directly by extinction could experience substantial changes in abundance, distribution, and in their interactions with other species. In turn, this may affect ecosystems, crop growth, and the spread of disease, and have other unanticipated consequences.
"It's hard enough to predict change, but in the end, we have one climate to contend with," says Urban. "With living things, we are dealing with millions of species, none of which act precisely the same. In fact, we may be surprised, as indirect biologic risks that are not even recognized at present may turn out to have a greater impact than we've ever anticipated."
Story Source:
The above story is based on materials provided by University of Connecticut . The original article was written by Sheila Foran. Note: Materials may be edited for content and length.
Journal Reference:
M. C. Urban. Accelerating extinction risk from climate change. Science, 2015; 348 (6234): 571 DOI: 10.1126/science.aaa4984
Cite This Page:
</MainBody>
    </Article>
    <Article id="4">
        <date>Fri May 01 04:20:48 EEST 2015</date>
        <title>England set for 'substantial increase' in record-breaking warm years</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/XL6eDCbKlVw/150430212048.htm</Link>
        <Description>The likelihood of record-breaking warm years in England is set to substantially increase as a result of the human influence on the climate, new research suggests.</Description>
        <MainBody>Institute of Physics
Summary:
The likelihood of record-breaking warm years in England is set to substantially increase as a result of the human influence on the climate, new research suggests.
Share:
Total shares: 
FULL STORY
The likelihood of record-breaking warm years in England is set to substantially increase as a result of the human influence on the climate, new research suggests.
In a study published today, 1 May, in IOP Publishing's journal Environmental Research Letters, an international team of researchers has shown that the chances of England experiencing a record-breaking warm year, such as the one seen in 2014, is at least 13 times more likely as a result of anthropogenic climate change.
This is according to climate model simulations and detailed analyses of the Central England Temperature (CET) record--the world's longest instrumental temperature record dating back to 1659.
The results of the study showed that human activities have a large influence on extreme warm years in England, which the researchers claim is remarkable given England is such a small region of the world.
Lead author of the study Dr Andrew King, from the ARC Centre of Excellence for Climate System Science at the University of Melbourne, said: "When you look at average annual temperatures over larger regions of the world, such as the whole of Europe, there is a lower variability in temperatures from year to year compared with smaller areas.
"As a result of this low variability, it is easier to spot anomalies. This is why larger regions tend to produce stronger attribution statements, so it is remarkable that we get such a clear anthropogenic influence on temperatures in a relatively small area across central England."
To arrive at their results, the researchers firstly used climate model simulations to calculate the likelihood of very warm years when there is just natural forcings on the climate and no human influence, and then when there is both natural forcings and human influence. The change in the likelihood of warm years due to human influences on the climate was then calculated.
The researchers then observed the CET and picked out the warmest years from the record since 1900. The warmest years were then plotted onto a graph which the researchers used to calculate the likelihood of warm years happening now and warms years happening 100 years ago.
The model-based method suggested at least a 13-fold increase (with 90% confidence) due to human influences on the climate, whilst the observation-based approach suggested at least a 22-fold increase in the probability of very warm years in the climate of today compared with the climate of a century ago (again with 90% confidence).
"Both of our approaches showed that there is a significant and substantial increase in the likelihood of very warm years occurring in central England," Dr King Continued.
According to the CET, 2014 was the warmest year on record in central England. It has been reported that during the last 60 years there has been rapid warming in the CET in line with the anthropogenic influence on the climate, with the highest average annual temperature of 10.93 C recorded in 2014.
The Central England Temperature (CET) series, which is the longest instrumental time series of temperature in the world, has monthly recordings of average temperatures dating back to 1659 and recordings of average daily temperatures dating back to 1772.
The CET is designed to represent the climate of the English Midlands, which is approximated by a triangular area enclosed by Lancashire in the north, Bristol in the south-west and London in the south-east. The CET has undergone thorough and extensive quality control, making it an ideal resource for studying long-term temperature trends across the region.
As to whether these results can be seen to be representative of areas outside of central England, Dr King said: "I would expect that other areas near the UK would produce similar results.
"For larger regions, stronger attribution statements can often be made. For example, we performed a similar attribution study for Europe as a whole and found a 35-fold increase in the likelihood of extremely warm years using model simulations."
This research was undertaken with the assistance of resources from the National Computational Infrastructure (NCI), which is supported by the Australian Government.
Story Source:
The above story is based on materials provided by Institute of Physics . Note: Materials may be edited for content and length.
Journal Reference:
Andrew D King, Geert Jan van Oldenborgh, David J Karoly, Sophie C Lewis, Heidi Cullen. Attribution of the record high Central England temperature of 2014 to anthropogenic influences. Environmental Research Letters, 2015; 10 (5): 054002 DOI: 10.1088/1748-9326/10/5/054002
Cite This Page:
</MainBody>
    </Article>
    <Article id="5">
        <date>Fri May 01 02:11:40 EEST 2015</date>
        <title>Gravity data show that Antarctic ice sheet is melting increasingly faster</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/G0BO15u15mE/150430191140.htm</Link>
        <Description>Researchers 'weighed' Antarctica's ice sheet using gravitational satellite data and found that during the past decade, Antarctica's massive ice sheet lost twice the amount of ice in its western portion compared with what it accumulated in the east. Their conclusion -- the southern continent's ice cap is melting ever faster.</Description>
        <MainBody>Gravity data show that Antarctic ice sheet is melting increasingly faster
Date:
Princeton  University
Summary:
Researchers 'weighed' Antarctica's ice sheet using gravitational satellite data and found that during the past decade, Antarctica's massive ice sheet lost twice the amount of ice in its western portion compared with what it accumulated in the east. Their conclusion -- the southern continent's ice cap is melting ever faster.
Share:
Total shares: 
FULL STORY
Princeton University researchers "weighed" Antarctica's ice sheet using gravitational satellite data and found that from 2003 to 2014, the ice sheet lost 92 billion tons of ice per year.
Credit: Image by Christopher Harig, Department of Geosciences
Princeton University researchers "weighed" Antarctica's ice sheet using gravitational satellite data and found that from 2003 to 2014, the ice sheet lost 92 billion tons of ice per year.
Credit: Image by Christopher Harig, Department of Geosciences
Close
During the past decade, Antarctica's massive ice sheet lost twice the amount of ice in its western portion compared with what it accumulated in the east, according to Princeton University researchers who came to one overall conclusion -- the southern continent's ice cap is melting ever faster.
The researchers "weighed" Antarctica's ice sheet using gravitational satellite data and found that from 2003 to 2014, the ice sheet lost 92 billion tons of ice per year, the researchers report in the journal Earth and Planetary Science Letters. If stacked on the island of Manhattan, that amount of ice would be more than a mile high -- more than five times the height of the Empire State Building.
The vast majority of that loss was from West Antarctica, which is the smaller of the continent's two main regions and abuts the Antarctic Peninsula that winds up toward South America. Since 2008, ice loss from West Antarctica's unstable glaciers doubled from an average annual loss of 121 billion tons of ice to twice that by 2014, the researchers found. The ice sheet on East Antarctica, the continent's much larger and overall more stable region, thickened during that same time, but only accumulated half the amount of ice lost from the west, the researchers reported.
"We have a solution that is very solid, very detailed and unambiguous," said co-author Frederik Simons, a Princeton associate professor of geosciences. "A decade of gravity analysis alone cannot force you to take a position on this ice loss being due to anthropogenic global warming. All we have done is take the balance of the ice on Antarctica and found that it is melting -- there is no doubt. But with the rapidly accelerating rates at which the ice is melting, and in the light of all the other, well-publicized lines of evidence, most scientists would be hard pressed to find mechanisms that do not include human-made climate change."
Compared to other types of data, the Princeton study shows that ice is melting from West Antarctica at a far greater rate than was previously known and that the western ice sheet is much more unstable compared to other regions of the continent, said first author Christopher Harig, a Princeton postdoctoral research associate in geosciences. Overall, ice-loss rates from all of Antarctica increased by 6 billion tons per year each year during the 11-year period the researchers examined. The melting rate from West Antarctica, however, grew by 18 billion tons per year every year, Harig and Simons found. Accelerations in ice loss are measured in tons per year, per year, or tons per year squared.
Of most concern, Harig said, is that this massive and accelerating loss occurred along West Antarctica's Amundsen Sea, particularly Pine Island and the Thwaites Glacier, where heavy losses had already been recorded. An iceberg more than 2,000 square miles in size broke off from the Thwaites Glacier in 2002.
In Antarctica, it's the ocean currents rather than air temperatures that melt the ice, and melted land ice contributes to higher sea levels in a way that melting icebergs don't, Harig said. As the ocean warms, floating ice shelves melt and can no longer hold back the land ice.
"The fact that West Antarctic ice-melt is still accelerating is a big deal because it's increasing its contribution to sea-level rise," Harig said. "It really has potential to be a runaway problem. It has come to the point that if we continue losing mass in those areas, the loss can generate a self-reinforcing feedback whereby we will be losing more and more ice, ultimately raising sea levels by tens of feet."
The Princeton study differs from existing approaches to measuring Antarctic ice loss in that it derives from the only satellite data that measure the mass of ice rather than its volume, which is more typical, Simons explained. He and Harig included monthly data from GRACE, or the Gravity Recovery and Climate Experiment, a dual-satellite joint mission between NASA and the German Aerospace Center. GRACE measures gravity changes to determine the time-variable behavior of various components in the Earth's mass system such as ocean currents, earthquake-induced changes and melting ice. Launched in 2002, the GRACE satellites are expected to be retired by 2016 with the first of two anticipated replacement missions scheduled for 2017.
While the volume of an ice sheet -- or how much space it takes up -- is also crucial information, it can change without affecting the amount of ice that is present, Simons explained. Snow and ice, for instance, compact under their own weight so that to the lasers that are bounced off the ice's surface to determine volume, there appears to be a reduction in the amount of ice, Simons said. Mass or weight, on the other hand, changes when ice is actually redistributed and lost.
Simons equated the difference between measuring ice volume and mass to a person weighing himself by only looking in the mirror instead of standing on a scale.
"You shouldn't only look at the ice volume -- you should also weigh it to find the mass changes," Simons said. "But there isn't going to be a whole lot of research of this type coming up because the GRACE satellites are on their last legs. This could be the last statement of this kind on these kinds of data for a long time. There may be a significant data gap during which the only monitoring available will not be by 'weighing' but by 'looking' via laser or radar altimetry, photogrammetry or field studies."
Harig and Simons developed a unique data-analysis method that allowed them to separate GRACE data by specific Antarctic regions. Because the ice sheet behaves differently in different areas, a continent-wide view would provide a general sense of how all of the ice mass, taken together, has changed, but exclude finer-scale geographical detail and temporal fluctuations. They recently published a paper about their computational methods in the magazine EOS, Transactions of the American Geophysical Union, and used a similar method for a 2012 paper published in the Proceedings of the National Academy of Sciences that revealed sharper-than-ever details about Greenland's accelerating loss of its massive ice sheet.
Robert Kopp, a Rutgers University associate professor of earth and planetary sciences and associate director of the Rutgers Energy Institute, said the analysis method Harig and Simons developed allowed them to capture a view of regional Antarctic ice loss "more accurately than previous approaches." Beyond the recent paper, Harig and Simons' method could be important for testing models of Antarctic ice-sheet stability developed by other researchers, he said.
"The notable feature of this research is the power of their method to resolve regions geographically in gravity data," Kopp said. "I expect that [their] technique will be an important part of monitoring future changes in the ice sheet and testing such models."
Story Source:
The above story is based on materials provided by Princeton  University . The original article was written by Morgan Kelly. Note: Materials may be edited for content and length.
Journal Reference:
Christopher Harig, Frederik J. Simons. Accelerated West Antarctic ice mass loss continues to outpace East Antarctic gains. Earth and Planetary Science Letters, 2015; 415: 134 DOI: 10.1016/j.epsl.2015.01.029
Cite This Page:
</MainBody>
    </Article>
    <Article id="6">
        <date>Fri May 01 02:11:38 EEST 2015</date>
        <title>Replacing one serving of sugary drink per day by water or unsweetened tea or coffee cuts risk of type 2 diabetes, study shows</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/Nb_fIqxEUfI/150430191138.htm</Link>
        <Description>Replacing the daily consumption of one serving of a sugary drink with either water or unsweetened tea or coffee can lower the risk of developing diabetes by between 14 percent and 25 percent, concludes new research.</Description>
        <MainBody>Science News
from research organizations
Replacing one serving of sugary drink per day by water or unsweetened tea or coffee cuts risk of type 2 diabetes, study shows
Date:
Diabetologia
Summary:
Replacing the daily consumption of one serving of a sugary drink with either water or unsweetened tea or coffee can lower the risk of developing diabetes by between 14 percent and 25 percent, concludes new research.
Share:
Total shares: 
FULL STORY
New research published in Diabetologia (the journal of the European Association for the Study of Diabetes) indicates that for each 5% increase of a person's total energy intake provided by sweet drinks including soft drinks, the risk of developing type 2 diabetes may increase by 18%. However, the study also estimates that replacing the daily consumption of one serving of a sugary drink with either water or unsweetened tea or coffee can lower the risk of developing diabetes by between 14% and 25%.
This research is based on the large EPIC-Norfolk study which included more than 25,000 men and women aged 40-79 years living in Norfolk, UK. Study participants recorded everything that they ate and drank for 7 consecutive days covering weekdays and weekend days, with particular attention to type, amount and frequency of consumption, and whether sugar was added by the participants. During approximately 11 years of follow-up, 847 study participants were diagnosed with new-onset type 2 diabetes.
Lead scientist Dr Nita Forouhi, of the UK Medical Research Council (MRC) Epidemiology Unit, University of Cambridge, said: "By using this detailed dietary assessment with a food diary, we were able to study several different types of sugary beverages, including sugar-sweetened soft drinks, sweetened tea or coffee and sweetened milk drinks as well as artificially sweetened beverages (ASB) and fruit juice, and to examine what would happen if water, unsweetened tea or coffee or ASB were substituted for sugary drinks."
In an analysis that accounted for a range of important factors including total energy intake the researchers found that there was an approximately 22% increased risk of developing type 2 diabetes per extra serving per day habitually of each of soft drinks, sweetened milk beverages and ASB consumed, but that consumption of fruit juice and sweetened tea or coffee was not related to diabetes. After further accounting for body mass index and waist girth as markers of obesity, there remained a higher risk of diabetes associated with consumption of both soft drinks and sweetened milk drinks, but the link with ASB consumption no longer remained, likely explained by the greater consumption of ASB by those who were already overweight or obese.
This new research with the greater detail on types of beverages adds to previous research published in Diabetologia by the authors in 2013 which collected information from food frequency questionnaires across 8 European countries. That previous work indicated that habitual daily consumption of sugar sweetened beverages (defined as carbonated soft drinks or diluted syrups) was linked with higher risk of type 2 diabetes, consistent with the current new findings.
In this new study, the authors also found that if study participants had replaced a serving of soft drinks with a serving of water or unsweetened tea or coffee, the risk of diabetes could have been cut by 14%; and by replacing a serving of sweetened milk beverage with water or unsweetened tea or coffee, that reduction could have been 20%-25%. However, consuming ASB instead of any sugar-sweetened drink was not associated with a statistically significant reduction in type 2 diabetes, when accounting for baseline obesity and total energy intake.
Finally, they found that each 5% of higher intake of energy (as a proportion of total daily energy intake) from total sweet beverages (soft drinks, sweetened tea or coffee, sweetened milk beverages, fruit juice) was associated with a 18% higher risk of diabetes. The authors estimated that if study participants had reduced the energy they obtained from sweet beverages to below 10%, 5% or 2% of total daily energy, 3%, 7% or 15% respectively of new-onset diabetes cases could have been avoided.
Dr Forouhi said: "The good news is that our study provides evidence that replacing a daily serving of a sugary soft drink or sugary milk drink with water or unsweetened tea or coffee can help to cut the risk of diabetes, offering practical suggestions for healthy alternative drinks for the prevention of diabetes."
The authors acknowledge limitations of dietary research which relies on asking people what they eat, but their study was large with long follow-up and had detailed assessment of diet that was collected in real-time as people consumed the food/drinks, rather than relying on memory. They concluded that their study helps to provide evidence with a robust method within the currently available methods and with detailed attention to accounting for factors that could distort the findings.
Commenting on the wider implications of these results, Dr Forouhi concluded: "Our new findings on the potential to reduce the burden of diabetes by reducing the percentage of energy consumed from sweet beverages add further important evidence to the recommendation from the World Health Organization to limit the intake of free sugars in our diet."
Story Source:
The above story is based on materials provided by Diabetologia . Note: Materials may be edited for content and length.
Journal Reference:
Nita G. Forouhi et al. Prospective associations and population impact of sweet beverage intake and type 2 diabetes, and effects of substitutions with alternative beverages. Diabetologia, April 2015 DOI: 10.1007/s00125-015-3572-1
Cite This Page:
</MainBody>
    </Article>
    <Article id="7">
        <date>Fri May 01 02:10:28 EEST 2015</date>
        <title>Frequent aspirin use reduces risk of cervical cancer by nearly half</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/jOlqpX3Usnk/150430191028.htm</Link>
        <Description>Long-term and frequent use of aspirin is associated with significantly decreased risk of cervical cancer, according to a study. According to the American Cancer Society, 12,900 new cases of cervical cancer will be diagnosed and 4,100 women will die from the disease in 2015.</Description>
        <MainBody>Frequent aspirin use reduces risk of cervical cancer by nearly half
Date:
Roswell Park Cancer Institute
Summary:
Long-term and frequent use of aspirin is associated with significantly decreased risk of cervical cancer, according to a study. According to the American Cancer Society, 12,900 new cases of cervical cancer will be diagnosed and 4,100 women will die from the disease in 2015.
Share:
Total shares: 
FULL STORY
Long-term and frequent use of aspirin is associated with significantly decreased risk of cervical cancer, according to a study led by researchers at Roswell Park Cancer Institute (RPCI) and published in the Journal of Lower Genital Tract Disease.
Aspirin use was associated with a 47% reduced risk of cervical cancer among frequent users -- those who used aspirin seven or more times a week, regardless of duration -- and 41% reduced risk among long-term frequent users -- those with five or more years of frequent use. Acetaminophen use was not associated with decreased risk of cervical cancer. A research team led by Kirsten Moysich, PhD, Professor of Oncology in the Department of Cancer Prevention and Control at Roswell Park, reported the results from the first U.S.-based study to examine the association between regular use of aspirin or acetaminophen.
"Aspirin use remains an attractive cancer-prevention option, due to the fact that most people will be more likely to take a pill rather than make major lifestyle modifications such as quitting smoking, eating a healthy diet and engaging in physical activity. However, people need to talk to their doctor before starting an aspirin regimen," says Dr. Moysich.
The study examined 328 patients with cervical cancer and 1,312 controls, matched on age and decade, who enrolled in a hospital-based case-control study drawn from 26,831 patients who received treatment at Roswell Park Cancer Institute and completed the Patient Epidemiology Data System questionnaire between 1982 and 1998. Participants provided self-reported information on the frequency and duration of aspirin and/or acetaminophen use.
"Further research is needed," adds Dr. Moysich, "on the role of daily, long-term use of aspirin and acetaminophen as both cervical cancer chemopreventive agents and enhancement to standard treatment strategies post-diagnosis."
According to the American Cancer Society, 12,900 new cases of cervical cancer will be diagnosed and 4,100 women will die from the disease in 2015.
Story Source:
The above story is based on materials provided by Roswell Park Cancer Institute . Note: Materials may be edited for content and length.
Journal Reference:
Grace Friel, Cici S. Liu, Nonna V. Kolomeyevskaya, Shalaka S. Hampras, Bridget Kruszka, Kristina Schmitt, Rikki A. Cannioto, Shashikant B. Lele, Kunle O. Odunsi, Kirsten B. Moysich. Aspirin and Acetaminophen Use and the Risk of Cervical Cancer. Journal of Lower Genital Tract Disease, 2015; 1 DOI: 10.1097/LGT.0000000000000104
Cite This Page:
</MainBody>
    </Article>
    <Article id="8">
        <date>Fri May 01 02:10:26 EEST 2015</date>
        <title>Keep it long: Most science writing advice flops, analysis finds</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/SxQ2TXeX47c/150430191026.htm</Link>
        <Description>When writing the abstracts for journal articles, most scientists receive similar advice: keep it short, dry, and simple. But a new analysis of over 1 million abstracts finds that many of these tips backfire, producing abstracts cited less than their long, flowery, and jargon-filled peers.</Description>
        <MainBody>Computation Institute
Summary:
When writing the abstracts for journal articles, most scientists receive similar advice: keep it short, dry, and simple. But a new analysis of over 1 million abstracts finds that many of these tips backfire, producing abstracts cited less than their long, flowery, and jargon-filled peers.
Share:
Total shares: 
FULL STORY
When writing the abstracts for journal articles, most scientists receive similar advice: keep it short, dry, and simple. But a new analysis by University of Chicago researchers of over one million abstracts finds that many of these tips backfire, producing abstracts cited less than their long, flowery, and jargon-filled peers.
The study, published in PLoS Computational Biology, suggests that most general writing rules are not as effective in scientific publications, which may reflect the influence of online search upon how science is discovered and consumed today.
"What I think is funny is there's this disconnect between what you'd like to read, and what scientists actually cite," said Stefano Allesina, professor of evolution and ecology at the University of Chicago, Computation Institute fellow and faculty, and senior author of the study. "It's very suggestive that we should not trust writing tips we take for granted."
During a seminar for incoming graduate students on how to write effective abstracts, Allesina wondered whether there was hard evidence for the "rules" that were taught. So Allesina and Cody Weinberger, a University of Chicago undergraduate, gathered hundreds of writing suggestions from scientific literature and condensed them into "Ten Simple Rules," including "Keep it short," "Keep it simple," "Signal novelty and importance," and "Show confidence."
The authors, which also include University of Chicago associate professor of sociology, Computation Institute Senior Fellow, and director of Knowledge Lab James Evans, then collected one million abstracts from disciplines such as biology, chemistry, geology, and psychology, and tested how the above rules affected a paper's citations, relative to other papers in the same journal. For example, testing "Keep it short," looked at the relationship between the number of words or sentences in an abstract and subsequent citations.
This particular analysis found that shorter abstracts led to fewer citations across all disciplines tested -- a refutation of the idea that "brief is better." Other tests found that using more adjectives, adverbs, uncommon words, signals of novelty and importance, and "pleasant" words boosted citations, despite frequent warnings or rules against using each of these features.
Taken together and literally, the results would advise using a "lengthy, convoluted, highly-indexible, self-describing abstract" to attract more citations. But the authors don't actually recommend that approach.
"If you were to follow all the rules, it would be absolutely horrible, terrible to read," Allesina said. "I would discount the suggestions you are typically given, but I would not blindly embrace those we provide. There's no magic trick; you have to write good papers and good abstracts, and it has to make sense."
However, "more might be more, rather than less," Evans suggests. "In an era of online searching, a more complete description can attract more eyeballs, the results of more searches and, ultimately, more attention and acclaim."
The authors hypothesize that the results reflect changing patterns of scientific discovery and consumption, as researchers access articles online instead of in print. Many commonly used databases, such as PubMed or Web of Science, search only abstracts when a user submits a query. As a result, longer abstracts with more specific words about the research within the full paper are more likely to be found, and subsequently cited.
Regardless of the reason for the relationship, the results suggest that journals do a disservice to scientists and their research by enforcing low word counts for abstracts. Instead of writing constraints, science indexes that search the full text of articles would be more effective at helping scientists find relevant research, Allesina proposed.
"My feeling is that with these word limits, the stricter the worse," Allesina said. "It's completely unnecessary in this day and age, when articles are rarely printed."
Story Source:
The above story is based on materials provided by Computation Institute . Note: Materials may be edited for content and length.
Journal Reference:
Cody J. Weinberger, James A. Evans, Stefano Allesina. Ten Simple (Empirical) Rules for Writing Science. PLOS Computational Biology, 2015; 11 (4): e1004205 DOI: 10.1371/journal.pcbi.1004205
Cite This Page:
</MainBody>
    </Article>
    <Article id="9">
        <date>Fri May 01 00:07:57 EEST 2015</date>
        <title>See flower cells in 3-D: No electron microscopy required</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/K4_Sav9NAKQ/150430170757.htm</Link>
        <Description>High-resolution imaging of plant cells is important in many plant studies, and the most commonly used method is scanning electron microscopy (SEM). But SEM can have limitations, including damage to material during sample preparation and high equipment costs. Researchers have developed an optical sectioning-3-D reconstruction method using a compound fluorescence light microscope. The new method is simpler and more cost-effective than SEM.</Description>
        <MainBody>Botanical Society of America
Summary:
High-resolution imaging of plant cells is important in many plant studies, and the most commonly used method is scanning electron microscopy (SEM). But SEM can have limitations, including damage to material during sample preparation and high equipment costs. Researchers have developed an optical sectioning-3-D reconstruction method using a compound fluorescence light microscope. The new method is simpler and more cost-effective than SEM.
Share:
Total shares: 
FULL STORY
A mature flower of Saltugilia caruifolia (left), accompanied with a microscopic view of the petal lobes (right) showing the 3-D cell structure, which has implications in the flower's development and its ability to attract pollinators.
Credit: Photo by Jacob B. Landis from: Landis, J. B., K. L. Ventura, D. E. Soltis, P. S. Soltis, and D. G. Oppenheimer. 2015. Optical sectioning and 3D reconstructions as an alternative to scanning electron microscopy for analysis of cell shape. Applications in Plant Sciences 3(4): 1400112. doi:10.3732/apps.1400112; Articles and materials published in Applications in Plant Sciences are licensed under a Creative Commons Attribution license (CC-BY-NC-SA).
A mature flower of Saltugilia caruifolia (left), accompanied with a microscopic view of the petal lobes (right) showing the 3-D cell structure, which has implications in the flower's development and its ability to attract pollinators.
Credit: Photo by Jacob B. Landis from: Landis, J. B., K. L. Ventura, D. E. Soltis, P. S. Soltis, and D. G. Oppenheimer. 2015. Optical sectioning and 3D reconstructions as an alternative to scanning electron microscopy for analysis of cell shape. Applications in Plant Sciences 3(4): 1400112. doi:10.3732/apps.1400112; Articles and materials published in Applications in Plant Sciences are licensed under a Creative Commons Attribution license (CC-BY-NC-SA).
Close
Scientists require high-resolution imaging of plant cells to study everything from fungal infections to reproduction in maize. These images are captured with scanning electron microscopy (SEM), where an electron microscope focuses beams of electrons to increase magnification of objects. SEM is a common technique for all fields of science.
However, preparing objects for SEM and other common imaging methods can compromise delicate biological samples. Freeze-drying of material, electron beams, and vacuum pressure in the microscope can all result in cell damage, which is problematic for visualizing cell shape, size, and development. In addition, preparing material to be viewed under the microscope using SEM requires specialized equipment, causing a low throughput and placing a large financial burden on scientific labs.
Researchers at the University of Florida have developed new methods for a less damaging, more cost-effective, and higher-throughput approach for imaging cellular structure. "For visualizing cells of a variety of plant material," explains researcher Jacob Landis, "there are alternatives to the commonly used SEM. Some of these alternatives may be more easily accessible to researchers and do not require specialized equipment for processing prior to imaging."
Landis and colleagues created protocols for an optical sectioning-3D reconstruction method that uses a compound fluorescence light microscope. A compound microscope has multiple lenses to reflect light and project a high-magnification image. The microscope and a mounted camera "sections" the plant cells, capturing pictures of several layers and planes of the sample material. The images are reconstructed into a final 3D image with equal to higher resolution as images produced using SEM.
Landis developed the new protocol (detailed in a recent issue of Applications in Plant Sciences) to see epidermal cells on the surfaces of flower petals. This group of cells reveals extensive information to geneticists, agronomists, and ecologists. Flower epidermal cells cause all types of cellular-level changes that can scale up to influence flower color and development, as well as ecological interactions such as a plant's success in attracting pollinators and avoiding insect enemies.
Unlike in SEM, material to be imaged using the new method gets preserved, allowing for clear imaging months after the collection of any samples. This is valuable when studies require bulk processing of samples. The material is then dehydrated in ethanol alcohol, dipped in a liquid to remove the waxy film that protects the outer layer of petal cells, and mounted in a fluorescent dye to help visualize details under the microscope.
The method was catalyzed by a larger study on the cellular and genetic basis of flower size in the woodland plant group Saltugilia. Saltugilia plants have a large range of flower sizes and host all types of pollinators--including hummingbirds, bees, and bee flies--while some host no pollinators at all. SEM could not be used to understand these diverse flowers because it is limited to samples 3-5 mm in size. The new optical sectioning-3D reconstruction approach allows for samples as large as 5 cm to be imaged before cutting petals and compromising petal cellular structure.
Optical-sectioning and 3D reconstruction have been informally tested on other plant material, including leaf cells and reproductive organs (stamens, anthers, and sepals). "We believe that any kind of tissue that can be mounted on a microscope slide will be possible to image," says Landis. "I have spent a lot of time using SEM and prepping samples, and optical sectioning with 3D reconstruction is by far the simplest and most cost-effective method I have ever used."
Story Source:
The above story is based on materials provided by Botanical Society of America . Note: Materials may be edited for content and length.
Journal Reference:
Jacob B. Landis, Kayla L. Ventura, Douglas E. Soltis, Pamela S. Soltis, David G. Oppenheimer. Optical Sectioning and 3D Reconstructions as an Alternative to Scanning Electron Microscopy for Analysis of Cell Shape. Applications in Plant Sciences, 2015; 3 (4): 1400112 DOI: 10.3732/apps.1400112
Cite This Page:
</MainBody>
    </Article>
    <Article id="10">
        <date>Fri May 01 00:07:55 EEST 2015</date>
        <title>Did dinosaur-killing asteroid trigger largest lava flows on Earth?</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/1YXJOoen7xo/150430170755.htm</Link>
        <Description>The theory that an asteroid impact killed off the dinosaurs 66 million years ago is well accepted, but one puzzle is why another global catastrophe -- the huge, million-year eruption of the Deccan Traps flood basalts in India -- occurred at the same time. Geologists now argue this is not a coincidence. The impact probably rang Earth like a bell, reigniting an underground magma plume and generating the largest lava flows on Earth.</Description>
        <MainBody>University of California - Berkeley
Summary:
The theory that an asteroid impact killed off the dinosaurs 66 million years ago is well accepted, but one puzzle is why another global catastrophe -- the huge, million-year eruption of the Deccan Traps flood basalts in India -- occurred at the same time. Geologists now argue this is not a coincidence. The impact probably rang Earth like a bell, reigniting an underground magma plume and generating the largest lava flows on Earth.
Share:
Total shares: 
FULL STORY
Illustration of a hot mantle plume "head" pancaked beneath the Indian Plate. The theory by Richards and his colleagues suggests that existing magma within this plume head was mobilized by strong seismic shaking from the Chicxulub asteroid impact, resulting in the largest of the Deccan Traps flood basalt eruptions.
Credit: Mark Richards et al, UC Berkeley
Illustration of a hot mantle plume "head" pancaked beneath the Indian Plate. The theory by Richards and his colleagues suggests that existing magma within this plume head was mobilized by strong seismic shaking from the Chicxulub asteroid impact, resulting in the largest of the Deccan Traps flood basalt eruptions.
Credit: Mark Richards et al, UC Berkeley
Close
The asteroid that slammed into the ocean off Mexico 66 million years ago and killed off the dinosaurs probably rang the Earth like a bell, triggering volcanic eruptions around the globe that may have contributed to the devastation, according to a team of University of California, Berkeley, geophysicists.
Specifically, the researchers argue that the impact likely triggered most of the immense eruptions of lava in India known as the Deccan Traps, explaining the "uncomfortably close" coincidence between the Deccan Traps eruptions and the impact, which has always cast doubt on the theory that the asteroid was the sole cause of the end-Cretaceous mass extinction.
"If you try to explain why the largest impact we know of in the last billion years happened within 100,000 years of these massive lava flows at Deccan ... the chances of that occurring at random are minuscule," said team leader Mark Richards, UC Berkeley professor of earth and planetary science. "It's not a very credible coincidence."
Richards and his colleagues marshal evidence for their theory that the impact reignited the Deccan flood lavas in a paper to be published in The Geological Society of America Bulletin, available online today (April 30) in advance of publication.
While the Deccan lava flows, which started before the impact but erupted for several hundred thousand years after re-ignition, probably spewed immense amounts of carbon dioxide and other noxious, climate-modifying gases into the atmosphere, it's still unclear if this contributed to the demise of most of life on Earth at the end of the Age of Dinosaurs, Richards said.
"This connection between the impact and the Deccan lava flows is a great story and might even be true, but it doesn't yet take us closer to understanding what actually killed the dinosaurs and the 'forams,'" he said, referring to tiny sea creatures called foraminifera, many of which disappeared from the fossil record virtually overnight at the boundary between the Cretaceous and Tertiary periods, called the KT boundary. The disappearance of the landscape-dominating dinosaurs is widely credited with ushering in the age of mammals, eventually including humans.
He stresses that his proposal differs from an earlier hypothesis that the energy of the impact was focused around Earth to a spot directly opposite, or antipodal, to the impact, triggering the eruption of the Deccan Traps. The "antipodal focusing" theory died when the impact crater, called Chicxulub, was found off the Yucatn coast of Mexico, which is about 5,000 kilometers from the antipode of the Deccan traps.
Flood basalts
Richards proposed in 1989 that plumes of hot rock, called "plume heads," rise through Earth's mantle every 20-30 million years and generate huge lava flows, called flood basalts, like the Deccan Traps. It struck him as more than coincidence that the last four of the six known mass extinctions of life occurred at the same time as one of these massive eruptions.
"Paul Renne's group at Berkeley showed years ago that the Central Atlantic Magmatic Province is associated with the mass extinction at the Triassic/Jurassic boundary 200 million years ago, and the Siberian Traps are associated with the end Permian extinction 250 million years ago, and now we also know that a big volcanic eruption in China called the Emeishan Traps is associated with the end-Guadalupian extinction 260 million years ago," Richards said. "Then you have the Deccan eruptions -- including the largest mapped lava flows on Earth -- occurring 66 million years ago coincident with the KT mass extinction. So what really happened at the KT boundary?"
Richards teamed up with experts in many areas to try to discover faults with his radical idea that the impact triggered the Deccan eruptions, but instead came up with supporting evidence. Renne, a professor in residence in the UC Berkeley Department of Earth and Planetary Science and director of the Berkeley Geochronology Center, re-dated the asteroid impact and mass extinction two years ago and found them essentially simultaneous, but also within approximately 100,000 years of the largest Deccan eruptions, referred to as the Wai subgroup flows, which produced about 70 percent of the lavas that now stretch across the Indian subcontinent from Mumbai to Kolkata.
Michael Manga, a professor in the same department, has shown over the past decade that large earthquakes -- equivalent to Japan's 9.0 Tohoku quake in 2011 -- can trigger nearby volcanic eruptions. Richards calculates that the asteroid that created the Chicxulub crater might have generated the equivalent of a magnitude 9 or larger earthquake everywhere on Earth, sufficient to ignite the Deccan flood basalts and perhaps eruptions many places around the globe, including at mid-ocean ridges.
"It's inconceivable that the impact could have melted a whole lot of rock away from the impact site itself, but if you had a system that already had magma and you gave it a little extra kick, it could produce a big eruption," Manga said.
Similarly, Deccan lava from before the impact is chemically different from that after the impact, indicating a faster rise to the surface after the impact, while the pattern of dikes from which the supercharged lava flowed -- "like cracks in a souffl," Renne said -- are more randomly oriented post-impact.
"There is a profound break in the style of eruptions and the volume and composition of the eruptions," said Renne. "The whole question is, 'Is that discontinuity synchronous with the impact?'"
Reawakened volcanism
Richards, Renne and graduate student Courtney Sprain, along with Deccan volcanology experts Steven Self and Loc Vanderkluysen, visited India in April 2014 to obtain lava samples for dating, and noticed that there are pronounced weathering surfaces, or terraces, marking the onset of the huge Wai subgroup flows. Geological evidence suggests that these terraces may signal a period of quiescence in Deccan volcanism prior to the Chicxulub impact. Apparently never before noticed, these terraces are part of the western Ghats, a mountain chain named after the Hindu word for steps.
"This was an existing massive volcanic system that had been there probably several million years, and the impact gave this thing a shake and it mobilized a huge amount of magma over a short amount of time," Richards said. "The beauty of this theory is that it is very testable, because it predicts that you should have the impact and the beginning of the extinction, and within 100,000 years or so you should have these massive eruptions coming out, which is about how long it might take for the magma to reach the surface."
Story Source:
The above story is based on materials provided by University of California - Berkeley . The original article was written by Robert Sanders. Note: Materials may be edited for content and length.
Journal Reference:
Mark A. Richards, Walter Alvarez, Stephen Self, Leif Karlstrom, Paul R. Renne, Michael Manga, Courtney J. Sprain, Jan Smit, Loc Vanderkluysen, and Sally A. Gibson. Triggering of the largest Deccan eruptions by the Chicxulub impact. Geological Society of America Bulletin, April 30, 2015 DOI: 10.1130/B31167.1
Cite This Page:
</MainBody>
    </Article>
    <Article id="11">
        <date>Fri May 01 00:07:52 EEST 2015</date>
        <title>Dull forest glow yields orbital tracking of photosynthesis</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/grkj4yF2NVs/150430170752.htm</Link>
        <Description>New research provides some crucial ground truth for a method of measuring plant photosynthesis on a global scale from orbit. The work shows that chlorophyll fluorescence, a faint glow produced by plant leaves as a byproduct of photosynthesis, is a strong proxy for photosynthetic activity in the canopy of a deciduous forest.</Description>
        <MainBody>Dull forest glow yields orbital tracking of photosynthesis
Date:
Brown University
Summary:
New research provides some crucial ground truth for a method of measuring plant photosynthesis on a global scale from orbit. The work shows that chlorophyll fluorescence, a faint glow produced by plant leaves as a byproduct of photosynthesis, is a strong proxy for photosynthetic activity in the canopy of a deciduous forest.
Share:
Total shares: 
FULL STORY
Chlorophyll fluorescence, dull glow produced by plant leaves as a byproduct of photosynthesis, could provide a means of monitoring photosynthetic activity from orbiting satellites. Research conducted by Brown University scientists in the Harvard Forest helps to confirm that fluorescence is a good proxy for photosynthesis in a forest canopy.
Credit: Marc Mayes/Brown University
Chlorophyll fluorescence, dull glow produced by plant leaves as a byproduct of photosynthesis, could provide a means of monitoring photosynthetic activity from orbiting satellites. Research conducted by Brown University scientists in the Harvard Forest helps to confirm that fluorescence is a good proxy for photosynthesis in a forest canopy.
Credit: Marc Mayes/Brown University
Close
A research team led by geoscientists from Brown University and the Marine Biological Laboratory has provided some crucial ground-truth for a method of measuring plant photosynthesis on a global scale from low-Earth orbit.
The researchers have shown that chlorophyll fluorescence, a faint glow produced by plant leaves as a byproduct of photosynthesis, is a strong proxy for photosynthetic activity in the canopy of a deciduous forest. That glow can be detected by orbiting satellites and could be used to monitor global photosynthetic activity in real time.
"We show that fluorescence is tightly coupled to photosynthesis, capturing both daily and seasonal fluctuations," said Xi Yang, a postdoctoral researcher at Brown and the study's lead author. "This is the first time anyone has linked fluorescence to photosynthesis over a long time scale in a deciduous forest and validated orbital measurements of fluorescence with ground-based measurements."
The findings are published in the journal Geophysical Research Letters. Yang led the work as a graduate student in the Brown-Marine Biological Laboratory (MBL) graduate program, working with Brown geoscientist Jack Mustard and MBL associate scientist Jianwu (Jim) Tang.
Catching photons on the rebound
When plants photosynthesize, chlorophyll molecules in leaves absorb photons from sunlight. The plant then converts the energy from those photons into sugar and other carbohydrates using carbon dioxide absorbed from the atmosphere as a carbon source. But not all the photons absorbed by chlorophyll are for photosynthesis. Around 1 percent of them are re-emitted as lower energy photons, which creates the faint glow known as fluorescence.
The glow isn't visible to the naked eye, but a few years ago scientists from NASA and the Japanese Aerospace Exploration Agency found that spectrometers aboard climate satellites could detect fluorescence coming from croplands and forest canopies. That raised the possibility of measuring photosynthesis on a global scale.
Currently, the gold standard for measuring photosynthesis involves directly measuring the exchange of carbon dioxide gas between plants and the air directly around them. The technique, called eddy covariance, relies on tower-mounted detectors that can only monitor an area of a few square kilometers. Even with hundreds of towers around the world, eddy covariance can still only provide a patchwork of data. That's part of the reason scientists have looked to orbital instruments to get a broader view of photosynthesis.
Scientists are just beginning to measure the extent to which orbital measurements of fluorescence correlate to ground-based measurements of photosynthesis. A few studies have shown fluorescence to be a good proxy over cropland, but there hadn't been any studies looking at the link to a forest canopy over an extended period. This latest study fills that gap.
For the study, the researchers made use of the Harvard Forest in Massachusetts. Over the course of a full summer, they compared photosynthesis measurements from the forest's eddy covariance tower with fluorescence data taken with a tower-mounted spectrometer above the forest. The readings from the ground-based spectrometer were then compared to readings from an orbital instrument aboard the European Space Agency's GOME-2 satellite.
The study showed that fluorescence measurements from both the ground-based spectrometer and the satellite were tightly correlated to photosynthesis as measured by eddy covariance, capturing both day-to-day fluctuations and fluctuations that occurred over the course of the summer.
"The findings help to establish an empirical link between fluorescence and photosynthesis and help to validate the satellite product with ground-based observations," Yang said.
The fact that fluorescence appears to capture daily fluctuations of photosynthesis gives it an advantage over other remote sensing methods. One current method uses an index of greenness as a proxy for plant production. But greenness can sometimes lag well behind plant stress, Yang said, so fluorescence could be a much better way of getting real-time data.
"Measuring fluorescence is the hottest topic in remote sensing today. It is transforming how we measure photosynthesis on local to global scales," said Jack Mustard, professor of earth, environmental and planetary sciences and a fellow in the Institute at Brown for Environment and Society. "This work is fundamental in linking the signature of photosynthesis from leaf to orbit."
Story Source:
The above story is based on materials provided by Brown University . Note: Materials may be edited for content and length.
Journal Reference:
Xi Yang, Jianwu Tang, John F. Mustard, Jung-Eun Lee, Micol Rossini, Joanna Joiner, J. William Munger, Ari Kornfeld, Andrew D. Richardson. Solar-induced chlorophyll fluorescence that correlates with canopy photosynthesis on diurnal and seasonal scales in a temperate deciduous forest. Geophysical Research Letters, 2015; DOI: 10.1002/2015GL063201
Cite This Page:
</MainBody>
    </Article>
    <Article id="12">
        <date>Fri May 01 00:07:48 EEST 2015</date>
        <title>Listening for whales and fish in the Northwest Atlantic ocean</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/tvICy1btM10/150430170748.htm</Link>
        <Description>Scientists are using a variety of buoys and autonomous underwater vehicles to record and archive sounds from marine mammals and fish species in the western North Atlantic through a new listening network known as the US Northeast Passive Acoustic Sensing Network (NEPAN). Researchers hope NEPAN will be the first link in an extensive listening network that would extend along the entire US East Coast, and eventually to waters around the US.</Description>
        <MainBody>Listening for whales and fish in the Northwest Atlantic ocean
Date:
NOAA Northeast Fisheries Science Center
Summary:
Scientists are using a variety of buoys and autonomous underwater vehicles to record and archive sounds from marine mammals and fish species in the western North Atlantic through a new listening network known as the US Northeast Passive Acoustic Sensing Network (NEPAN). Researchers hope NEPAN will be the first link in an extensive listening network that would extend along the entire US East Coast, and eventually to waters around the US.
Share:
Total shares: 
FULL STORY
Autonomous underwater vehicles like this glider setting off on a mission are already in use for NEPAN.
Credit: Eric Matzen, NEFSC/NOAA
Autonomous underwater vehicles like this glider setting off on a mission are already in use for NEPAN.
Credit: Eric Matzen, NEFSC/NOAA
Close
Scientists are using a variety of buoys and autonomous underwater vehicles to record and archive sounds from marine mammals and fish species in the western North Atlantic through a new listening network known as the U.S. Northeast Passive Acoustic Sensing Network (NEPAN).
Stretching from the northern Gulf of Maine to the New York Bight in the northwest Atlantic Ocean, NEPAN provides year-round, long-term information on the presence and physical distribution of vocal whales, dolphins and porpoises and some fish species. The archived and near-real-time data comes from recorders on various platforms ranging from fixed bottom mounted and surface buoys to mobile underwater vehicles and surface wave gliders. The data will be used to reduce threats from human activities and address conservation and managements needs central to the mission of the National Oceanic and Atmospheric Administration (NOAA) and other federal agencies. Data will also fill in gaps in scientific information about many marine mammal and fish species.
"NEPAN is an example of how collaborative scientific efforts and financial investment across many federal agencies can produce a novel and wide-ranging solution to gaps in current scientific information," said Sofie Van Parijs, head of the passive acoustics group at NOAA's Northeast Fisheries Science Center (NEFSC) laboratory in Woods Hole, Mass.
Researchers hope NEPAN will be the first link in an extensive listening network that would extend along the entire U.S. East Coast, and eventually to waters around the U.S., to monitor marine mammals, fish and ocean noise over time periods ranging from days and weeks to months and years. The vision for the network, and details on the technologies, applications and projects being conducted and planned between 2014 and 2017, are described in an article in the March/April 2015 issue of the Marine Technology Society Journal. The special issue of MTS Journal focuses on "Discovering NOAA: Applications of Science and Technology Then, Now, and in the Future."
The NEFSC passive acoustics research program, together with support from Woods Hole Oceanographic Institution (WHOI) and a number of federal agencies, are creating the technological infrastructure for NEPAN to address long-term monitoring and mitigation needs for endangered marine mammals and fish. Six of eight whale populations in the western North Atlantic are endangered or of special concern under existing U.S. and Canadian law.
Their vision would include both fixed and mobile assets that would be deployed in sensitive or industrial areas, such as wind farm construction sites, shipping lanes, heavily fished areas or marine reserves. Other assets would cover broad spatial scales to address questions about species' range, migration routes, or presence in unexpected locations. Others would be co-located with oceanographic observatories, such as the Northeast Regional Association of Coastal and Ocean Observing Systems (NERACOOS) buoys and the U.S. Integrated Observing System Pioneer array, to enable research on the influence of environmental variability on the occurrence of marine mammals and fish.
Near-real-time reporting of acoustic detections should improve the efficiency of visual surveys from research vessels and aircraft and on the water research efforts by providing notice of a species presence. Archival passive acoustic recorders would aid monitoring needs that do not require an immediate response to a detection, such as defining migration corridors. The data would be used to inform NOAA Fisheries stock assessment reports, permit consultations, and specific management actions.
Several NEPAN projects are already underway, one on Atlantic cod conservation and the other on ambient ocean noise in Stellwagen Bank National Marine Sanctuary off the coast of Massachusetts. A moored acoustic buoy, deployed in March 2014, is providing real time information in Rhode Island Sound monitoring the presence of baleen whales, and gliders have been used to detect cod spawning. In 2015, a variety of platforms and technologies will be put into operation. They include a wave glider to survey areas of the New York Bight, and a moored buoy that will be installed near Mount Desert Rock, Maine, where fin and humpback whales are common.
"Passive acoustic data collection can be integrated among agencies, regions, and platforms to meet monitoring and management needs within NOAA," said Van Parijs, lead author of the article describing NEPAN. "We are also looking at integrating the NEPAN data into other passive acoustic monitoring efforts being conducted in both U.S. and Canadian waters."
One example is the Western Atlantic Passive Acoustics analysis of mysticetes, or baleen whales, (WAPAW). The project, headed by NEFSC, will utilize existing archival acoustic data from recorders previously deployed along the western North Atlantic continental shelf area. In another project, NOAA's NEFSC and Southeast Fisheries Science Center will collaborate with Duke University and Scripps Institution of Oceanography to monitor 10 sites along the western Atlantic shelf break to understand the occurrence of marine species prior to the start of oil and gas exploration in the western North Atlantic Ocean.
Funding for NEPAN has been provided through competitive research grants and various programs within the U.S. Department of Defense and Department of Energy, Bureau of Ocean Energy Management (BOEM), Department of Defense's Environmental Security Technology Certification Program (ESTCP), the U.S. Navy's Living Marine Resources Program (LMR) and the Naval Operations Energy and Environmental Readiness Division (N45), with some funding provided by the NOAA Ocean Acoustics Program, and the NOAA Office of Protected Resources.
"Expanding the range of NEPAN throughout the western North Atlantic Ocean will enable researchers to detect more species and address a wider variety of management and conservation applications," Van Parijs said. "The reality is that the continued operation and expansion of a listening network like NEPAN will only be possible in the long-term with clear and direct support from NOAA."
Story Source:
The above story is based on materials provided by NOAA Northeast Fisheries Science Center . Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="13">
        <date>Fri May 01 00:07:46 EEST 2015</date>
        <title>Short-term debt, depressive symptoms may go hand-in-hand</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/-a4c7Xi45ec/150430170746.htm</Link>
        <Description>Having short-term household debt -- credit cards and overdue bills -- increases depressive symptoms, research shows. The association is particularly strong among unmarried people, people reaching retirement age and those who are less well educated, according to a new study.</Description>
        <MainBody>Springer Science+Business Media
Summary:
Having short-term household debt -- credit cards and overdue bills -- increases depressive symptoms, research shows. The association is particularly strong among unmarried people, people reaching retirement age and those who are less well educated, according to a new study.
Share:
Total shares: 
FULL STORY
Results to be published in the Journal of Family and Economic Issues suggest that having short-term household debt -- credit cards and overdue bills -- increases depressive symptoms. The association is particularly strong among unmarried people, people reaching retirement age and those who are less well educated, according to a new study by lead author Lawrence Berger of the University of Wisconsin-Madison.
These are the first results to show the impact of different types of debt on depression and their effects on different sectors of the US population. Little evidence was found for an association with mid- or long-term debt.
"New debt contracts could be offered to vulnerable borrowers and the population sectors we identified could be targeted with help in building their financial capacity," says Berger. "The findings could also be used to help mental health practitioners better understand the impact of clients' borrowing habits on depression."
Debt contract provisions could include mandatory financial counselling and the right to rescind within a specified timeframe.
Those who had debt were younger, more likely to be male, less likely to be black or Hispanic, had more highly educated parents, were more highly educated themselves, were more likely to be married and working, had greater income and assets, and were in better health.
It was when the researchers began to adjust for measures of socio-economic status, and to refine their analysis to subgroups defined by age, education and marital status, that a negative association began to emerge. They also controlled for reverse causality to check that debt was causing depression and not vice versa.
The study was focused on around 8,500 working-age adults. The data were taken from two waves of the National Survey of Families and Households, conducted six years apart and ending in 1994. Overall findings included the fact that 79 percent of respondents had some debt, totalling an average of $42,000. Long-term debt accounted for by far the largest portion.
Spurred by increased homeownership and an increase in unsecured revolving credit card debt, household debt has increased dramatically in the last 40 years. While it has declined since 2008 as credit has become more difficult to obtain, it remains at historically high levels.
The researchers suggest that future research should include an analysis of whether the effects can be reversed and reducing short-term debt can help alleviate symptoms of depression.
Story Source:
The above story is based on materials provided by Springer Science+Business Media . Note: Materials may be edited for content and length.
Journal Reference:
Lawrence M. Berger, J. Michael Collins, Laura Cuesta. Household Debt and Adult Depressive Symptoms in the United States. Journal of Family and Economic Issues, 2015; DOI: 10.1007/s10834-015-9443-6
Cite This Page:
</MainBody>
    </Article>
    <Article id="14">
        <date>Fri May 01 00:07:17 EEST 2015</date>
        <title>Study questions quality of U.S. health data</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/76penlgotZ8/150430170717.htm</Link>
        <Description>Most U.S. clinical registries that collect data on patient outcomes are substandard and lack critical features necessary to render the information they collect useful for patients, physicians and policy makers, new research suggests.</Description>
        <MainBody>Study questions quality of U.S. health data
Date:
Johns Hopkins Medicine
Summary:
Most U.S. clinical registries that collect data on patient outcomes are substandard and lack critical features necessary to render the information they collect useful for patients, physicians and policy makers, new research suggests.
Share:
Total shares: 
FULL STORY
A new study by Johns Hopkins researchers concludes that most U.S. clinical registries that collect data on patient outcomes are substandard and lack critical features necessary to render the information they collect useful for patients, physicians and policy makers.
Findings of the study, published ahead of print April 24 in the Journal for Healthcare Quality, reveal poor data monitoring and reporting that researchers say are hurting national efforts to study disease, guide patient choice of optimal treatments, formulate rational health policies and track in a meaningful way how well physicians and hospitals perform.
"Our results highlight the acute need to improve the way clinical outcomes data are collected and reported," says senior investigator Marty Makary, M.D., M.P.H., professor of surgery at the Johns Hopkins University School of Medicine. "Failure to measure and accurately track patient outcomes remains one of the greatest problems in modern health care, curtailing our ability to understand disease, evaluate treatments and make the health-care industry a value-driven marketplace."
In addition, the failure to track patient outcomes in a systematic way is tantamount to not measuring the performance of a sector that claims one-fifth of the nation's economy, the research team says.
Clinical registries are databases of patient outcomes developed and maintained by medical organizations and medical specialty groups.
To evaluate the quality of clinical registries, Makary and colleagues say they created "a registry of registries" to study the way the health care industry measures its performance.
"We found it's the Wild West," Makary says. "With a few notable exceptions, most registries are underdeveloped, underfunded and often are not based on sound scientific methodology."
The investigators assessed 153 U.S. clinical registries containing health service and disease outcomes data. On average, a registry contained information on more than 160,000 patients treated across more than 1,600 hospitals.
Less than one-quarter of registries adjusted their results for differences in disease complexity -- information statistically reflective of disparities in illness severity and socio-economic status among patients treated across hospitals. Unadjusted data, the researchers say, could be misleading and should be interpreted with great caution.
Less than one-fifth of registries contained independently entered data -- information entered by clinicians other than the ones involved in care -- an important principle in mitigating the well-established bias of self-reported data, the researchers say.
Although one-quarter of registries -- 40 in total -- were funded by taxpayers, only three shared their data publicly. Of note, 84 percent (98 of 117) of U.S. recognized medical specialties had no national clinical registries -- a significant gap in the efforts to compare the efficacy of treatments and evaluate the quality of care on a large scale.
The researchers say such failure to capture and measure patient outcomes is troubling because the insights gleaned from such information could have a direct and profound impact on scientific research and human lives.
"A robust clinical registry can tell doctors in real time what medications work well and which are harming patients, yet the infrastructure to achieve that is vastly under-supported," says study co-author Michol Cooper, M.D., Ph.D., a surgical resident at the Johns Hopkins University School of Medicine. "The same rigorous standards we use to evaluate how well a drug does ought to apply to the way we report patient outcomes data."
Makary and team point out that several organizations maintained exemplary registries with rich, carefully analyzed data, audited and reported in a meaningful way. For example, information in the United Network for Organ Sharing registry has led to important research and discoveries that in turn became the catalyst for the creation of new national policies on organ transplantation. Likewise, Makary says, data from the National Surgical Quality Improvement Program, maintained by the American College of Surgeons, have generated valuable insights about surgical infections, transformed practice and improved patient outcomes. The National Cardiovascular Data Registry of the American College of Cardiology has also led to improvement in the rates of inpatient mortality among participating hospitals. The detailed and accurate data in the Cystic Fibrosis Foundation Patient Registry, Makary says, has allowed for hundreds of research trials, the results of which have played pivotal roles in developing better therapies and prolonging the lifespans of people with this genetic disease.
"These organizations' databases illustrate the power and potential of clinical registries to improve patient outcomes and inform best practices," says study lead author Heather Lyu, a research fellow at the Johns Hopkins University School of Medicine. "And if we really want to get serious about measuring and improving performance, we need to develop criteria that will help others run similarly successful registries."
The hallmarks of a good registry, the authors say, include:
--Data accounting for differences in patient case complexity across hospitals that allows for meaningful comparisons of outcomes
--Broad hospital participation --Measurement of complications that matter to patients and affect their quality of life
--Independent data collection that eliminates the bias inherent in self-reporting
--Public reporting and open access to hospital performance for taxpayer-funded registries
Story Source:
The above story is based on materials provided by Johns Hopkins Medicine . Note: Materials may be edited for content and length.
Journal Reference:
Lyu, Heather; Cooper, Michol; Patel, Kavita; Daniel, Michael; Makary, Martin A. Prevalence and Data Transparency of National Clinical Registries in the United States. Journal for Healthcare Quality, April 2015 DOI: 10.1097/JHQ.0000000000000001
Cite This Page:
</MainBody>
    </Article>
    <Article id="15">
        <date>Fri May 01 00:07:15 EEST 2015</date>
        <title>Walking an extra two minutes each hour may offset hazards of sitting too long</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/z_ceCWzl-AA/150430170715.htm</Link>
        <Description>A new study suggests that engaging in low intensity activities such as standing may not be enough to offset the health hazards of sitting for long periods of time. On the bright side, adding two minutes of walking each hour to your routine just might do the trick.</Description>
        <MainBody>Walking an extra two minutes each hour may offset hazards of sitting too long
Date:
University of Utah Health Sciences
Summary:
A new study suggests that engaging in low intensity activities such as standing may not be enough to offset the health hazards of sitting for long periods of time. On the bright side, adding two minutes of walking each hour to your routine just might do the trick.
Share:
Even casual walking for an extra two minutes each hour may help prolong life.
Credit:  Rawpixel / Fotolia
Even casual walking for an extra two minutes each hour may help prolong life.
Credit:  Rawpixel / Fotolia
Close
A new study suggests that engaging in low intensity activities such as standing may not be enough to offset the health hazards of sitting for long periods of time. On the bright side, adding two minutes of walking each hour to your routine just might do the trick. These findings were published in the Clinical Journal of the American Society of Nephrology (CJASN).
Numerous studies have shown that sitting for extended periods of time each day leads to increased risk for early death, as well as heart disease, diabetes and other health conditions. Considering that 80 percent of Americans fall short of completing the recommended amount of exercise, 2.5 hours of moderate activity each week, it seems unrealistic to expect that people will replace sitting with even more exercise.
With this in mind, scientists at the University of Utah School of Medicine investigated the health benefits of a more achievable goal, trading sitting for lighter activities for short periods of time. They used observational data from National Health and Nutrition Examination Survey (NHANES) to examine whether longer durations of low intensity activities (e.g. standing), and light intensity activities (e.g. casual walking, light gardening, cleaning) extends the life span of people who are sedentary for more than half of their waking hours.
They found that there is no benefit to decreasing sitting by two minutes each hour, and adding a corresponding two minutes more of low intensity activities. However, a "trade-off" of sitting for light intensity activities for two minutes each hour was associated with a 33 percent lower risk of dying.
"It was fascinating to see the results because the current national focus is on moderate or vigorous activity. To see that light activity had an association with lower mortality is intriguing," says lead author Srinivasan Beddhu, M.D., professor of internal medicine.
Beddhu explains that while it's obvious that it takes energy to exercise, strolling and other light activities use energy, too. Even short walks add up to a lot when repeated many times over the course of a week. Assuming 16 awake hours each day, two minutes of strolling each hour expends 400 kcal each week. That number approaches the 600 kcal it takes to accomplish the recommended weekly goal of moderate exercise. It is also substantially larger than the 50 kcal needed to complete low intensity activities for two minutes each awake hour over the course of one week.
"Based on these results we would recommend adding two minutes of walking each hour in combination with normal activities, which should include 2.5 hours of moderate exercise each week," says Beddhu. Moderate exercise strengthens the heart, muscles, and bones, and confers health benefits that low and light intensity activities can't.
The study examined 3,243 NHANES participants who wore accelerometers that objectively measured the intensities of their activities. Participants were followed for three years after the data were collected; there were 137 deaths during this period.
"Exercise is great, but the reality is that the practical amount of vigorous exercise that can be achieved is limited. Our study suggests that even small changes can have a big impact," says senior author Tom Greene, Ph.D., director of the Study Design and Biostatistics Center at the Center for Clinical and Translational Science.
Beddhu adds that large, randomized, interventional trials will be needed to definitively answer whether exchanging sitting for light activities leads to better health.
Story Source:
The above story is based on materials provided by University of Utah Health Sciences . Note: Materials may be edited for content and length.
Journal Reference:
Srinivasan Beddhu,     Guo Wei,     Robin L. Marcus,     Michel Chonchol,     Tom Greene. Light-Intensity Physical Activities and Mortality in the United States General Population and CKD Subpopulation. CJASN, April 30, 2015 DOI: 10.2215/%u200BCJN.08410814
Cite This Page:
</MainBody>
    </Article>
    <Article id="16">
        <date>Thu Apr 30 22:52:42 EEST 2015</date>
        <title>Dwindling productivity in Congress linked to vanishing cooperation</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/Yq0cYftAL90/150430155242.htm</Link>
        <Description>As the number of bills passed by Congress declines, fewer and fewer Congressional representatives are voting across party lines, leaving only a few key representatives as collaborative voters, according to researchers.</Description>
        <MainBody>Dwindling productivity in Congress linked to vanishing cooperation
Date:
Penn State
Summary:
As the number of bills passed by Congress declines, fewer and fewer Congressional representatives are voting across party lines, leaving only a few key representatives as collaborative voters, according to researchers.
Share:
Total shares: 
FULL STORY
These two graphics highlight how the number of interactions between cross-party pairs has decreased drastically from 1949 to 2011. Each node represents a member of the U.S. House of Representatives (red are Republicans; blue are Democrats), with lines between nodes if Congress members agreed on a number of bills above the threshold, which is the value at which a given pair is equally likely to be comprised of two members of the same party (e.g. D-D or R-R) or a cross-party pair (e.g. D-R).
Credit: Clio Andris
These two graphics highlight how the number of interactions between cross-party pairs has decreased drastically from 1949 to 2011. Each node represents a member of the U.S. House of Representatives (red are Republicans; blue are Democrats), with lines between nodes if Congress members agreed on a number of bills above the threshold, which is the value at which a given pair is equally likely to be comprised of two members of the same party (e.g. D-D or R-R) or a cross-party pair (e.g. D-R).
Credit: Clio Andris
Close
As the number of bills passed by Congress declines, fewer and fewer Congressional representatives are voting across party lines, leaving only a few key representatives as collaborative voters, according to researchers.
"We can't say for sure that the decline in cooperation is the sole reason that there are fewer bills being introduced or passed by Congress, but we do know the two are statistically correlated, and both have been dropping steadily over the past 60 years," said Clio Andris, lead author and assistant professor of geography at Penn State.
The researchers tracked roll call vote data -- the record of whether Congressional representatives abstained or voted "yay" or "nay" on motions or bills -- on every vote, beginning in 1949 -- the start of the 81st Congress -- and continuing through 2012 -- the end of the 112th Congress. The researchers then identified all times when a representative voted with members of the opposite party, referred to as a cross-party pair, or with members of the same party, referred to as a same-party pair.
The findings show that the number of cross-party votes decreased at an exponentially higher rate than same-party votes. From 1967 to 1979, there were more than 10,000 instances of representatives voting across party lines. From 2001 to 2010, there were fewer than 1,500 of these cooperative votes.
The findings also show that far fewer representatives today engage in cross-party voting than in the past. "If you look at past data, it was uncharacteristic that one representative would be involved in even 1 percent of the total cross-party voting, because people were more likely to vote against party lines. Today, several people account for upwards of 50 percent of the total cross-party votes, and these are the people we refer to as 'super-cooperators' because they account for a bulk of the cross-aisle voting," said Andris.
In the 112th Congress, 7 of the 444 members accounted for 98.3 percent of cross-party pairs, which indicates that the majority of representatives vote with their party members only. This study was the first to analyze congressional role call voting using a network model, and this approach allowed the researchers to identify patterns at the individual level.
"Our original goal was to try to identify 'hidden partnerships' among Congressional representatives and to understand whether geography played a role in any of those bonds. Surprisingly, we found that geography had very little influence over how representatives voted. You might think that issues around land resource management or tourism might be affected by local geographies, but that's not the case," said Andris. "What we're seeing now is that, more and more, party platforms are the determining factor for how the majority of representatives are voting."
Story Source:
The above story is based on materials provided by Penn State . The original article was written by Liam Jackson. Note: Materials may be edited for content and length.
Journal Reference:
Clio Andris, David Lee, Marcus J. Hamilton, Mauro Martino, Christian E. Gunning, John Armistead Selden. The Rise of Partisanship and Super-Cooperators in the U.S. House of Representatives. PLOS ONE, 2015; 10 (4): e0123507 DOI: 10.1371/journal.pone.0123507
Cite This Page:
</MainBody>
    </Article>
    <Article id="17">
        <date>Thu Apr 30 22:52:40 EEST 2015</date>
        <title>California's 4.8 million low-wage workers now earn less than in 1979</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/fPtYSfB9xjk/150430155240.htm</Link>
        <Description>Over the past 35 years, California's high-wage workers have seen steady increases in their paychecks. But low-wage workers, 4.8 million strong and about one-third of the state's workforce, earned less in inflation-adjusted dollars in 2014 than they did in 1979, according to an analysis.</Description>
        <MainBody>University of California - Berkeley
Summary:
Over the past 35 years, California's high-wage workers have seen steady increases in their paychecks. But low-wage workers, 4.8 million strong and about one-third of the state's workforce, earned less in inflation-adjusted dollars in 2014 than they did in 1979, according to an analysis.
Share:
Total shares: 
FULL STORY
Over the past 35 years, California's high-wage workers have seen steady increases in their paychecks. But low-wage workers, 4.8 million strong and about one-third of the state's workforce, earned less in inflation-adjusted dollars in 2014 than they did in 1979, according to a new analysis from UC Berkeley.
Berkeley researchers analyzing U.S. Census Bureau data at the campus's Center for Labor Research and Education found that low-wage workers, defined as those earning hourly wages of $13.63 or less, have seen steady declines in their inflation-adjusted buying power. This low-wage workforce, nearly three-quarters nonwhite and concentrated in two industries -- retail trade, and restaurants and other food services -- has also become older and more highly educated.
Teens made up 5 percent of low-wage workers in 2014, down from 16 percent in 1979, and 48 percent of low-wage earners in 2014 had attended some college, compared to 39 percent in 1979. The analysis also showed that 40 percent of the state's low-wage workers in 2014 were foreign-born.
"We found that low-wage workers in California are older and more educated than they were 30 years ago, and yet they've seen stagnant and even declining wages," said Annette Bernhardt, a visiting UC Berkeley professor of sociology and a senior researcher at the center. "The story of growing inequality is not just about the top 1 percent, it is also about the millions of low-wage workers and their families who struggle with economic insecurity every day."
In 2013, the median income of low-wage workers' families was $29,100, compared to $63,000 for all California families, a gap that has widened since 2000. From 2007 to 2011, families of low-wage workers received $14.3 billion in annual support from public-assistance programs such as the Earned Income Tax Credit, Medicaid and food stamps.Bernhardt and the study's other authors, Ian Perry and Lindsay Cattell, found that the top occupations of California's low-wage workers are retail sales workers; cooks and food preparation workers; material-moving workers; and personal care and childcare workers. About half are in Southern California.
The researchers published their analysis in chart form in Low-Wage Work in California: 2014 Chartbook, the first edition of an ongoing resource with a wide range of information about low-wage workers, their families and their jobs. The chartbook will be updated annually as new census data becomes available. It is available online at: http://laborcenter.berkeley.edu/lowwageca/
Story Source:
The above story is based on materials provided by University of California - Berkeley . The original article was written by Tom Levy. Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="18">
        <date>Thu Apr 30 22:52:38 EEST 2015</date>
        <title>MarkerMiner 1.0: An easy-to-use bioinformatics platform for DNA analysis in angiosperms</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/6Tps6uwkTkc/150430155238.htm</Link>
        <Description>Researchers have developed MarkerMiner, a new software that simplifies analysis of next-generation sequencing data in angiosperms. MarkerMiner is an automated, open-source, bioinformatics workflow that aids plant researchers in the discovery of single-copy nuclear genes. The software is easy to use, offers a multipurpose, configurable output, and is accessible to users with limited bioinformatics training or without access to computing resources.</Description>
        <MainBody>Botanical Society of America
Summary:
Researchers have developed MarkerMiner, a new software that simplifies analysis of next-generation sequencing data in angiosperms. MarkerMiner is an automated, open-source, bioinformatics workflow that aids plant researchers in the discovery of single-copy nuclear genes. The software is easy to use, offers a multipurpose, configurable output, and is accessible to users with limited bioinformatics training or without access to computing resources.
Share:
Total shares: 
FULL STORY
Flowering plants, also known as angiosperms, add an allure to the world that is unlike anything else in nature, but more importantly, they sustain us. Most of the fruits, vegetables, grains, beans, nuts, and even herbs and spices that we consume are produced by flowering plants. They all belong to the green plant branch of the tree of life, and a novel DNA analysis software program named MarkerMiner facilitates identification of genes that can be used to elucidate the evolutionary relationships between them.
University of Florida (UF) biologist Srikar Chamala, working with UF Genetic Institute faculty Brad Barbazuk, Pamela Soltis, and Douglas Soltis, along with a team of researchers from the United States, Chile, and Belgium, designed MarkerMiner to facilitate greater progress in angiosperm phylogenetics. There are currently over 300,000 species of angiosperms, within which many finer evolutionary details are unknown. The software will help researchers discover genes useful for inferring evolutionary patterns within plants that appear superficially similar, by aiding close comparisons of their genes.
"MarkerMiner works by filtering through large amounts of assembled transcriptome sequence data [the gene-encoding portion of the genome] to identify single-copy nuclear loci, by making use of conserved low-copy nuclear gene information available from sequenced angiosperm genomes," explains Dr. Chamala, who led the study. "Because gene duplications are very frequent in angiosperms, the putative single-copy status of the genes targeted by MarkerMiner makes them great genetic markers for resolving angiosperm phylogenies."
Despite advances in DNA sequencing technology, analyzing genetic sequence data can be expensive, time-consuming, and a bottleneck for many researchers. MarkerMiner eliminates those constraints by providing an easy-to-use platform, which researchers across the globe with limited bioinformatics training and limited access to high-performance computing resources can leverage to produce phylogenetic markers for any angiosperm group of interest.
A full description of MarkerMiner is available in a recent issue of Applications in Plant Sciences. The study includes a test in which hundreds of single-copy nuclear loci from four different flowering plant groups were successfully identified. A user manual including source code is publicly available at https://bitbucket.org/srikarchamala/markerminer .
Mohammad Vatanparast, a researcher at the Smithsonian Institution says, "I'm glad that I found MarkerMiner. I'm using it to pull up single-copy orthologous genes from transcriptomes to perform phylogenomic analysis across phaseoloid legumes, a group that contains important species like soybean and common bean. MarkerMiner has great potential to facilitate next-generation sequence research and makes locus selection a lot easier."
Norman A. Douglas, a researcher at Oberlin College, is using MarkerMiner to develop loci for multilocus phylogeography across diverse lineages of gypsum-endemic plants in the Chihuahuan Desert, located in Mexico and the southwestern United States.
"Our first effort took several months of work to identify likely orthologs of single-copy loci in lineages that have few genomic resources, like Nyctaginaceae, Boraginaceae, and some isolated genera in Asteraceae. MarkerMiner simplifies the process greatly, and provides output in several useful formats. I am confident that this pipeline will find wide use in the phylogeography community," says Douglas.
MarkerMiner could soon empower scientists worldwide by making angiosperm research in fields of study such as ecology, evolution, systematics, phylogeography, and population biology more efficient and economical.
Story Source:
The above story is based on materials provided by Botanical Society of America . Note: Materials may be edited for content and length.
Journal Reference:
Srikar Chamala, Nicols Garca, Grant T. Godden, Vivek Krishnakumar, Ingrid E. Jordon-Thaden, Riet De Smet, W. Brad Barbazuk, Douglas E. Soltis, Pamela S. Soltis. MarkerMiner 1.0: A New Application for Phylogenetic Marker Development Using Angiosperm Transcriptomes. Applications in Plant Sciences, 2015; 3 (4): 1400115 DOI: 10.3732/apps.1400115
Cite This Page:
</MainBody>
    </Article>
    <Article id="19">
        <date>Thu Apr 30 22:52:36 EEST 2015</date>
        <title>Higher levels of inattention at age seven linked with lower final high school exam grades</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/yLRh8WD6iuY/150430155236.htm</Link>
        <Description>New research has shown that children who display increasing levels of inattention at the age of seven are at risk of worse academic outcomes in their GCSE examinations (UK).</Description>
        <MainBody>Higher levels of inattention at age seven linked with lower final high school exam grades
Date:
University of Nottingham
Summary:
New research has shown that children who display increasing levels of inattention at the age of seven are at risk of worse academic outcomes in their GCSE examinations (UK).
Share:
Total shares: 
FULL STORY
New research has shown that children who display increasing levels of inattention at the age of seven are at risk of worse academic outcomes in their GCSE examinations (high school final exams in U.K.).
Researchers at the Universities of Nottingham and Bristol studied more than 11,000 children as part of the research which was funded by the Economic and Social Research Council (ESRC) and is published in the Journal of the American Academy of Child and Adolescent Psychiatry.
The findings of the research have a range of implications for parents, teachers and clinicians.
The research was led by Kapil Sayal, Professor of Child and Adolescent Psychiatry in the School of Medicine at The University of Nottingham. Professor Sayal said: "Teachers and parents should be aware of the long-term academic impact of behaviours such as inattention and distractibility. The impact applies across the whole spectrum of scores at the population level and is not just confined to those scoring above a cut-off or at the extreme end.
"Prevention and intervention strategies are key and, in the teenage years, could include teaching students time-management and organisational skills, minimising distractions and helping them to prioritise their work and revision."
The results of the study are based on the analysis of behavioural and academic data of participants in Children of the 90s, a population-based study at The University of Bristol.
Parents and teachers completed detailed questionnaires when the children were seven years old to assess a variety of different behaviours including inattention, hyperactivity/impulsivity and oppositional/defiant problems. This information was compared with the children's academic achievements by looking at their GCSE results at age 16.
After taking into account factors such as IQ and parental education and social class, the researchers found that for every one-point increase in inattention symptoms at age seven, across the whole sample, there was a two to three point reduction in GCSE scores and a 6 to 7 per cent increased likelihood of not achieving a minimum level of five 'good' GCSE grades (A to C) at age 16. This relationship was linear -- each one-point increase in inattention symptoms increased the risk of worse academic outcomes across the full range of inattention scores in the sample.
When the researchers took inattention into account, the study also found that, in boys, oppositional/defiant behaviours at age seven pose an independent risk to academic achievement.
Story Source:
The above story is based on materials provided by University of Nottingham . Note: Materials may be edited for content and length.
Journal Reference:
Kapil Sayal, Elizabeth Washbrook, Carol Propper. Childhood Behavior Problems and Academic Outcomes in Adolescence: Longitudinal Population-Based Study. Journal of the American Academy of Child &amp; Adolescent Psychiatry, 2015; 54 (5): 360 DOI: 10.1016/j.jaac.2015.02.007
Cite This Page:
</MainBody>
    </Article>
    <Article id="20">
        <date>Thu Apr 30 22:50:57 EEST 2015</date>
        <title>Internet as new frontier in collecting data on the mind</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/5Cj2f_3oeY0/150430155057.htm</Link>
        <Description>With Apple's launch of new health tracking tools for the iPhone and medical researchers' forays into Facebook to recruit clinical trial volunteers, Web and mobile apps are increasingly seen as a new source for health data. But psychologists are also looking to the Internet as a new source of information about the mind.</Description>
        <MainBody>Internet as new frontier in collecting data on the mind
Date:
Indiana University
Summary:
With Apple's launch of new health tracking tools for the iPhone and medical researchers' forays into Facebook to recruit clinical trial volunteers, Web and mobile apps are increasingly seen as a new source for health data. But psychologists are also looking to the Internet as a new source of information about the mind.
Share:
Total shares: 
FULL STORY
With Apple's launch of new health tracking tools for the iPhone and medical researchers' forays into Facebook to recruit clinical trial volunteers, Web and mobile apps are increasingly seen as a new source for health data.
But psychologists are also looking to the Internet as a new source of information about the mind -- and an Indiana University researcher is on the forefront of those developing the tools to make it happen.
Josh de Leeuw, a graduate student in the IU Bloomington College of Arts and Sciences' Department of Psychological and Brain Sciences, is the creator of jsPsych, a free open-source software platform that employs a common Web technology to conduct psychology experiments over the Internet. The program allows psychologists without significant programming skills to deliver tasks common to research on the mind through a Web browser.
"Conducting psychology research online is appealing for a number of reasons: faster data collection, lower costs and improved anonymity of subjects and experimenters," de Leeuw said. "Internet users are more demographically diverse than the population conventionally sampled for behavioral research in psychology -- a fact that could contribute to results that better reflect the population at large."
This means the current pool of psychology research volunteers -- generally, college students -- could expand since participants would no longer need to travel to a researcher's lab to perform tasks common in psychological research. Using jsPsych, a psychologist could present anyone with Internet access with on-screen stimuli and then collect responses. Study instructions and consent forms could also be delivered electronically.
"Online experimentation is the bleeding edge of psychology research, and many believe online studies will play an important role in addressing the so-called 'replication crisis' in psychology," said Ben Motz, senior lecturer and director of undergraduate instruction in the Department of Psychological and Brain Sciences, who has collaborated with de Leeuw. "Josh's work is at the epicenter of this -- and it's all the more impressive that he's developed this platform as a graduate student."
The replication crisis refers to a growing concern in experimental psychology -- and the larger scientific community -- about the drop in studies able to confirm previous work with experiments that achieve the same results using the same methods, as well as the increased risk of data manipulation in studies with small sample sizes.
Reducing costs and other barriers to psychological research could increase the number of replication studies, as well as the overall number of volunteers and data produced, in turn creating stronger results.
A few psychology researchers are already using online services to conduct research. One of those tools is Amazon Mechanical Turk, a service from the retail giant that enables anyone to hire online workers to perform short, simple tasks.
Others remain hesitant to employ these methods due to worries about the quality of data collected outside the highly controlled environment of a research lab. To address this concern, de Leeuw recently published a study in the journal Behavior Research Methods investigating whether one specific issue among brain researchers -- the potential delays in response introduced by the use of online technology -- represents a significant barrier to conducting research online.
That study looked at differences between Matlab's Psychophysics Toolbox -- one of the most popular software systems for psychological experiments -- and jsPsych.
To measure the speed that information is displayed on-screen and its effect on reaction time, de Leeuw asked 30 volunteers to sit in front of a projection screen and hold a thumb-activated switch in each hand. If volunteers saw the letter "N," they clicked a button with their dominant hand. If they did not see the letter, they clicked with their other hand. Volunteers repeated this experiment 400 times, with only a 45-second break in the middle.
To prevent weariness from affecting the results, de Leeuw set up the screen to project two images simultaneously -- one generated by jsPsych, the other by the traditional system -- enabling response times to both systems to be measured simultaneously.
In psychology research, response times may be used to discover which mental operations are more difficult than others, or to measure how uncertain people are about a decision. They can help distinguish between different theories of perception, memory, decision-making and social cognition. Psychologists refer to the study of the relationships between stimulus and response as "psychophysics." Measuring response times is only one of many applications of jsPsych.
The study revealed that jsPsych produced delays of 10 to 40 milliseconds compared to laboratory-based software. Three other statistical parameters measured showed no reliable difference between the systems.
"This is an extremely small lag time -- it's roughly the same as might be caused by other variations in experimental setup, such as a change in keyboard, mouse or monitor -- and it is constant between different experimental conditions," de Leeuw said. "In the context of a real human experiment, both systems are nearly identical."
He added that results are also applicable to other software packages that use JavaScript, the only programming language supported by all major Web browsers that does not require additional software to run.
"Hopefully this study will convince more people that you can, in fact, conduct psychophysics in a Web browser," de Leeuw said. "We're looking at technology with potential to transform how research on the brain gets done."
In addition to Motz, IU researchers contributing to the study were Richard Viken, Chris Eller, Michael Bailey, John Kruschke, Tony Walker and Alex Shroyer. Equipment and technical assistance were provided by the IU Advanced Visualization Laboratory. This research was supported by a National Science Foundation Graduate Research Fellowship Grant (DGE-1342962).
A tutorial on jsPsych is available online . De Leeuw will also deliver a tutorial on the use of the program at the 2015 Cognitive Science Society meeting July 22.
Story Source:
The above story is based on materials provided by Indiana University . Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="21">
        <date>Thu Apr 30 22:50:53 EEST 2015</date>
        <title>Engineering a better solar cell: Defects in popular perovskites pinpointed</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/Rjt8LbxVWXE/150430155053.htm</Link>
        <Description>A new study demonstrates that perovskite materials - superefficient crystal structures that have recently taken the scientific community by storm - contain flaws that can be engineered to improve solar cells and other devices even further.</Description>
        <MainBody>University of Washington
Summary:
A new study demonstrates that perovskite materials - superefficient crystal structures that have recently taken the scientific community by storm - contain flaws that can be engineered to improve solar cells and other devices even further.
Share:
Total shares: 
FULL STORY
UW researchers used microscopy to identify inefficient regions in perovskite materials used in solar cells, as evidenced by dark areas in C.
Credit: University of Washington
UW researchers used microscopy to identify inefficient regions in perovskite materials used in solar cells, as evidenced by dark areas in C.
Credit: University of Washington
Close
One of the fastest-growing areas of solar energy research is with materials called perovskites. These promising light harvesters could revolutionize the solar and electronics industries because they show potential to convert sunlight into electricity more efficiently and less expensively than today's silicon-based semiconductors.
These superefficient crystal structures have taken the scientific community by storm in the past few years because they can be processed very inexpensively and can be used in applications ranging from solar cells to light-emitting diodes (LEDs) found in phones and computer monitors.
A new study published online April 30 in the journal Science by University of Washington and University of Oxford researchers demonstrates that perovskite materials, generally believed to be uniform in composition, actually contain flaws that can be engineered to improve solar devices even further.
"Perovskites are the fastest-growing class of photovoltaic material over the past four years," said lead author Dane deQuilettes, a UW doctoral student working with David Ginger, professor of chemistry and associate director of the UW's Clean Energy Institute.
"In that short amount of time, the ability of these materials to convert sunlight directly into electricity is approaching that of today's silicon-based solar cells, rivaling technology that took 50 years to develop," deQuilettes said. "But we also suspect there is room for improvement."
The research team used high-powered imaging techniques to find defects in the perovskite films that limit the movement of charges and, therefore, limit the efficiency of the devices. Perovskite solar cells have so far have achieved efficiencies of roughly 20 percent, compared to about 25 percent for silicon-based solar cells.
In a collaboration made possible by the Clean Energy Institute, the team used a technique called confocal optical microscopy, which is more often used in biology, and applied it to semiconductor technology. They used fluorescent images and correlated them with electron microscopy images to find "dark" or poorly performing regions of the perovskite material at intersections of the crystals. In addition, they discovered that they could "turn on" some of the dark areas by using a simple chemical treatment.
The images offered several surprises but also will lead to accelerated improvements in the materials' uniformity, stability and efficiency, according to corresponding author Ginger, the Alvin L. and Verla R. Kwiram Endowed Professor of Chemistry and Washington Research Foundation Distinguished Scholar.
"Surprisingly, this result shows that even what are being called good, or highly-efficient perovskite films today still are 'bad' compared to what they could be. This provides a clear target for future researchers seeking to improve and grow the materials," Ginger said.
The imaging technique developed by the UW team also offers an easy way to identify previously undiscovered flaws in perovskite materials and to pinpoint areas where their composition can be chemically altered to boost performance, Ginger said.
deQuilettes, who spearheaded the project as a Clean Energy Institute graduate fellow, estimates there are more than a thousand laboratories around the world currently researching the semiconducting properties of perovskite materials. Yet there is more work to be done to understand how to consistently make a material that is stable, has uniform brightness and can stand up to moisture without degrading. The UW research offers new ways for people to think strategically about how to improve the materials and how to extend their applications to high performance light-emitting devices such as LEDs and lasers.
"There are so many of us focusing on perovskites, so hopefully this technique will offer some new direction and steer us toward the places we can look to optimize their energy-capturing and emitting potential," deQuilettes said.
Co-authors of the study are Sarah M. Vorpahl, Hirokazu Nagaoka and Mark E. Ziffer of the UW and Samuel D. Stranks, Giles E. Eperon and Henry J. Snaith at Oxford.
Funding for the research was provided by the state of Washington through the UW Clean Energy Institute.
Story Source:
The above story is based on materials provided by University of Washington . The original article was written by Renee Gastineau. Note: Materials may be edited for content and length.
Journal Reference:
Dane W. Dequilettes, Sarah M. Vorpahl, Samuel D. Stranks, Hirokazu Nagaoka, Giles E. Eperon, Mark E. Ziffer, Henry J. Snaith, David S. Ginger. Impact of microstructure on local carrier lifetime in perovskite solar cells. Science, 2015 DOI: 10.1126/science.aaa5333
Cite This Page:
</MainBody>
    </Article>
    <Article id="22">
        <date>Thu Apr 30 22:50:48 EEST 2015</date>
        <title>Metal contamination makes gasoline production inefficient</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/u-9xUBhP-78/150430155048.htm</Link>
        <Description>Scientists have identified key mechanisms of the aging process of catalyst particles that are used to refine crude oil into gasoline. This advance could lead to more efficient gasoline production.</Description>
        <MainBody>Metal contamination makes gasoline production inefficient
Date:
SLAC National Accelerator Laboratory
Summary:
Scientists have identified key mechanisms of the aging process of catalyst particles that are used to refine crude oil into gasoline. This advance could lead to more efficient gasoline production.
Share:
Total shares: 
FULL STORY
SSRL X-rays are focused to illuminate a small sample of catalysts inside a movable cylindrical holder. A lens magnifies the resulting sample image onto a screen, a CCD camera captures the 2-D image, and software is used to reconstruct a 3-D image of the single catalyst particle from a series of these 2-D images.
Credit: Florian Meirer/Utrecht University
SSRL X-rays are focused to illuminate a small sample of catalysts inside a movable cylindrical holder. A lens magnifies the resulting sample image onto a screen, a CCD camera captures the 2-D image, and software is used to reconstruct a 3-D image of the single catalyst particle from a series of these 2-D images.
Credit: Florian Meirer/Utrecht University
Close
Scientists at the Department of Energy's SLAC National Accelerator Laboratory and Utrecht University have identified key mechanisms of the aging process of catalyst particles that are used to refine crude oil into gasoline. This advance could lead to more efficient gasoline production.
Their recent experiments studied so-called fluid catalytic cracking (FCC) particles that are used to break long-chain hydrocarbons in crude oil into smaller, more valuable hydrocarbons like gasoline.
"A major problem is that these catalysts quickly age and lose their activity, so tons of fresh catalysts have to be added to a reactor system every day," said lead researcher Florian Meirer, assistant professor of inorganic chemistry and catalysis at Utrecht University in the Netherlands. "We are trying to understand how this aging happens, and we're working with companies that produce these FCC catalysts to make the process more efficient."
In experiments using X-ray beams at SLAC's Stanford Synchrotron Radiation Lightsource (SSRL), a DOE Office of Science User Facility, the researchers studied FCC catalysts of various ages to better understand the effects of aging. They were able to image whole catalyst particles with high resolution so they could also see the catalysts' internal structure -- like taking a panoramic landscape photograph where you can zoom in to see the ants.
"We have been able to localize the metal poisons that are a leading cause of catalyst aging and also determine how they influence the materials," said Bert Weckhuysen, professor of inorganic chemistry and catalysis at Utrecht University. "It's been studied in the past, but not at this resolution and not on the single particle level. That is the beauty of what we have done."
Casting Light on Metals
The problem of catalyst aging is widespread and costly. Worldwide, about 400 reactor systems refine crude oil into gasoline, and each system requires 10 to 40 tons of fresh FCC catalysts daily.
Crude oil is contaminated with metals, mostly iron, nickel and vanadium. These metals accumulate in catalysts during refining and eventually deactivate them. This is particularly an issue for low-quality crude oil, the largest available oil supply.
"Cheaper oil is often more polluted with metal poisons," said Weckhuysen. "If we want to use cheap oil as feedstock, then we have to improve FCC catalysts to make them more resistant against metal poisons."
In the SSRL study, the research team took a series of two-dimensional images of catalyst particles at various angles and used software they developed to combine them into three-dimensional images of whole particles. These images show the 3-D distribution of iron and nickel in catalysts of various ages.
"This 'mosaic imaging' technique is a major reason why we do our experiments at the SSRL," Meirer said.
Designing Better 'Catalyst Highways'
The researchers determined that the metals quickly accumulate on the outer surface of a catalyst, blocking the crude oil molecules from traveling through catalyst pores to reach deeper into its still-active core. They also showed that some catalyst particles stick together in clusters, which disturbs the fluidity of the catalysts and lowers gasoline production yields.
The Utrecht researchers are already working with companies to redesign these FCC catalysts.
"All of our conclusions are about the macropores, which are like highways for crude oil traveling through the catalysts. It's about blockage along Highway 101 or 280, not small streets in San Francisco," explained Weckhuysen. "Can we design better catalyst pores or highways?"
The research team also included Sam Kalirai at Utrecht and Darius Morris, Yijin Liu and Joy C. Andrews at SSRL. This research was supported by the NWO Gravitation program, Netherlands Center for Multiscale Catalytic Energy Conversion, Netherlands Research School Combination-Catalysis and a European Research Council Advanced Grant.
Story Source:
The above story is based on materials provided by SLAC National Accelerator Laboratory . Note: Materials may be edited for content and length.
Journal References:
Florian Meirer, Darius T. Morris, Sam Kalirai, Yijin Liu, Joy C. Andrews, Bert M. Weckhuysen. Mapping Metals Incorporation of a Whole Single Catalyst Particle Using Element Specific X-ray Nanotomography. Journal of the American Chemical Society, 2015; 137 (1): 102 DOI: 10.1021/ja511503d
F. Meirer, S. Kalirai, J. Nelson Weker, Y. Liu, J. C. Andrews, B. M. Weckhuysen. Agglutination of single catalyst particles during fluid catalytic cracking as observed by X-ray nanotomography. Chem. Commun., 2015; 51 (38): 8097 DOI: 10.1039/c5cc00401b
F. Meirer, S. Kalirai, D. Morris, S. Soparawalla, Y. Liu, G. Mesu, J. C. Andrews, B. M. Weckhuysen. Life and death of a single catalytic cracking particle. Science Advances, 2015; 1 (3): e1400199 DOI: 10.1126/sciadv.1400199
Cite This Page:
</MainBody>
    </Article>
    <Article id="23">
        <date>Thu Apr 30 21:51:25 EEST 2015</date>
        <title>New origin theory for cells that gave rise to vertebrates</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/YdcwijEkDq4/150430145125.htm</Link>
        <Description>Zebras' vivid pigmentation and the fight or flight instinct. These and other features of the world's vertebrates stem from neural crest cells, but little is known about their origin. Scientists propose a new model for how neural crest cells, and thus vertebrates, arose more than 500 million years ago. They report that these cells retain the molecular underpinnings that control pluripotency -- the ability to give rise to all the cell types that make up the body.</Description>
        <MainBody>New origin theory for cells that gave rise to vertebrates
Date:
Northwestern University
Summary:
Zebras' vivid pigmentation and the fight or flight instinct. These and other features of the world's vertebrates stem from neural crest cells, but little is known about their origin. Scientists propose a new model for how neural crest cells, and thus vertebrates, arose more than 500 million years ago. They report that these cells retain the molecular underpinnings that control pluripotency -- the ability to give rise to all the cell types that make up the body.
Share:
Total shares: 
FULL STORY
Rendering of cells (stock image). Scientists propose a new model for how neural crest cells, and thus vertebrates, arose more than 500 million years ago.
Credit:  sakkmesterke / Fotolia
Rendering of cells (stock image). Scientists propose a new model for how neural crest cells, and thus vertebrates, arose more than 500 million years ago.
Credit:  sakkmesterke / Fotolia
Close
The vivid pigmentation of zebras, the massive jaws of sharks, the fight or flight instinct and the diverse beaks of Darwin's finches. These and other remarkable features of the world's vertebrates stem from a small group of powerful cells, called neural crest cells, but little is known about their origin.
Now Northwestern University scientists propose a new model for how neural crest cells, and thus vertebrates, arose more than 500 million years ago.
The researchers report that, unlike other early embryonic cells that have their potential progressively restricted as an embryo develops, neural crest cells retain the molecular underpinnings that control pluripotency -- the ability to give rise to all the cell types that make up the body.
"This study provides deep new insights into the evolutionary origins of humans and other vertebrates," said evolutionary molecular biologist Carole LaBonne, who led the research. "It also provides critical new information about the molecular circuitry of stem cells, including cancer stem cells."
Regenerative medicine scientists now have an updated framework for future studies aiming to harness the power of stem cells to treat human diseases and congenital defects, LaBonne said.
The study also turns conventional thought on its head. Previously, scientists thought neural crest cells had to evolve to gain their incredible properties, but the Northwestern work shows the power was there all along. Researchers now can focus on the molecular mechanisms by which neural crest cells escaped having their potential restricted.
In a study using embryos from the frog Xenopus, a powerful model system used in studies of development, LaBonne and her team found that neural crest cells and the early pluripotent cells present in blastula embryos have surprising similarities, including shared expression of a key set of genes which work together to endow the cells with their unique properties.
The findings will be published as a Science Express article by the journal Science. The article also will be the cover story of the journal's June 19 issue.
"Neural crest cells never had their potential restricted at all," LaBonne said. "We believe a small population of early stem cells were set aside, so that when the time came, their immense developmental potential could be unleashed to create new features characteristic of vertebrates."
LaBonne is a professor of molecular biosciences in the Weinberg College of Arts and Sciences. She holds the Arthur Andersen Teaching and Research Chair and is co-leader of the Tumor Invasion and Metastasis program of the Robert H. Lurie Comprehensive Cancer Center of Northwestern University.
Acquisition of neural crest cells more than 500 million years ago led vertebrates to evolve and leave behind less complex life forms (simple aquatic filter feeders, much like today's sea squirts and lancelets). With these cells, animals developed important new features such as a skull to house a complex brain, jaws for predation, a complex peripheral nervous system and many other cell types essential to the vertebrate body.
In their study, LaBonne and her research team studied the genetic toolkit that early embryonic cells use to promote pluripotency or "stemness" and compared it to the one used by neural crest cells. They found that the toolkit used by neural crest cells also is used by pluripotent blastula cells, and they showed that it is essential for pluripotency in both cell types. The proteins that derive from this toolkit work together to enable a dizzying array of tissues to arise from a population of single cells.
One of these proteins, called Snail1, has been the focus of previous studies by LaBonne's lab. They and others had shown that Snail1 plays key roles in controlling not only the immense developmental potential of neural crest cells but also their capacity for migratory and invasive behavior.
Cancer cells co-opt the function of Snail1 and other neural crest regulatory proteins to allow the formation of cancer stem cells and mediate the process of metastasis, whereby cancer cells disperse throughout the body to form new tumors, LaBonne said. Researchers therefore gain insights into Snail1's role in cancer by studying its function in the developing embryo.
In early blastula embryos, pluripotent cells were thought to exist only transiently; as an embryo develops, cells become restricted into categories of cells called germ layers and then into specialized cell types. The Northwestern study suggests that not all cells get restricted at those early stages. Instead, neural crest cells may have evolved as a consequence of a subset of blastula cells retaining activity of the regulatory network underlying pluripotency.
The study underscores just how much remains to be discovered about embryonic development. The human body has more than 10 trillion cells elaborately organized into tissues and organs that are intricate and highly complex, yet it all is self-assembled from a single cell, the fertilized egg.
"It's a fascinating process," LaBonne said. "One of the great frontiers in biology is understanding both how complexity is generated and how it evolves to create what Charles Darwin memorably called 'endless forms most beautiful.'"
Story Source:
The above story is based on materials provided by Northwestern University . Note: Materials may be edited for content and length.
Journal Reference:
Elsy Buitrago-Delgado, Kara Nordin, Anjali Rao, Lauren Geary, and Carole LaBonne. Shared Regulatory Programs Suggest Retention of Blastula-Stage Potential in Neural Crest Cells. Science, April 2015 DOI: 10.1126/science.aaa3655
Cite This Page:
</MainBody>
    </Article>
    <Article id="24">
        <date>Thu Apr 30 21:51:22 EEST 2015</date>
        <title>How some beetles produce a scalding defensive spray</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/J8NRsItFUGU/150430145122.htm</Link>
        <Description>Bombardier beetles, which exist on every continent except Antarctica, have a pretty easy life. Virtually no other animals prey on them, because of one particularly effective defense mechanism: When disturbed or attacked, the beetles produce an internal chemical explosion in their abdomen and then expel a jet of boiling, irritating liquid toward their attackers.</Description>
        <MainBody>How some beetles produce a scalding defensive spray
Date:
Massachusetts Institute of Technology
Summary:
Bombardier beetles, which exist on every continent except Antarctica, have a pretty easy life. Virtually no other animals prey on them, because of one particularly effective defense mechanism: When disturbed or attacked, the beetles produce an internal chemical explosion in their abdomen and then expel a jet of boiling, irritating liquid toward their attackers.
Share:
Total shares: 
FULL STORY
Bombardier beetles eject a liquid called benzoquinone, which they superheat and expel in an intense, pulsating jet. The explosive mechanism used by the beetle generates a spray that's much hotter than that of other insects that use the liquid, and propels the jet five times faster.
Credit: Charles Hedgcock
Bombardier beetles eject a liquid called benzoquinone, which they superheat and expel in an intense, pulsating jet. The explosive mechanism used by the beetle generates a spray that's much hotter than that of other insects that use the liquid, and propels the jet five times faster.
Credit: Charles Hedgcock
Close
Bombardier beetles, which exist on every continent except Antarctica, have a pretty easy life. Virtually no other animals prey on them, because of one particularly effective defense mechanism: When disturbed or attacked, the beetles produce an internal chemical explosion in their abdomen and then expel a jet of boiling, irritating liquid toward their attackers.
Researchers had been baffled by the half-inch beetles' ability to produce this noxious spray while avoiding any physical damage. But now that conundrum has been solved, thanks to research by a team at MIT, the University of Arizona, and Brookhaven National Laboratory. The findings are published this week in the journal Science by MIT graduate student Eric Arndt, professor of materials science and engineering Christine Ortiz, Wah-Keat Lee of Brookhaven National Laboratory, and Wendy Moore of the University of Arizona.
"Their defensive mechanism is highly effective," Arndt says, making bombardier beetles "invulnerable to most vertebrates, and invertebrates" -- except for a few very specialized predators that have developed countermeasures against the noxious spray.
The liquid these beetles eject is called benzoquinone, and is actually a fairly common defensive agent among insects, Arndt says. But bombardier beetles are unique in their ability to superheat the liquid and expel it in an intense, pulsating jet.
The key is that they synthesize the chemical at the instant of use, mixing two chemical precursors in a protective chamber in their hindquarters. As the materials combine to form the irritant, they also give off intense heat that brings the liquid almost to the boiling point -- and, in the process, generates the pressure needed to expel it in a jet.
Seeing inside a living beetle
"For decades, the complex mechanism of how the bombardier beetle achieves spray pulsation as a chemical defense has not been understood, because only external observations were used previously," Ortiz says. In the current study, the researchers used high-speed synchrotron X-ray imaging to "see" inside the abdomens of living bombardier beetles during explosions. They used a facility at Argonne National Laboratory to carry out the experiments and produce detailed images that revealed, for the first time, how the process works, with a camera recording the action at a rate of 2,000 frames per second.
The X-ray images of the explosion reveal the dynamics of vapor inside the beetles' abdomens. They show that spray pulsation is controlled by the passageway between two internal chambers; two structures control this process: a flexible membrane and a valve.
The opening and closing of this passageway between a chamber holding the precursor liquid and an explosion chamber seems to take place passively; an increase in pressure during the explosion expands the membrane, closing the valve. Then, after the pressure is released when the liquid is ejected, the membrane relaxes back to its original state and the passage reopens, allowing the next pulse to form. This all takes place so rapidly -- not to mention inside the insect -- that the process had never been directly observed.
The explosive mechanism used by the bombardier beetle generates a spray that is not only much hotter than that emitted by other insects that use the same chemical irritant, but also propels the jet five times faster. Both the speed and the heat serve to make the spray even more effective against potential predators, Arndt says.
The pulsing nature of the spray may help protect the structure of the beetle's reaction chamber, Arndt says, allowing time for the chamber walls to cool a bit before the next pulse.
Understanding the beetles' ability to survive these intense internal explosions may help in designing blast-protection systems; this study shows how the sophisticated and specialized biological design of the system works to simultaneously achieve defensive and protective functions, Ortiz says. The reaction chamber, for example, possesses a rigid, reinforcing structure to minimize stretching and sustain temperature increases during an explosion, while other components allow for controlled, reversible stretching and movement to control the jet of fluid. The dynamics of the spray generation might also provide information useful in the design of propulsion systems, the researchers say.
R. Jeffrey Dean, a professor of biology at Cleveland State University who studies the defense mechanisms of the bombardier beetle, says the new work is a "wonderful confirmation of the qualitative passive 'pulse jet' model" first proposed by his team. "Although the findings are not unexpected, I'm amazed at the progressive advances in techniques," he adds.
This research was supported by the Department of Energy, the Department of Defense through the U.S. Army Research Office and the National Security Science and Engineering Faculty Fellowship, and the National Science Foundation.
Video: https://www.youtube.com/watch?v=TgqF-ND2XcY
Story Source:
The above story is based on materials provided by Massachusetts Institute of Technology . The original article was written by David L. Chandler. Note: Materials may be edited for content and length.
Journal Reference:
Eric M. Arndt, Wendy Moore, Wah-Keat Lee, Christine Ortiz. Mechanistic origins of bombardier beetle (Brachinini) explosion-induced defensive spray pulsation. Science, 2015 DOI: 10.1126/science.1261166
Cite This Page:
</MainBody>
    </Article>
    <Article id="25">
        <date>Thu Apr 30 21:51:20 EEST 2015</date>
        <title>Dam removal study reveals river resiliency</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/TckW00J1gnI/150430145120.htm</Link>
        <Description>More than 1,000 dams have been removed across the United States because of safety concerns, sediment buildup, inefficiency or having otherwise outlived usefulness. A paper finds that rivers are resilient and respond relatively quickly after a dam is removed.</Description>
        <MainBody>Dam removal study reveals river resiliency
Date:
United States Geological Survey
Summary:
More than 1,000 dams have been removed across the United States because of safety concerns, sediment buildup, inefficiency or having otherwise outlived usefulness. A paper finds that rivers are resilient and respond relatively quickly after a dam is removed.
Share:
Total shares: 
FULL STORY
More than 1000 dams have been removed across the United States because of safety concerns, sediment buildup, inefficiency or having otherwise outlived usefulness. A paper published in Science finds that rivers are resilient and respond relatively quickly after a dam is removed.
"The apparent success of dam removal as a means of river restoration is reflected in the increasing number of dams coming down, more than 1,000 in the last 40 years," said lead author of the study Jim O'Connor, geologist with the U.S. Geological Survey. "Rivers quickly erode sediment accumulated in former reservoirs and redistribute it downstream, commonly returning the river to conditions similar to those prior to impoundment."
Dam removal and the resulting river ecosystem restoration is being studied by scientists from several universities and government agencies, including the USGS and U.S. Forest Service, as part of a national effort to document the effects of removing dams. Studies show that most river channels stabilize within months or years, not decades, particularly when dams are removed rapidly.
"In many cases, fish and other biological aspects of river ecosystems also respond quickly to dam removal," said co-author of the study Jeff Duda, an ecologist with USGS. "When given the chance, salmon and other migratory fish will move upstream and utilize newly opened habitat."
The increase in the number of dam removals, both nationally and internationally, has spurred the effort to understand the consequences and help guide future dam removals.
"As existing dams age and outlive usefulness, dam removal is becoming more common, particularly where it can benefit riverine ecosystems," said Gordon Grant, Forest Service hydrologist. "But it can be a complicated decision with significant economic and ecologic consequences. Better understanding of outcomes enables better decisions about which dams might be good candidates for removal and what the river might look like as a result."
Story Source:
The above story is based on materials provided by United States Geological Survey . Note: Materials may be edited for content and length.
Journal Reference:
J. E. O'Connor, J. J. Duda, and G. E. Grant. 1000 dams down and counting. Science, April 2015 DOI: 10.1126/science.aaa9204
Cite This Page:
</MainBody>
    </Article>
    <Article id="26">
        <date>Thu Apr 30 21:51:13 EEST 2015</date>
        <title>Physicists find long sought-after Efimov state in helium trimer</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/yUkcnuyyOxg/150430145113.htm</Link>
        <Description>A quantum state predicted by the Russian theoretician Vitaly Efimov 40 years ago has been discovered by physicists in a molecule consisting of three helium atoms. The molecule is of enormous spatial extent and exists mainly in the classically forbidden tunneling region, explain the researchers.</Description>
        <MainBody>Goethe University Frankfurt
Summary:
A quantum state predicted by the Russian theoretician Vitaly Efimov 40 years ago has been discovered by physicists in a molecule consisting of three helium atoms. The molecule is of enormous spatial extent and exists mainly in the classically forbidden tunneling region, explain the researchers.
Share:
Total shares: 
FULL STORY
This is efimov trimer in a gas beam of other particles. The three helium atoms form an acute triangle, their distance from the quantum cloud, shown in yellow, amounts to a hundredfold of the size of the atoms.
Credit: GU
This is efimov trimer in a gas beam of other particles. The three helium atoms form an acute triangle, their distance from the quantum cloud, shown in yellow, amounts to a hundredfold of the size of the atoms.
Credit: GU
Close
In 1970, Vitaly Efimov analysed a three-body quantum system in which the attraction between two bodies reduced such that they become unbound. His prediction was that instead of breaking up, the molecule consisting of three particles can support an infinite number of bound states with huge distances between the binding partners. "Every classical notion as to why such a structure remains stable fails here," explains Prof. Reinhard Drner, head of the research group at the Institute for Nuclear Physics.
This counter-intuitive prediction led to the currently booming field of "Efimov physics." It soon became apparent that a system consisting of three helium atoms, a so-called trimer, would be the prime example of this quantum mechanical effect. But all experiments conducted to prove the existence of the gigantic, extremely weakly bound helium system failed.
In 2006, physicists at the University of Innsbruck first found indirect indications of Efimov systems in cold quantum gases of caesium atoms. In the atom traps they used, the interaction between the particles can be externally controlled. Efimov systems, however, as soon as they appear, are ejected from the artificial environment of the trap and fall apart unseen.
The Frankfurt physicist Dr. Maksim Kunitski, of the research group of Prof. Drner has now produced a stable Efimov system consisting of three helium atoms, by pressing gaseous helium at a temperature of only eight degrees above absolute zero through a tiny nozzle into a vacuum. In this ultracold molecular beam, helium molecules with two, three or more helium atoms are formed. By diffraction of the molecular beam at a super-fine transmission grating, the physicist was able to spatially separate the trimers.
The researchers created an exploded view of this Efimov state which directly show the structure of and, in particular, the distances between the atoms in the trimer. They ionized each helium atom of the molecule with the help of a laser beam. Due to the electrostatic repulsion, the now triply positively charged trimer broke apart explosively. Subsequently, using the COLTRIMS microscope developed at the Goethe University, researchers measured the momenta of the helium ions in three-dimensions, which allowed to reconstruct the geometry of the trimer.
In collaboration with the theoretician Doerte Blume of Washington State University in the USA, Maksim Kunitski found out that only one of the many possible Efimov states had in fact occurred naturally in the molecular beam. The distances between bonds in the huge molecule extend to more than 100 angstroms (compared to a mere two angstroms in a water molecule). Thereby, the helium atoms do not form an isosceles triangle, but are arranged asymmetrically. That correlates well with the theoretical predictions that have already existed for many years.
"This is the first stable Efimov system that has ever been discovered. The three-body system flies through the laboratory inside the vacuum chamber without further interaction and without the need for external fields," Drner explains. "Maksim Kunitski has conducted this ground breaking work in a laser laboratory at the Goethe University Frankfurt. He did not need a big machine to accomplish this."
"The Efimov state is not an exotic special case, but rather an example of a universal quantum effect that plays an essential role in many areas of physics," Kunitski explains. Examples of this are cold atoms, clusters, nuclear physics and recently also solid-state physics. Moreover, there are also first reports about its significance in biology.
Reinhard Drner could afford to tackle a research project that was so risky with respect to its prospects of success because in 2009 the German Research Foundation (DFG) made 1.25 million Euros available as part of its Koselleck programme. "It was a rather bold plan," says Drner in retrospect, "but now, at the end of the project and really only because the DFG provided me with this large amount for a risky project without detailed planning -- the search was successful."
Story Source:
The above story is based on materials provided by Goethe University Frankfurt . Note: Materials may be edited for content and length.
Journal Reference:
Maksim Kunitski, Stefan Zeller, Jrg Voigtsberger, Anton Kalinin, Lothar Ph. H. Schmidt, Markus Schffler, Achim Czasch, Wieland Schllkopf, Robert E. Grisenti, Till Jahnke, Drte Blume, and Reinhard Drner. Observation of the Efimov state of the helium trimer. Science, 2015 DOI: 10.1126/science.aaa5601
Cite This Page:
</MainBody>
    </Article>
    <Article id="27">
        <date>Thu Apr 30 21:49:58 EEST 2015</date>
        <title>Waking proteins up from deep sleep to study their motions</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/m78w4-sR08U/150430144958.htm</Link>
        <Description>In order to carry out their functions, proteins need to move. Scientists have developed a new technique to study motions in proteins with unprecedented accuracy. The method, which is based on NMR, freezes proteins down to immobility, then slowly heats them to 'wake them up' and restart motions individually and in sequence, providing a slow-motion image of real conditions.</Description>
        <MainBody>Waking proteins up from deep sleep to study their motions
Date:
Ecole Polytechnique Fdrale de Lausanne
Summary:
In order to carry out their functions, proteins need to move. Scientists have developed a new technique to study motions in proteins with unprecedented accuracy. The method, which is based on NMR, freezes proteins down to immobility, then slowly heats them to 'wake them up' and restart motions individually and in sequence, providing a slow-motion image of real conditions.
Share:
Total shares: 
FULL STORY
Proteins inside a cell are in constant motion, changing shape continuously in order to carry out their functions. In addition, their multiple component atoms each have individual patterns of motion, making the entire protein a system of non-stop highly complex movement. Understanding how a protein moves is the key to developing drugs that can efficiently interact with it. But because of this complexity, protein motion has been notoriously difficult to study. Scientists at EPFL, IBS-Grenoble, and ENS-Lyon, have developed a new method for studying protein motion by first freezing proteins and then slowly "waking them up" with increasing temperature. The breakthrough method, which was developed at EPFL's advanced NMR facility, is published in Science.
Protein motion is highly complex
Motion is part of a protein's function, allowing it to adjust its 3D shape and interact with other molecules like biological molecules and synthetic drugs. These "functional" motions however are complex, and can be thought as the mechanism of a watch, where motions between interlocking cogs and springs, at different timescales, result in the smooth movement of the hands.
In a protein the cogs and springs are the molecules that make it up: amino acids form its backbone each with side-chains of different molecules branching out on all sides in three dimensions. In addition, water molecules on the protein as well as the solution where it exists, e.g. the cell's cytoplasm, add even more layers of motion complexity to the system. But unlike a watch, whose individual movements are all well defined, each of the component motions in a protein are actually random. As a result, protein motion seems almost chaotic, and is practically impossible to study.
Freeze, sleep, wake up, and move
A team of scientists at EPFL, IBS-Grenoble, and ENS-Lyon led by Lyndon Emsley and Martin Blackledge developed an innovative solution to the motion problem: freeze the proteins and then watch them "wake up" from deep sleep. Protein motion depends on energy, and temperature is basically a measurement of the energy of a system. By freezing proteins down to temperatures of -168C, the researchers were able to completely stop all the motions of interest in the molecules. Then, they slowly raised the temperature to the point where the proteins could regain their natural motions, but at a much slower pace. This way, it was possible to look at each motion a protein makes individually and -- more importantly -- in sequence.
In order to detect the individual motions of proteins, the scientists used a spectroscopic technique called nuclear magnetic resonance (NMR), which exploits the magnetic properties of certain atoms like hydrogen and carbon. NMR works by placing the sample of the protein to be studied inside a device with a strong magnetic field, and observing how they respond to different radio frequencies. This response is registered on a computer that produces a diagram of peaks, each representing energy transitions in specific atoms. Depending on the properties of the peaks on the diagram, scientists can determine the degree of motion of each atom in the protein, e.g. its backbone, a side-chain etc.
Because the proteins in this study needed to be frozen down, the team had to adjust their NMR methodology to work with samples at very low temperatures, and keep doing so as the researchers slowly raised the temperature to "wake the proteins up." In addition, samples that are frozen solid are difficult to read in NMR, so the tube containing the proteins had to also be constantly spinning at a specific ("magic") angle to the NMR's magnetic field, to improve resolution. Finally, every NMR experiment took days to perform.
These complications were overcome by using a newly developed device that had been specifically designed to work with NMR at low and changing temperatures. To achieve the necessary high resolution, the scientists combined this device with a precise rotor system that could spin the sample over long periods of time.
A hierarchy of motion
Using their innovative approach, Emsley's team found that the sequence of protein motions follows a specific hierarchy as temperature increases: first the protein's solvent molecules, then the protein's side-chains and water molecules, and finally the protein's backbone. The sequence culminates with a functionally active protein at temperatures even as low as -53C, well below physiological levels. This means that the "waking up" method is very effective for studying the motions of a protein individually and sequentially.
"Our work shows that we can use this technique, which is called 'variable-temperature solid-state NMR', to gain unique and novel insights into the role of protein dynamics in biology," says Lyndon Emsley. The team is now interested in using this method to find out just how universal this hierarchy of motions is, and what might cause variations between different molecules.
Story Source:
The above story is based on materials provided by Ecole Polytechnique Fdrale de Lausanne . Note: Materials may be edited for content and length.
Journal Reference:
Lewandowski J.R, Halse ME, Blackledge M, Emsley, L. Direct observation of hierarchical protein dynamics. Science, 2015 DOI: 10.1126/science.aaa6111
Cite This Page:
</MainBody>
    </Article>
    <Article id="28">
        <date>Thu Apr 30 21:49:55 EEST 2015</date>
        <title>Tropical marine ecosystems most at threat from human impact</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/xaOnWx6GyZo/150430144955.htm</Link>
        <Description>An international team of scientists has used the fossil record during the past 23 million years to predict which marine animals and ecosystems are at greatest risk of extinction from human impact. The researchers found those animals and ecosystems most threatened are predominantly in the tropics.</Description>
        <MainBody>Tropical marine ecosystems most at threat from human impact
Date:
ARC Centre of Excellence in Coral Reef Studies
Summary:
An international team of scientists has used the fossil record during the past 23 million years to predict which marine animals and ecosystems are at greatest risk of extinction from human impact. The researchers found those animals and ecosystems most threatened are predominantly in the tropics.
Share:
Tropical marine ecosystems have been found to be most at risk from human impact.
Credit: Andrew Baird
Tropical marine ecosystems have been found to be most at risk from human impact.
Credit: Andrew Baird
Close
An international team of scientists has used the fossil record during the past 23 million years to predict which marine animals and ecosystems are at greatest risk of extinction from human impact.
In a paper published in the journal Science, the researchers found those animals and ecosystems most threatened are predominantly in the tropics.
"Marine species are under threat from human impacts, but knowledge of their vulnerabilities is limited," says study co-author, Professor John Pandolfi from the ARC Centre of Excellence for Coral Reef Studies at the University of Queensland.
The researchers found that the predictors of extinction vulnerability, geographic range size and the type of organism, have remained consistent over the past 23 million years.
As such, they were able to use fossil records to assess the baseline extinction risk for marine animals, including sharks, whales and dolphins, as well as small sedentary organisms such as snails, clams and corals.
They then mapped the regions where those species with a high intrinsic risk are most affected today by human impact and climate change.
"Our goal was to diagnose which species are vulnerable in the modern world, using the past as a guide" says study lead author, Assistant Professor Seth Finnegan from the University of California Berkeley.
"We used these estimates to map natural extinction risk in modern oceans, and compare it with recent human pressures on the ocean such as fishing, and climate change to identify the areas most at risk," says Professor Pandolfi.
"These regions are disproportionately in the tropics, raising the possibility that these ecosystems may be particularly vulnerable to future extinctions."
The scientists say that identifying the regions and species at greatest risk means conservation efforts can be better targeted.
"We believe the past can inform the way we plan our conservation efforts. However there is a lot more work that needs to be done to understand the causes underlying these patterns and their policy implications," says Asst. Professor, Seth Finnegan
Co-author, Dr Sean Anderson from Simon Fraser University in Burnaby, British Columbia adds, "It's very difficult to detect extinctions in the modern oceans but fossils can help fill in the gaps."
"Our findings can help prioritize areas and species that might be at greater risk of extinction and that might require extra attention, conservation or management -- protecting vulnerable species in vulnerable places."
Story Source:
The above story is based on materials provided by ARC Centre of Excellence in Coral Reef Studies . Note: Materials may be edited for content and length.
Journal Reference:
Seth Finnegan, Sean C. Anderson, Paul G. Harnik, Carl Simpson, Derek P. Tittensor, Jarrett E. Byrnes, Zoe V. Finkel, David R. Lindberg, Lee Hsiang Liow, Rowan Lockwood, Heike K. Lotze, Craig R. McClain, Jenny L. McGuire, Aaron ODea, and John M. Pandolfi. Paleontological baselines for evaluating extinction risk in the modern oceans. Science, April 2015 DOI: 10.1126/science.aaa6635
Cite This Page:
</MainBody>
    </Article>
    <Article id="29">
        <date>Thu Apr 30 21:49:53 EEST 2015</date>
        <title>HIV-1: Optimizing treatment protocols when diagnostics are costly</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/8m_qGC0-030/150430144953.htm</Link>
        <Description>HIV-1 continues to spread globally. While neither a cure, nor an effective vaccine are available, recent focus has been put on 'treatment-for-prevention', which is a method by which treatment is used to reduce the contagiousness of an infected person. A new study challenges current treatment paradigms in the context of 'treatment for prevention' against HIV-1.</Description>
        <MainBody>PLOS
Summary:
HIV-1 continues to spread globally. While neither a cure, nor an effective vaccine are available, recent focus has been put on 'treatment-for-prevention', which is a method by which treatment is used to reduce the contagiousness of an infected person. A new study challenges current treatment paradigms in the context of 'treatment for prevention' against HIV-1.
Share:
Total shares: 
FULL STORY
Optimal treatment strategies in the context of 'treatment for prevention' against HIV-1. Without medical treatment (upper panel) HIV-1 infected individuals have a high viral titer, which is related to a high probability to infect a sero-discordant partner after sexual contact. In contrast, diagnostic-guided (middle panel) and pro-active treatment switching strategies (lower panel) can durably suppress the virus in an HIV-1 infected individual, thus reducing the probability that the individual spreads the infection.
Credit: Sulav Duwal
Optimal treatment strategies in the context of 'treatment for prevention' against HIV-1. Without medical treatment (upper panel) HIV-1 infected individuals have a high viral titer, which is related to a high probability to infect a sero-discordant partner after sexual contact. In contrast, diagnostic-guided (middle panel) and pro-active treatment switching strategies (lower panel) can durably suppress the virus in an HIV-1 infected individual, thus reducing the probability that the individual spreads the infection.
Credit: Sulav Duwal
Close
HIV-1 continues to spread globally. While neither a cure, nor an effective vaccine are available, recent focus has been put on 'treatment-for-prevention', which is a method by which treatment is used to reduce the contagiousness of an infected person. A study published this week in PLOS Computational Biology challenges current treatment paradigms in the context of 'treatment for prevention' against HIV-1.
Sulav Duwal, Max von Kleist and their collaborators develop and employ optimal control theory to compute and assess diagnostic-guided vs. pro-active treatment strategies in terms of their expected costs, treatment benefit and reduction of onwards transmission.
In the study published this week in PLOS Computational Biology, the authors provide a mathematical platform that can be used to compute optimal diagnostic-guided vs. pro-active treatment strategies under consideration of available resources. They apply this framework to a stochastic model of viral intra-host dynamics and drug resistance development. When applied to resource-constrained settings, they show that pro-active strategies may be worthwhile.
Story Source:
The above story is based on materials provided by PLOS . Note: Materials may be edited for content and length.
Journal Reference:
Sulav Duwal, Stefanie Winkelmann, Christof Schtte, Max von Kleist. Optimal Treatment Strategies in the Context of Treatment for Prevention against HIV-1 in Resource-Poor Settings. PLOS Computational Biology, 2015; 11 (4): e1004200 DOI: 10.1371/journal.pcbi.1004200
Cite This Page:
</MainBody>
    </Article>
    <Article id="30">
        <date>Thu Apr 30 21:49:51 EEST 2015</date>
        <title>Sustainability progress should precede seafood market access, researchers urge</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/iY15XKeWpn8/150430144951.htm</Link>
        <Description>Fishery improvement projects -- programs designed to fast-track access to the world seafood market in exchange for promises to upgrade sustainable practices -- need to first make good on those sustainability pledges before retailers and fisheries actually do business, researchers recommend. The findings are particularly important as major retailers rush to meet the growing demand for seafood by tapping fisheries of developing countries that haven't yet achieved sustainable certification.</Description>
        <MainBody>Sustainability progress should precede seafood market access, researchers urge
Date:
University of California - Davis
Summary:
Fishery improvement projects -- programs designed to fast-track access to the world seafood market in exchange for promises to upgrade sustainable practices -- need to first make good on those sustainability pledges before retailers and fisheries actually do business, researchers recommend. The findings are particularly important as major retailers rush to meet the growing demand for seafood by tapping fisheries of developing countries that haven't yet achieved sustainable certification.
Share:
Total shares: 
FULL STORY
Demand for seafood from wild fisheries and aquaculture around the world has nearly doubled over the past four decades. In the past several years, major retailers in developed countries have committed to source their seafood from only sustainably certified fisheries and aquaculture, even though it is not clear where that supply will come from.
A team of researchers led by the University of California, Davis, has focused its attention on fishery improvement projects, or FIPs, which are designed to bring seafood from wild fisheries to the certified market, with only a promise of sustainability in the future. They conclude that FIPs need to be fine tuned to ensure that fisheries are delivering on their promises.
The researchers' policy forum titled "Secure Sustainable Seafood from Developing Countries" will be published May 1 in the journal Science.
"We're cautiously optimistic that fishery improvement projects, which fast-track access to international markets, can lead to sustainable fisheries, especially in developing countries," said UC Davis Professor James Sanchirico, associate director of the UC Davis Coastal and Marine Sciences Institute.
Encouraging sustainable seafood:
Retailers like Walmart in the U.S. and Sainsbury's in the U.K. have promised that soon all the fresh, frozen, farmed and wild seafood they sell will come from sustainable sources. Respected, private third-party certifying programs like the Marine Stewardship Council are helping to ensure compliance with meaningful sustainability standards designed to help conserve fish populations and protect oceans.
While many of the sustainability standards have been met by commercial fisheries in the developed world, fisheries overseen by developing countries make up only 7 percent of MSC-certified fisheries, even though these developing-country fisheries account for about half of all seafood entering the international market.
Fishery improvement projects
Fishery improvement projects aim to get fisheries on a path to sustainability and potentially certification by the MSC. These projects involve partnerships between the fishermen and firms up and down the international seafood supply chain. A critical objective of the partnerships is to create market incentives for improvements by allowing seafood from these developing-country fisheries to enter the potentially more lucrative export market for certified seafood.
"It is hoped that the projects will protect marine life and ecosystems in areas where local and national governments have not acted to oversee sustainable practices, while also satisfying the demand for sustainable seafood," said Gabriel Sampson, UC Davis graduate student and lead author of the study.
Potential 'race to the bottom' in standards:
Sampson and colleagues report that seafood from two-thirds of developing-world fisheries enrolled in fishery improvement projects is already being bought by retailers, intending to satisfy their sustainability commitments while making little progress in improving their management.
Fishery management reforms should include data collection and ongoing monitoring, strengthening harvest and access rights to the resources, limits on the catch, and instituting traceability throughout the supply chain, the researchers say. If access to the fisheries is not better regulated, the current efforts by retailers to secure sustainable, wild-caught seafood could stimulate a "race to fish" and ultimately undermine the sustainability claims.
Without the proper safeguards to ensure progress and reforms in fishery improvement projects, fisheries with full sustainability certification could find their market benefits diluted by the increased competition for a share of the global certified seafood market.
Having multiple types of certified seafood in the market could lead to a "race to the bottom" in sustainability standards, unless the fishery improvement projects are carefully monitored to make sure that seafood retailers closely adhere to the sustainable-improvement requirements for market access.
"The retailers and organizations involved with managing fishery improvement projects need to insist on progress toward reforms from the fishery as a condition for purchasing seafood from that fishery," Sanchirico said.
"This would likely lead to more durable conservation and greater assurance for consumers that marketing claims of 'sustainable' seafood are valid," he said.
Story Source:
The above story is based on materials provided by University of California - Davis . Note: Materials may be edited for content and length.
Journal Reference:
Gabriel S. Sampson, James N. Sanchirico, Cathy A. Roheim, Simon R. Bush, J. Edward Taylor, Edward H. Allison, James L. Anderson, Natalie C. Ban, Rod Fujita, Stacy Jupiter, and Jono R. Wilson. Secure Sustainable Seafood from Developing Countries. Science, April 2015 DOI: 10.1126/science.aaa4639
Cite This Page:
</MainBody>
    </Article>
    <Article id="31">
        <date>Thu Apr 30 21:49:47 EEST 2015</date>
        <title>Worm index closely associated with a nation's human development index</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/8-ZO4iqghu0/150430144947.htm</Link>
        <Description>With the Millennium Development Goals established by the United Nations in 2000 coming to an end in 2015, and the new Sustainable Development Goals now in the works to establish a set of targets for the future of international development, experts have developed a new tool to show why neglected tropical diseases, the most common infections of the world's poor, should be an essential component of these goals.</Description>
        <MainBody>Worm index closely associated with a nation's human development index
Date:
PLOS
Summary:
With the Millennium Development Goals established by the United Nations in 2000 coming to an end in 2015, and the new Sustainable Development Goals now in the works to establish a set of targets for the future of international development, experts have developed a new tool to show why neglected tropical diseases, the most common infections of the world's poor, should be an essential component of these goals.
Share:
Total shares: 
FULL STORY
With the Millennium Development Goals established by the United Nations in 2000 coming to an end in 2015, and the new Sustainable Development Goals now in the works to establish a set of targets for the future of international development, experts at Baylor College of Medicine have developed a new tool to show why neglected tropical diseases, the most common infections of the world's poor, should be an essential component of these goals.
Using World Health Organization data for the number people at risk of parasitic worm infections in each of the largest nations and comparing this number to each nation's population, Dr. Peter Hotez, dean of the National School of Tropical Medicine at Baylor College of Medicine, and Dr. Jennifer R. Herricks, postdoctoral fellow at Baylor, developed the worm index, which they found to have a strong association with a nation's human development index, a summary measure of average achievement in key dimensions of human development.
Their work was published in PLOS Neglected Tropical Diseases.
"Through this paper, we've shown how the major neglected tropical diseases, which include intestinal worm infections, schistosomiasis and lymphatic filariasis, are intimately tied to human development," said Hotez, who also is Texas Children's Hospital Endowed Chair of Tropical Pediatrics and president of the Sabin Vaccine Institute.
"We found a very tight association between the worm index of a country and the human development index. The higher the worm index, the lower the human development index."
Parasitic worm infections affect millions of people and can cause long-term, chronic and disabling diseases.
"Because decreased human development is related to increased burden of parasitic worm infections, we recommend that serious consideration should be given to parasitic worm infections and other neglected tropical diseases when trying to attain goals that will ultimately improve human development; for example, when implementing the sustainable development goals," said Herricks, postdoctoral fellow in Disease and Poverty at the National School of Tropical Medicine at Baylor, who is also with Rice University's James A. Baker III Institute for Public Policy.
Story Source:
The above story is based on materials provided by PLOS . Note: Materials may be edited for content and length.
Journal Reference:
Peter J. Hotez, Jennifer R. Herricks. Helminth Elimination in the Pursuit of Sustainable Development Goals: A "Worm Index" for Human Development. PLOS Neglected Tropical Diseases, 2015; 9 (4): e0003618 DOI: 10.1371/journal.pntd.0003618
Cite This Page:
</MainBody>
    </Article>
    <Article id="32">
        <date>Thu Apr 30 21:18:03 EEST 2015</date>
        <title>Scientists discover key driver of human aging: May lead to slowing or reversing aging process</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/_dgPslxrB_Y/150430141803.htm</Link>
        <Description>A study tying the aging process to the deterioration of tightly packaged bundles of cellular DNA could lead to methods of preventing and treating age-related diseases such as cancer, diabetes and Alzheimer's disease, experts say.</Description>
        <MainBody>Salk Institute for Biological Studies
Summary:
A study tying the aging process to the deterioration of tightly packaged bundles of cellular DNA could lead to methods of preventing and treating age-related diseases such as cancer, diabetes and Alzheimer's disease, experts say.
Share:
Total shares: 
FULL STORY
Salk Institute researchers discovered that a protein mutated in the premature aging disorder, Werner syndrome, plays a key role in stabilizing heterochromatin, a tightly packaged form of DNA. More generally, the findings suggest that heterochromatin disorganization may be a key driver of aging. This image shows normal human cells (left) and genetically modified cells developed by the Salk scientists to simulate Werner syndrome (right), which showed signs of aging, including their larger size.
Credit: Salk Institute
Salk Institute researchers discovered that a protein mutated in the premature aging disorder, Werner syndrome, plays a key role in stabilizing heterochromatin, a tightly packaged form of DNA. More generally, the findings suggest that heterochromatin disorganization may be a key driver of aging. This image shows normal human cells (left) and genetically modified cells developed by the Salk scientists to simulate Werner syndrome (right), which showed signs of aging, including their larger size.
Credit: Salk Institute
Close
A study tying the aging process to the deterioration of tightly packaged bundles of cellular DNA could lead to methods of preventing and treating age-related diseases such as cancer, diabetes and Alzheimer's disease, as detailed April 30, 2015, in Science.
In the study, scientists at the Salk Institute and the Chinese Academy of Science found that the genetic mutations underlying Werner syndrome, a disorder that leads to premature aging and death, resulted in the deterioration of bundles of DNA known as heterochromatin.
The discovery, made possible through a combination of cutting-edge stem cell and gene-editing technologies, could lead to ways of countering age-related physiological declines by preventing or reversing damage to heterochromatin.
"Our findings show that the gene mutation that causes Werner syndrome results in the disorganization of heterochromatin, and that this disruption of normal DNA packaging is a key driver of aging," says Juan Carlos Izpisua Belmonte, a senior author on the paper. "This has implications beyond Werner syndrome, as it identifies a central mechanism of aging--heterochromatin disorganization--which has been shown to be reversible."
Werner syndrome is a genetic disorder that causes people to age more rapidly than normal. It affects around one in every 200,000 people in the United States. People with the disorder suffer age-related diseases early in life, including cataracts, type 2 diabetes, hardening of the arteries, osteoporosis and cancer, and most die in their late 40s or early 50s.
The disease is caused by a mutation to the Werner syndrome RecQ helicase-like gene, known as the WRN gene for short, which generates the WRN protein. Previous studies showed that the normal form of the protein is an enzyme that maintains the structure and integrity of a person's DNA. When the protein is mutated in Werner syndrome it disrupts the replication and repair of DNA and the expression of genes, which was thought to cause premature aging. However, it was unclear exactly how the mutated WRN protein disrupted these critical cellular processes.
In their study, the Salk scientists sought to determine precisely how the mutated WRN protein causes so much cellular mayhem. To do this, they created a cellular model of Werner syndrome by using a cutting-edge gene-editing technology to delete WRN gene in human stem cells. This stem cell model of the disease gave the scientists the unprecedented ability to study rapidly aging cells in the laboratory. The resulting cells mimicked the genetic mutation seen in actual Werner syndrome patients, so the cells began to age more rapidly than normal. On closer examination, the scientists found that the deletion of the WRN gene also led to disruptions to the structure of heterochromatin, the tightly packed DNA found in a cell's nucleus.
This bundling of DNA acts as a switchboard for controlling genes' activity and directs a cell's complex molecular machinery. On the outside of the heterochromatin bundles are chemical markers, known as epigenetic tags, which control the structure of the heterochromatin. For instance, alterations to these chemical switches can change the architecture of the heterochromatin, causing genes to be expressed or silenced.
The Salk researchers discovered that deletion of the WRN gene leads to heterochromatin disorganization, pointing to an important role for the WRN protein in maintaining heterochromatin. And, indeed, in further experiments, they showed that the protein interacts directly with molecular structures known to stabilize heterochromatin--revealing a kind of smoking gun that, for the first time, directly links mutated WRN protein to heterochromatin destabilization.
"Our study connects the dots between Werner syndrome and heterochromatin disorganization, outlining a molecular mechanism by which a genetic mutation leads to a general disruption of cellular processes by disrupting epigenetic regulation," says Izpisua Belmonte. "More broadly, it suggests that accumulated alterations in the structure of heterochromatin may be a major underlying cause of cellular aging. This begs the question of whether we can reverse these alterations--like remodeling an old house or car--to prevent, or even reverse, age-related declines and diseases."
Izpisua Belmonte added that more extensive studies will be needed to fully understand the role of heterochromatin disorganization in aging, including how it interacts with other cellular processes implicated in aging, such as shortening of the end of chromosomes, known as telomeres. In addition, the Izpisua Belmonte team is developing epigenetic editing technologies to reverse epigenetic alterations with a role in human aging and disease.
Story Source:
The above story is based on materials provided by Salk Institute for Biological Studies . Note: Materials may be edited for content and length.
Journal Reference:
Weiqi Zhang, Jingyi Li, Keiichiro Suzuki, Jing Qu, Ping Wang, Junzhi Zhou, Xiaomeng Liu, Ruotong Ren, Xiuling Xu, Alejandro Ocampo, Tingting Yuan, Jiping Yang, Ying Li, Liang Shi, Dee Guan, Huize Pan, Shunlei Duan, Zhichao Ding, Mo Li, Fei Yi, Ruijun Bai, Yayu Wang, Chang Chen, Fuquan Yang, Xiaoyu Li, Zimei Wang, Emi Aizawa, April Goebl, Rupa Devi Soligalla, Pradeep Reddy, Concepcion Rodriguez Esteban, Fuchou Tang, Guang-Hui Liu, and Juan Carlos Izpisua Belmonte. A Werner syndrome stem cell model unveils heterochromatin alterations as a driver of human aging. Science, 30 April 2015 DOI: 10.1126/science.aaa1356
Cite This Page:
</MainBody>
    </Article>
    <Article id="33">
        <date>Thu Apr 30 21:16:06 EEST 2015</date>
        <title>Mysterious case of the disappearing honey bee: New clues about decline</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/zhaHUTcdTHw/150430141606.htm</Link>
        <Description>A new study shows poor nutrition for honey bee larvae leads to compromised pollination capabilities as adult bees. This is a possible link to Colony Collapse Disorder (CCD).</Description>
        <MainBody>Wellesley College
Summary:
A new study shows poor nutrition for honey bee larvae leads to compromised pollination capabilities as adult bees. This is a possible link to Colony Collapse Disorder (CCD).
Share:
Total shares: 
FULL STORY
Heather Mattila working with honey bees and one of her undergraduate researchers, student Anita Yau '17 at Wellesley College.
Credit: Wellesley College/Ping Ji
Heather Mattila working with honey bees and one of her undergraduate researchers, student Anita Yau '17 at Wellesley College.
Credit: Wellesley College/Ping Ji
Close
A new study by Heather Mattila, a leading honey bee ecologist and Assistant Professor of Biological Sciences at Wellesley College, published this April in PLOS ONE, reveals that inadequate access to pollen during larval development has lifelong consequences for honey bees, leading not only to smaller workers and shorter lifespans, but also to impaired performance and productivity later in life. For the first time, this study demonstrates a crucial link between poor nutrition at a young age, and foraging and waggle dancing, the two most important activities that honey bees perform as providers for their colonies and as pollinators of human crops. The study was co-authored by Hailey Scofield, Wellesley Class of 2013, a former undergraduate research assistant who will begin a Ph. D program (in Neurobiology and Behavior) at Cornell University in Fall 2015.
The need to study nutritional stress in honey bees has grown pressing in recent years. In 2013, the U.S. Department of Agriculture and the Environmental Protection Agency named nutritional stress one of the top research priorities for understanding unexplained losses of honey bee colonies, a phenomenon known in the U.S. as Colony Collapse Disorder (CCD). With bee pollination accounting for over $15 billion in food crops and $150 million in honey annually in the United States alone, bee losses have enormous ecological and economic consequences. If bees vanish, many plants, including vital food crops like apples, almonds, berries and cucumbers, may also be at risk. Researchers believe there may be several interrelated factors contributing to bee decline, including nutritional stress, loss of foraging habitat, pesticides, pathogens, and parasites. These concerns prompted President Obama to form a Pollinator Health Task Force in 2014, an unprecedented action that named studies of the effect of poor nutrition on bees as one of its primary goals.
While a number of sophisticated nutrition studies have been undertaken recently, the Wellesley study is the first to show that nutritional deficits early in life can have far-reaching consequences for adult honey bees, including effects on complex behaviors like foraging and waggle dancing. "Nutritional stress has long been known to shorten bees' lifespan," stated Mattila, "but we've never had such a clear understanding of its impact on the tasks they perform, or known that its effects persist until their last days, even when bees have plentiful food as adults."
The study is also one of the few to be conducted entirely in a natural hive environment, which allowed larvae and adults to function in normal colonies, rather than in the incubators and cages that are more typical of nutrition studies. This unique methodology allowed Mattila, Scofield, and their undergraduate research assistants to observe the bees foraging and dancing in a natural context, activities they would not be able to perform in artificial lab conditions.
Foraging and waggle dancing are especially important to the health of a honey bee colony because they are the key means by which honey bees acquire food supplies like nectar and pollen, and communicate with other bees about the location of food sources and nest sites. When honey bee larvae were raised with a limited pollen supply, as might happen during periods of bad weather or as a consequence of habitat loss or commercial management practices, there were multiple negative consequences. The pollen-stressed bees were lighter and died younger, and fewer bees foraged. Those that did foraged earlier, for fewer days, and were more likely to die after just one day of foraging. Pollen-stressed workers were also less likely to waggle dance than workers that had been well-fed as larvae, and if they danced, the information they conveyed about the location of food sources was less precise. "Their dances were often visibly inconsistent and almost disoriented in the worst instances," said Scofield.
Importantly, nutritional stress interacts with a number of other stress factors, like pesticides and pathogens, which are already known to decrease longevity and impair foraging ability, creating a vicious cycle of poor health and population decline. Nutritional stress is also tied in part to a loss of foraging habitat, which can compound stresses from pesticide use and other commercial practices. Poor foraging and waggle dancing, in turn, could escalate bee decline if long-term pollen limitation if it prevents stressed foragers from providing sufficiently for developing workers. "If poor foraging habitats impose nutritional stress in colonies, then our study shows that the average stressed bee cannot compensate for reduced foraging opportunities by working harder to find food. This likely exacerbates nutritional stress and further limits the colony's ability to overcome food-finding challenges in areas that are no longer suitable for bees," explained Mattila.
The study also suggests that poor nutrition has the potential to undermine colony health and promote collapse. Conversely, ensuring that honey bees have access to diverse and plentiful forage throughout the year could mitigate the potential for collapse. "This means keeping bees in areas that are bee friendly, green, and full of flowering plants within the normal foraging radius of a colony, regularly checking colonies' food supplies, and providing supplements when natural forage is not available or colony stores are low," said Mattila. "Failure to provide these necessities may impose a legacy of dysfunction on colonies."
Research assistance was provided by Amanda Gardner, Rachel Reed, Catherine Oleskewicz, Anita Yau, and Amina Ziad. Additional funding was provided by the Essex County Beekeepers' Association of Massachusetts.
Story Source:
The above story is based on materials provided by Wellesley College . Note: Materials may be edited for content and length.
Journal Reference:
Hailey N. Scofield, Heather R. Mattila. Honey Bee Workers That Are Pollen Stressed as Larvae Become Poor Foragers and Waggle Dancers as Adults. PLOS ONE, 2015; 10 (4): e0121731 DOI: 10.1371/journal.pone.0121731
Cite This Page:
</MainBody>
    </Article>
    <Article id="34">
        <date>Thu Apr 30 21:16:04 EEST 2015</date>
        <title>Settling an old debate: Researchers solve a lingering mystery of cancer cell biology</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/fp9_L4M_QnE/150430141604.htm</Link>
        <Description>German biologist Theodor Boveri observed early in the last century that cancer cells often harbor multiple copies of a subcellular structure that he had previously named the centrosome. He was also the first to suggest that the extra centrosomes drive cancer. Biologists have since learned a great deal about the structure and many functions of Boveri’s “special organ of cell division.” But why cancer cells harbor multiple copies of this organelle—and whether they are “addicted” to having so many—has remained unanswered. So has the question of whether healthy human cells even require centrosomes to divide. Now, 101 years after Boveri aired his suspicions, a paper may have some answers.</Description>
        <MainBody>Ludwig Cancer Research
Summary:
German biologist Theodor Boveri observed early in the last century that cancer cells often harbor multiple copies of a subcellular structure that he had previously named the centrosome. He was also the first to suggest that the extra centrosomes drive cancer. Biologists have since learned a great deal about the structure and many functions of Boveris special organ of cell division. But why cancer cells harbor multiple copies of this organelleand whether they are addicted to having so manyhas remained unanswered. So has the question of whether healthy human cells even require centrosomes to divide. Now, 101 years after Boveri aired his suspicions, a paper may have some answers.
Share:
Total shares: 
FULL STORY
German biologist Theodor Boveri observed early in the last century that cancer cells often harbor multiple copies of a subcellular structure that he had previously named the centrosome. He was also the first to suggest that the extra centrosomes drive cancer. Biologists have since learned a great deal about the structure and many functions of Boveri's "special organ of cell division." But why cancer cells harbor multiple copies of this organelle -- and whether they are "addicted" to having so many -- has remained unanswered. So has the question of whether healthy human cells even require centrosomes to divide. Now, 101 years after Boveri aired his suspicions, a Ludwig Cancer Research paper published in advance online in Science may have some answers.
A team of researchers led by Karen Oegema and Andrew Shiau of Ludwig San Diego reports that while cancer cells are not addicted to multiple centrosomes, healthy cells absolutely require them to proceed with cell division. In the absence of centrosomes, healthy cells enter a state of latency, while malignant cells continue dividing. "Our results have settled a long-running debate in cell biology," says Oegema, a member of the Ludwig Institute for Cancer Research, San Diego. "Centrosomes make things so much better for healthy dividing cells, that they have a protective mechanism that halts their division if they lose these organelles."
Ordinarily, the resting cell's single centrosome serves as an organizing center for the cell's protein filament-based skeleton. When a cell divides, however, the centrosome takes on another function. It duplicates and plays a role in ensuring equal distribution of chromosomes to the two daughter cells. Many cancer cells contain multiple centrosomes, and this aberration contributes to the misdistribution and abnormal numbers of chromosomes in daughter cells.
Still, it wasn't clear that centrosomes are absolutely needed for cell division. Biologists have long known that other mechanisms exist to separate chromosomes. "The growing feeling among a number of cell biologists is that the 'centrosome is like the appendix of the cell'," says Shiau, director of the Ludwig Institute's Small Molecule Discovery Program in San Diego. His team, which is part of a broader Technology Development program at Ludwig, specializes in developing compounds that can be used to advance cancer research and have potential as therapies.
Earlier studies had sought to resolve the issue by cutting centrosomes out of cells or destroying them with lasers. But both normal and cancer cells treated this way simply remade their lost centrosomes, and then continued dividing.
To get around this limitation, the researchers designed and synthesized a molecule that specifically and reversibly inhibits an enzyme named Plk4, which controls the assembly of centrioles -- barrel-like protein structures from which centrosomes are made. They then showed that exposure to this inhibitor, centrinone, eliminates centrosomes from both healthy cells and cancerous ones. When the compound was removed, cancer cells reverted to precisely the number of centrosomes they had before exposure to the molecule. Those lacking centrosomes, however, did not stop dividing -- though fewer survived the ordeal.
"This was in marked contrast to what normal cells would do when we persistently removed centrosomes," says Oegema. "Normal cells arrested their growth when their centrosomes were absent. This suggests that they absolutely require centrosomes for division, which was not at all the thinking in the field."
The researchers show that the pause in the division of healthy cells is governed by a protein named p53, which is mutated in about half of all cancers. Levels of p53 were elevated in cells treated with centrinone. When the protein was temporarily inactivated in normal cells, they too failed to arrest upon exposure to centrinone.
The new ability to reversibly eliminate centrosomes is likely to benefit research in a wide variety of biomedical fields, given the organelle's multiple roles -- from organizing the cytoskeleton to sprouting hair-like structures known as cilia on certain cells. The findings might also have applications for cancer therapy, even if cancer cells aren't addicted to centrosomes.
"The idea," says Shiau, "is that you trigger p53 in normal cells and have them stop multiplying -- and then introduce another agent that only kills continuously dividing cells." This concept, dubbed cyclotherapy, was conceived several years ago by David Lane, Ludwig's Scientific Director and a co-discoverer of p53. Working with their colleagues in San Diego, the Ludwig Small Molecule Discovery team is developing more drug-like variants of centrinone with the goal of identifying combination therapies that can be tested in clinical studies.
Story Source:
The above story is based on materials provided by Ludwig Cancer Research . Note: Materials may be edited for content and length.
Journal Reference:
Yao Liang Wong, John V. Anzola, Robert L. Davis, Michelle Yoon, Amir Motamedi, Ashley Kroll, Chanmee P. Seo, Judy E. Hsia, Sun K. Kim, Jennifer W. Mitchell, Brian J. Mitchell, Arshad Desai, Timothy C. Gahman, Andrew K. Shiau, and Karen Oegema. Reversible centriole depletion with an inhibitor of Polo-like kinase 4. Science, April 2015 DOI: 10.1126/science.aaa5111
Cite This Page:
</MainBody>
    </Article>
    <Article id="35">
        <date>Thu Apr 30 21:16:02 EEST 2015</date>
        <title>Quantum mechanical monopoles experimentally identified</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/65irQcUKiuo/150430141602.htm</Link>
        <Description>Researchers have experimentally identified a pointlike monopole in a quantum field for the first time. The discovery gives scientists insight into the monopole magnet, an elementary particle that they believe exists but have not yet seen.</Description>
        <MainBody>Quantum mechanical monopoles experimentally identified
Date:
Amherst College
Summary:
Researchers have experimentally identified a pointlike monopole in a quantum field for the first time. The discovery gives scientists insight into the monopole magnet, an elementary particle that they believe exists but have not yet seen.
Share:
Artistic illustration of a quantum-mechanical monopole.
Credit: Heikka Valja
Artistic illustration of a quantum-mechanical monopole.
Credit: Heikka Valja
Close
Building on his own previous research, Amherst College professor David S. Hall and a team of international collaborators have experimentally identified a pointlike monopole in a quantum field for the first time. The discovery, announced this week, gives scientists further insight into the elusive monopole magnet, an elementary particle that researchers believe exists but have not yet seen in nature.
The development -- explored in a paper published in Science -- is a remarkable step forward in quantum research. A better understanding of the structure of monopoles and other topological entities is very valuable to scientists, in part because they appear in the models describing the first moments of the universe's existence and affect the properties of many different materials, such as metals.
"This was a very exciting experiment to perform," says former Amherst postdoctoral research associate Michael Ray, now a visiting assistant professor at Union College and the lead author of the paper. "These kinds of defects are relevant to theories that describe the early universe, so observing this monopole gives us a glimpse into those moments but on a much more accessible scale."
Hall and Ray manipulated a gas of rubidium atoms prepared in a nonmagnetic state near absolute zero temperature in an atomic refrigerator in Hall's lab in Amherst's Merrill Science Center. Under these extreme conditions, they were able to create monopoles in the quantum field of the ultracold gas.
"In this nonmagnetic state, a structure was created in the field describing the gas resembling the magnetic monopole particle as described in grand unified theories of particle physics," said Aalto University (Finland) Academy Research Fellow Mikko Mttnen, a collaborator on the team who led the theoretical analysis of the monopole. "Previously, we have used the gas to detect a monopole within a so-called synthetic magnetic field, but there has been no monopole in the quantum field describing the gas itself. Now we have finally witnessed the quantum-mechanical monopole."
Ordinarily, magnetic poles come in pairs: each magnet has both a north pole and a south pole. As the name suggests, however, a magnetic monopole is a magnetic particle possessing only a single, isolated pole -- a north pole without a south pole, or vice versa. Despite extensive experimental searches, in everything from lunar samples (moon rock) to ancient fossilized minerals, no observation of a naturally occurring magnetic monopole has yet been confirmed.
In this case, the gas is in a nonmagnetic state, and no quantum whirlpools or monopoles are created in the synthetic magnetic field, Mttnen continued. However, quantum-mechanical magnetic order prevailed in the sample itself, and the team was able to manipulate it with adjustments to an externally applied magnetic field to generate the quantum-mechanical monopoles.
"In the experiment, the control of those magnetic fields must be stable to a small fraction of the size of the Earth's magnetic field," said Hall. "The main experimental challenge we faced was to prepare the ultracold gas under highly sensitive conditions, in which field fluctuations due to the motion of metal objects or power-line variations can make observation of the monopoles difficult."
The team's result is further evidence that quantum-mechanical monopole structures do exist in nature, he said, even if magnetic monopoles themselves remain at large.
A 1991 Amherst graduate, Hall is an experimental physicist specializing in Bose-Einstein condensation. He received A.M. and Ph.D. degrees from Harvard University and did postgraduate work at the University of Colorado.
This material is based upon work supported by the National Science Foundation under Grant No. PHY-1205822, the Academy of Finland (grant nos. 251748, 135794, and 272806), the Finnish Doctoral Programme in Computational Sciences and the Magnus Ehrnrooth Foundation.
Story Source:
The above story is based on materials provided by Amherst College . Note: Materials may be edited for content and length.
Journal Reference:
M. W. Ray, E. Ruokokoski, K. Tiurev, M. Mttnen, D. S. Hall. Observation of isolated monopoles in a quantum field. Science, 2015; 348 (6234): 544-547 DOI: 10.1126/science.1258289
Cite This Page:
</MainBody>
    </Article>
    <Article id="36">
        <date>Thu Apr 30 21:16:00 EEST 2015</date>
        <title>Boosting the body's natural ability to fight urinary tract infections</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/wynP6RguSno/150430141600.htm</Link>
        <Description>Urinary tract infections (UTIs) are common, and widespread antibiotic resistance has led to urgent calls for new ways to combat them. Researchers report that an experimental drug that stabilizes a protein called HIF-1alpha protects human bladder cells and mice against a major UTI pathogen. The drug might eventually provide a therapeutic alternative or complement to standard antibiotic treatment.</Description>
        <MainBody>Boosting the body's natural ability to fight urinary tract infections
Date:
University of California, San Diego Health Sciences
Summary:
Urinary tract infections (UTIs) are common, and widespread antibiotic resistance has led to urgent calls for new ways to combat them. Researchers report that an experimental drug that stabilizes a protein called HIF-1alpha protects human bladder cells and mice against a major UTI pathogen. The drug might eventually provide a therapeutic alternative or complement to standard antibiotic treatment.
Share:
Total shares: 
FULL STORY
Urinary tract infections (UTIs) are common, and widespread antibiotic resistance has led to urgent calls for new ways to combat them. Researchers at University of California, San Diego School of Medicine and Skaggs School of Pharmacy and Pharmaceutical Sciences report that an experimental drug that stabilizes a protein called HIF-1alpha protects human bladder cells and mice against a major UTI pathogen. The drug might eventually provide a therapeutic alternative or complement to standard antibiotic treatment.
The study is published April 30 by PLOS Pathogens.
HIF-1alpha is known to influence the innate immune response, the body's first line of defense against intruding pathogens. Like many regulator proteins, HIF-1alpha is relatively short-lived. To increase HIF-1alpha levels, researchers have developed drugs that delay its breakdown. This same pathway has been the target for drugs now in advanced clinical trials for treatment of anemia.
In this study, Victor Nizet, MD, professor of pediatrics and pharmacy, and colleagues explored the use of HIF-1alpha-stabilizing drugs to boost the innate immune response to uropathogenic E.coli (UPEC) bacteria, a major cause of UTIs. In healthy human urinary tract cells, treatment with the drugs increased HIF-1alpha levels. Such cells were then more resistant to UPEC attachment, invasion and killing than human urinary tract cells with normal HIF-1alpha levels.
Using a mouse model of UTI, the researchers showed that administration of HIF-1alpha stabilizers directly into the bladder protected against UPEC infection. They also found that invasion of bladder cells, a critical early step in the infection process, was reduced in treated mice compared to untreated mice.
To verify the importance of HIF-1alpha in the defense against UPEC infection, the researchers studied mice with reduced HIF-1alpha levels. Exposed to UPEC, these mice were more susceptible to bladder infection, and pre-treatment with HIF-1alpha stabilizers made no difference. This demonstrates that the drugs combat UTIs through their effect on HIF-1alpha.
Finally, the researchers examined whether treatment with HIF-1alpha stabilizers would be beneficial against an established UTI. To do this, they infected mice with UPEC first and then administered the drugs into the bladder six hours later. The treated mice had a more than 10-fold reduction in bladder colonization with the bacteria, demonstrating that HIF-1alpha stabilization is beneficial even after the initial infection.
"The ultimate goal of this research will be to advance HIF-1alpha stabilizers toward clinical trials in humans, using versions of the drug that can be taken orally and reach the urinary tract," Nizet said.
Story Source:
The above story is based on materials provided by University of California, San Diego Health Sciences . The original article was written by Heather Buschman. Note: Materials may be edited for content and length.
Journal Reference:
Ann E. Lin, Federico C. Beasley, Joshua Olson, Nadia Keller, Robert A. Shalwitz, Thomas J. Hannan, Scott J. Hultgren, Victor Nizet. Role of Hypoxia Inducible Factor-1 (HIF-1) in Innate Defense against Uropathogenic Escherichia coli Infection. PLOS Pathogens, April 2015 DOI: 10.1371/journal.ppat.1004818
Cite This Page:
</MainBody>
    </Article>
    <Article id="37">
        <date>Thu Apr 30 20:50:01 EEST 2015</date>
        <title>Mammals not the only animals to feed embryo during gestation</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/RlHU6ycAKSs/150430135001.htm</Link>
        <Description>For over a century, the scientific understanding of matrotrophy of an embryo developing inside a mom's body has come from vertebrates. This process was thought to be infrequent among the other 33 or so major groups or phyla of animals. But a new major study reports that matrotrophy has evolved in at least 21 of 34 animal phyla.</Description>
        <MainBody>Mammals not the only animals to feed embryo during gestation
Date:
Field Museum
Summary:
For over a century, the scientific understanding of matrotrophy of an embryo developing inside a mom's body has come from vertebrates. This process was thought to be infrequent among the other 33 or so major groups or phyla of animals. But a new major study reports that matrotrophy has evolved in at least 21 of 34 animal phyla.
Share:
Total shares: 
FULL STORY
How and when does mom feed her embryo? We humans, like most mammals, experience pregnancy where a mother supplies nutrition directly to the embryo as it develops. But we're in the minority.
Most members of the animal kingdom supply eggs with nutritious yolk before they are fertilized. With this yolk supply, fertilized eggs develop as embryos in the environment outside the mother's body. For over a century, the scientific understanding of matrotrophy ("mother-feeding") of an embryo developing inside a mom's body has come from vertebrate animals, especially mammals like us. This process was thought to be infrequent among the other 33 or so major groups or phyla of animals. Not so, according to a major study published today in the journal Biological Reviews.
Scott Lidgard at Chicago's Field Museum and Andrey Ostrovsky at St. Petersburg State University led the international team that reports matrotrophy has evolved in at least 21 of 34 animal phyla. By comparing the examples of matrotrophy with the placement of species on the DNA-based tree of life, the authors propose that matrotrophy has evolved independently in 140 or more different animal lineages, and is often associated with live birth. According to the study, previous work scattered through the specialized scientific literature had talked about matrotrophy in many invertebrate groups, but it had never been appreciated just how common it might be, and how frequently it had evolved. There are more species of flatworms that employ mother-feeding than there are species of mammals!
The repeated shifts from ancestors whose embryos depended entirely on yolk to descendants in which moms supply nutrients directly, either partly or wholly supplanting yolk, is only part of the story. Animals have evolved a host of different mechanisms for carrying out the job of feeding the embryo: direct transfer with intimate cellular contact as in placental mammals, uterine "milk glands" in scorpions and some insects, nutrient secretion into fluid-filled body cavities where embryos are freely suspended, accelerated development of embryo digestive tracts that allows feeding on maternal tissues, even cannibalism of sibling eggs or embryos.
Mechanisms among invertebrate animals are less complex than among vertebrates, but are more diverse. We know a lot, for example, about what enables a human baby with a different genotype than its mother to develop in intimate contact with the placenta without triggering an attack by mom's immune system on the "foreign" body. How have independently evolved modes of mother-feeding in different animal groups eluded or accommodated dangerous immune responses?
Many of the invertebrate groups with matrotrophy remain stuck on the ocean floor for most of their lives, or are parasites that live part or most of their existence inside other host organisms. Vertebrates have neither of these life histories. If matrotrophy has evolved dozens or hundreds of times among distantly related animal groups, what factors of animal physiologies, life histories, and environments have shaped the course of natural selection? The sheer diversity of mothers feeding embryos shown by this study points toward a whole range of scientific questions for which animals other than mammals may hold the keys.
Story Source:
The above story is based on materials provided by Field Museum . Note: Materials may be edited for content and length.
Journal Reference:
Andrew N. Ostrovsky, Scott Lidgard, Dennis P. Gordon, Thomas Schwaha, Grigory Genikhovich, Alexander V. Ereskovsky. Matrotrophy and placentation in invertebrates: a new paradigm. Biological Reviews, 2015; DOI: 10.1111/brv.12189
Cite This Page:
</MainBody>
    </Article>
    <Article id="38">
        <date>Thu Apr 30 20:49:33 EEST 2015</date>
        <title>Rupture along the Himalayan Front</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/LVLOccoMalA/150430134933.htm</Link>
        <Description>In an article in March, geologists noted that the 700-km-long 'central seismic gap' is the most prominent segment of the Himalayan front not to have ruptured in a major earthquake during the last 200-500 years. This prolonged seismic quiescence has led to the proposition that this region, with a population of more 10 million, is overdue for a great earthquake.'</Description>
        <MainBody>Rupture along the Himalayan Front
Date:
Geological Society of America
Summary:
In an article in March, geologists noted that the 700-km-long 'central seismic gap' is the most prominent segment of the Himalayan front not to have ruptured in a major earthquake during the last 200-500 years. This prolonged seismic quiescence has led to the proposition that this region, with a population of more 10 million, is overdue for a great earthquake.'
Share:
Total shares: 
FULL STORY
Date and rupture patches for large historical Himalayan earthquakes (Rajendran and Rajendran, 2005; Kumar et al., 2006) with reference to the Uttarakhand region of the central seismic gap, and the physiographic transition 2 of Uttarakhand (UPT2 ) and Nepal (NPT2 ) (Wobus et al., 2006a). (B) Simplified geologic map for area shown in A (Clrier et al., 2009a; Webb et al., 2011). Focal mechanisms of all earthquakes within the recording period (Mw 5-7) are shown with location as white circle. Earthquake locations are based on Ni and Baranzangi (1984) and the National Earthquake Information Center (NEIC) catalog (earthquake.usgs.gov). Focal mechanisms are based on Ni and Baranzangi (1984) or the Global Centroid-Moment-Tensor (CMT) catalog (globalcmt.org). STD--South Tibetan Detachment; THS--Tethyan Himalayan Sequence; MCT--Main Central Thrust; GHS--Greater Himalayan Sequence; LHS--Lesser Himalayan Sequence; MBT--Main Boundary Thrust; MFT--Main Frontal Thrust.
Credit: Morell et al. and Lithosphere
Date and rupture patches for large historical Himalayan earthquakes (Rajendran and Rajendran, 2005; Kumar et al., 2006) with reference to the Uttarakhand region of the central seismic gap, and the physiographic transition 2 of Uttarakhand (UPT2 ) and Nepal (NPT2 ) (Wobus et al., 2006a). (B) Simplified geologic map for area shown in A (Clrier et al., 2009a; Webb et al., 2011). Focal mechanisms of all earthquakes within the recording period (Mw 5-7) are shown with location as white circle. Earthquake locations are based on Ni and Baranzangi (1984) and the National Earthquake Information Center (NEIC) catalog (earthquake.usgs.gov). Focal mechanisms are based on Ni and Baranzangi (1984) or the Global Centroid-Moment-Tensor (CMT) catalog (globalcmt.org). STD--South Tibetan Detachment; THS--Tethyan Himalayan Sequence; MCT--Main Central Thrust; GHS--Greater Himalayan Sequence; LHS--Lesser Himalayan Sequence; MBT--Main Boundary Thrust; MFT--Main Frontal Thrust.
Credit: Morell et al. and Lithosphere
Close
In their article for Lithosphere on 12 March, authors Kristin Morell and colleagues write, "The 700-km-long 'central seismic gap' is the most prominent segment of the Himalayan front not to have ruptured in a major earthquake during the last 200-500 years. This prolonged seismic quiescence has led to the proposition that this region, with a population of more 10 million, is overdue for a great earthquake. Despite the region's recognized seismic risk, the geometry of faults likely to host large earthquakes remains poorly understood."
A little more than a month on, the area experience a magnitude 7.8 earthquake, centered in Nepal (25 Apr. 2015).
In their study, Morell and colleagues use a series of complementary geomorphic and erosion rate data to define the ramp-flat geometry of the active detachment fault that is likely to host a large earthquake within the hinterland of the northwest Himalaya. Their analysis indicates that this detachment is sufficiently large to host another great earthquake in the western half of the central Himalayan seismic gap.
Specifically, their data sets point to a distinctive physiographic transition at the base of the high Himalaya in the state of Uttarakhand, India, characterized by abrupt strike-normal increases in channel steepness and a tenfold increase in erosion rates.
When combined with previously published geophysical imaging and seismicity data sets, Morell and colleagues interpret the observed spatial distribution of erosion rates and channel steepness to reflect the landscape response to spatially variable rock uplift due to a structurally coherent ramp-flat system of the Main Himalayan Thrust. They write, "Although it remains unresolved whether the kinematics of the Main Himalayan Thrust ramp involve an emergent fault or duplex, the landscape and erosion rate patterns suggest that the dcollement beneath the state of Uttarakhand provides a sufficiently large and coherent fault segment capable of hosting a great earthquake."
In conclusion, they note, "While this hypothesis remains speculative, it is supported by independent records of historical seismicity."
Story Source:
The above story is based on materials provided by Geological Society of America . Note: Materials may be edited for content and length.
Journal Reference:
K. D. Morell, M. Sandiford, C. P. Rajendran, K. Rajendran, A. Alimanovic, D. Fink, J. Sanwal. Geomorphology reveals active decollement geometry in the central Himalayan seismic gap. Lithosphere, 2015; DOI: 10.1130/L407.1
Cite This Page:
</MainBody>
    </Article>
    <Article id="39">
        <date>Thu Apr 30 20:49:31 EEST 2015</date>
        <title>Study results promising  for hepatitis C patients awaiting or completing liver transplant</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/UEKjVdI6TfA/150430134931.htm</Link>
        <Description>Hepatitis C patients who are awaiting a liver transplant or have completed one are a difficult group to cure because hepatitis C can come back after transplant. A recent trial showed that a large number of these patients can be cured with an oral regimen of daclatasvir, sofosbuvir and ribavirin. Treatment was well tolerated with few serious side effects.</Description>
        <MainBody>Study results promising  for hepatitis C patients awaiting or completing liver transplant
Date:
University of Texas Health Science Center at San Antonio
Summary:
Hepatitis C patients who are awaiting a liver transplant or have completed one are a difficult group to cure because hepatitis C can come back after transplant. A recent trial showed that a large number of these patients can be cured with an oral regimen of daclatasvir, sofosbuvir and ribavirin. Treatment was well tolerated with few serious side effects.
Share:
Total shares: 
FULL STORY
A number of new, highly effective oral treatments for various types of hepatitis C have been approved in the past few years. However, two groups who have not benefitted from the new treatments are patients with hepatitis C who have advanced liver disease and patients who have received a liver transplant but the advanced liver disease has returned because of hepatitis C.
"The problem for these patients is that unless the hepatitis C is cured, the virus continues circulating in their blood infecting the new liver, usually within a few months of transplant. One-third of them have cirrhosis again within five years," explained Fred Poordad, M.D., clinical professor of medicine and chief of hepatology at The University of Texas Health Science Center at San Antonio.
"This puts these patients back at high risk of dying from chronic hepatitis C or liver disease," said Dr. Poordad, principal investigator of the ALLY-1 study, who presented the results April 25 at The International Liver CongressTM of the European Association for the Study of the Liver (EASL) in Vienna, Austria.
The Phase III clinical trial evaluated a 12-week course of daclatasvir -- the new drug being evaluated -- combined with sofosbuvir and ribavirin for patients with chronic hepatitis C. Patients accepted into the trial either had a liver transplant with returning hepatitis C or had hepatitis C with advanced cirrhosis (scarring of the liver).
Study results showed an overall cure rate of 94 percent for patients with a liver transplant and returning hepatitis C, and 83 percent for patients with advanced cirrhosis.
The study's primary endpoints also were reached, with 95 percent of post-transplant genotype 1 patients and 82 percent of genotype 1 patients with advanced cirrhosis being cured 12 weeks after treatment. Patients with other genotypes of the disease were enrolled as well, with benefits seen in all groups.
Genotypes are subgroups or strains of a disease, such as hepatitis C. There are many subtypes of hepatitis C based on the geographic regions where the strain is most prevalent. Over time, each strain evolved differently so that treatments are based on the genotype of the disease. For example, genotype 1 is the type of hepatitis C most common in the United States and is the most difficult to treat.
The study regimen was well tolerated and showed few serious side effects. "Transplant patients take a variety of medications to prevent organ rejection that can complicate the treatment of hepatitis C. In ALLY-1, we saw no drug-to-drug interactions between transplant and hepatitis C therapies and no need to make close adjustments to patients' transplant-related drugs while they received the hepatitis C regimen," Dr. Poordad said.
The ALLY-1 study was conducted at five major transplant centers in San Antonio and Houston, Texas; Miami, Fla.; Ann Arbor, Mich., and Seattle, Wash.
Hepatitis C is a liver disease found worldwide that is spread though contact with blood or semen, such as shared drug injection needles, inadequate sterilization of medical equipment, unscreened blood and blood products, accidental needle sticks in the health profession, and sexual intercourse with a person who has hepatitis C. The disease also can be passed from mothers to their children through the birthing process.
According to the U.S. Centers for Disease Control and Prevention, 3.2 million people in the U.S. have chronic hepatitis C, and 70 to 80 percent do not have symptoms. Nonetheless, it is a serious disease that can lead to long-term health problems such as liver damage, liver failure, liver cancer and death. It is often discovered later, after significant liver damage has occurred.
In the U.S., people born between 1945 and 1965 have the highest risk of hepatitis C due to higher drug use. People in this age group are urged to have a one-time blood test for hepatitis C to detect the virus and begin receiving treatment, if necessary, before significant liver damage occurs. There is no vaccine to prevent hepatitis C.
Daclatasvir, a drug developed by Bristol-Myers Squibb, was approved in Europe in 2014 for use with other medications for genotypes 1 through 4 for the treatment of chronic hepatitis C in adults. It is also approved in Japan as well as many countries in Central and South America, the Middle East and Asia Pacific. Daclatasvir regimens also have been included in the EASL's recommendations for the treatment of hepatitis C in Europe.
Story Source:
The above story is based on materials provided by University of Texas Health Science Center at San Antonio . Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="40">
        <date>Thu Apr 30 20:48:52 EEST 2015</date>
        <title>Vitamin D toxicity rare in people who take supplements, researchers report</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/8MT-U20oRno/150430134852.htm</Link>
        <Description>Americans have low vitamin D levels, research shows, and as a result, vitamin D supplement use has climbed in recent years. Vitamin D has been shown to boost bone health and it may play a role in preventing diabetes, cancer, cardiovascular disease and other illnesses. In light of the increased use of vitamin D supplements, researchers set out to learn more about the health of those with high vitamin D levels. They found that toxic levels are actually rare.</Description>
        <MainBody>Vitamin D toxicity rare in people who take supplements, researchers report
Date:
Mayo Clinic
Summary:
Americans have low vitamin D levels, research shows, and as a result, vitamin D supplement use has climbed in recent years. Vitamin D has been shown to boost bone health and it may play a role in preventing diabetes, cancer, cardiovascular disease and other illnesses. In light of the increased use of vitamin D supplements, researchers set out to learn more about the health of those with high vitamin D levels. They found that toxic levels are actually rare.
Share:
Total shares: 
FULL STORY
Rochester, Minn. -- Over the last decade, numerous studies have shown that many Americans have low vitamin D levels and as a result, vitamin D supplement use has climbed in recent years. Vitamin D has been shown to boost bone health and it may play a role in preventing diabetes, cancer, cardiovascular disease and other illnesses. In light of the increased use of vitamin D supplements, Mayo Clinic researchers set out to learn more about the health of those with high vitamin D levels. They found that toxic levels are actually rare.
Their study appears in the May issue of Mayo Clinic Proceedings.
A vitamin D level greater than 50 nanograms per milliliter is considered high. Vitamin D levels are determined by a blood test called a serum 25-hydroxyvitamin D blood test. A normal level is 20-50 ng/mL, and deficiency is considered anything less than 20 ng/mL, according the Institute of Medicine (IOM).
The researchers analyzed data collected between 2002 and 2011 from patients in the Rochester Epidemiology Project, a National Institutes of Health-funded medical records pool that makes Olmsted County, Minn., the home of Mayo Clinic, one of the few places worldwide where scientists can study virtually an entire geographic population to identify health trends.
Of 20,308 measurements, 8 percent of the people who had their vitamin D measured had levels greater than 50 ng/mL, and less than 1 percent had levels over 100 ng/mL.
"We found that even in those with high levels of vitamin D over 50 ng/mL, there was not an increased risk of hypercalcemia, or elevated serum calcium, with increasing levels of vitamin D," says study co-author Thomas D. Thacher, M.D., a family medicine expert at Mayo Clinic.
Hypercalcemia, or high blood calcium, can occur when there are very high levels of vitamin D in the blood. Too much calcium in the blood can cause weakness, lead to kidney stones, and interfere with the heart and brain, and even be life threatening.
The Mayo researchers also found that women over age 65 were at the highest risk of having vitamin D levels above 50 ng/mL. The result was not surprising because that's a group that often takes vitamin D supplements, Dr. Thacher says.
Another notable outcome: The occurrence of high vitamin D levels over 50 ng/mL increased during the 10-year period of the study, from nine per 100,000 people at the start of the study up to 233 per 100,000 by the end.
"We were surprised by that degree of dramatic increase in vitamin D levels," Dr. Thacher says.
Only one case over the 10-year study period was identified as true acute vitamin D toxicity; the person's vitamin D level was 364 ng/mL. The individual had been taking 50,000 international units (IU) of vitamin D supplements every day for more than three months, as well as calcium supplements. The IOM-recommended upper limit of vitamin D supplementation for people with low or deficient levels is 4,000 IU a day.
It's important for doctors to ask their patients about the doses of vitamin D supplements that they are using, Dr. Thacher says, because even capsules containing as much as 50,000 IU of vitamin D are available without prescription. If taken on a daily basis, that amount could lead to toxicity.
Some natural sources of vitamin D include oily fish such as mackerel and salmon, fortified milk, and sunlight.
"Our bodies will naturally produce vitamin D when our skin is exposed to sunlight, however, we don't recommend excessive exposure to sun due to the risk of skin cancer," Dr. Thacher added.
In an accompanying editorial in Mayo Clinic Proceedings, Dr. Michael F. Hollick, Ph.D., M.D., describes vitamin D's dramatic medical history, the need for judicious dosing, but the importance of vitamin D supplementation in those with low or deficient levels.
"The evidence is clear that vitamin D toxicity is one of the rarest medical conditions and is typically due to intentional or inadvertent intake of extremely high doses," writes Hollick, a professor of medicine, physiology and biophysics at Boston University School of Medicine.
Story Source:
The above story is based on materials provided by Mayo Clinic . Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="41">
        <date>Thu Apr 30 20:48:10 EEST 2015</date>
        <title>UV radiations: NONO helps to mend the damage</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/6AkyRED8N4o/150430134810.htm</Link>
        <Description>A new molecular mechanism whereby human cells protect their genome from the detrimental effect of UV radiations has been discovered by researchers. NONO is a multifunctional protein involved in melanoma development and progression, in the cellular response to UV radiations.</Description>
        <MainBody>Sbarro Health Research Organization (SHRO)
Summary:
A new molecular mechanism whereby human cells protect their genome from the detrimental effect of UV radiations has been discovered by researchers. NONO is a multifunctional protein involved in melanoma development and progression, in the cellular response to UV radiations.
Share:
Total shares: 
FULL STORY
Researchers from the lab of Antonio Giordano, MD, PhD, Director and Founder of the Sbarro Institute for Cancer Research and Molecular Medicine at Temple University in Philadelphia, PA, have uncovered a new molecular mechanism whereby human cells protect their genome from the detrimental effect of UV radiations. UV radiations by directly causing harmful DNA lesions contribute to carcinogenesis and represent the main risk factor for skin cancer, including melanoma.
The study appeared as advanced online publication on Oncogene, a journal in cancer research from the Nature Publishing Group. Here, the authors investigated the function of NONO, a multifunctional protein involved in melanoma development and progression, in the cellular response to UV radiations.
Normally, cells respond to radiation-induced DNA damage by activating different checkpoints which allow to stall cell cycle and activate DNA repair pathways avoiding that damaged DNA is erroneously replicated in the S (DNA synthesis) phase or transmitted to daughter cells during mitosis. Luigi Alfano, PhD of the National Cancer Institute of Naples- Pascale Foundation - CROM- Cancer Research Center of Mercogliano, Italy and lead author of the study, showed that silencing NONO impairs cancer cell response following exposure to UV radiations. NONO-silenced cells, compared with control cells, fail to activate cell cycle checkpoints, continue to synthesize DNA and do not efficiently activate the biochemical cascade of events that ultimately lead to DNA repair. Further supporting the finding that NONO is involved in this cellular response, the authors found that NONO localizes at the sites of DNA damage where it favours the loading of other proteins which are key to this process.
"Our study provides an important missing link which contributes to further dissecting the complex cascade of events that orchestrate the cellular response to DNA damage," says Alfano.
"Considering that many studies are identifying NONO alterations in cancer, our findings will likely help to shed light on the molecular mechanisms of tumorigenesis, especially in tumour types like melanoma, in which exposure to UV radiations plays such a prominent part. Our work also provides the preclinical framework supporting the development of new agents targeting NONO that could be used to sensitize cancer cells to a variety of drugs that cause DNA damage, such as common chemotherapy agents," states Francesca Pentimalli PhD from the National Cancer Institute of Naples co-corresponding of the study with Antonio Giordano,  Director of the Sbarro Institute for Cancer Research and Molecular Medicine, at Temple University, Philadelphia.
Permalink to this article
Story Source:
The above story is based on materials provided by Sbarro Health Research Organization (SHRO) . Note: Materials may be edited for content and length.
Journal Reference:
L Alfano, C Costa, A Caporaso, A Altieri, P Indovina, M Macaluso, A Giordano, F Pentimalli. NONO regulates the intra-S-phase checkpoint in response to UV radiation. Oncogene, 2015; DOI: 10.1038/onc.2015.107
Cite This Page:
</MainBody>
    </Article>
    <Article id="42">
        <date>Thu Apr 30 20:48:08 EEST 2015</date>
        <title>Compound kills various human pathogenic fungi, may improve human health</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/qw9FCLFiaZQ/150430134808.htm</Link>
        <Description>A simple chemical compound kills several major fungi that affect human health, researchers report. The compound also may have applications for fungal diseases that affect wheat and rice plants.</Description>
        <MainBody>Compound kills various human pathogenic fungi, may improve human health
Date:
Kansas State University
Summary:
A simple chemical compound kills several major fungi that affect human health, researchers report. The compound also may have applications for fungal diseases that affect wheat and rice plants.
Share:
Total shares: 
FULL STORY
Antifungal resistant clinical Candida albicans and Cryptococcus neoformans without, on the left panel, and with drimenol, on the right panel, were tested in vitro. Some treated fungi showed blebbing and rupturing of fungal cells, depicted with the arrows. Chemical structure of the tested compound is shown in top right corner.
Credit: Image courtesy of Kansas State University
Antifungal resistant clinical Candida albicans and Cryptococcus neoformans without, on the left panel, and with drimenol, on the right panel, were tested in vitro. Some treated fungi showed blebbing and rupturing of fungal cells, depicted with the arrows. Chemical structure of the tested compound is shown in top right corner.
Credit: Image courtesy of Kansas State University
Close
A now-patented substance from two Kansas State University researchers may be an all-purpose solution for stopping fungus.
Govindsamy Vediyappan, assistant professor of biology, and Duy Hua, university distinguished professor of chemistry, received a U.S. patent for their invention "Sesquiterpenes for Antifungal Applications."
Vediyappan, who researches the microbiology of various bacteria and fungi, and Hua, who specializes in synthetic compounds, developed and identified a simple chemical compound that kills several major fungi that affect human health. The compound also may have applications for fungal diseases that affect wheat and rice plants.
The fungal cells are almost the same as human cells, which make it difficult to develop drugs that kill a fungus without damaging human cells, Vediyappan said. Similarly, most antifungal drugs that exist are for one specific cellular target, such as the cell wall or cell membrane of the fungus.
The compound developed at Kansas State University may be an exception. In tests, researchers discovered the compound was very effective against numerous fungi.
"We found that this compound kills lots of different pathogenic fungi -- from ones that severely affect human health to ones that are not life-threatening but are annoying," Vediyappan said. "We also saw that it only takes a small amount of the compound to affect a broad spectrum of fungi."
Researchers tested the compound on the following fungi, killing most of the cells in each fungus sample:
 Candida albicans,including with drug resistance -- A common resident fungus in our mouth and gut that can get into the blood stream, kidneys, liver and spleen. The fungus is persistent in immune suppressed patients, such as those with cancer or HIV. Once inside the organ, it continues to multiply and invade, making it difficult to kill.
 Cryptococcus neoformans -- A fungus common in bird feces. When inhaled, it travels to the lungs and leads to the development of respiratory diseases and fungal meningitis.
 Trichophyton equinum,a dermatophyte -- The fungus often responsible for ringworm fungal disease in animals and in humans. It lives in plant debris and decaying materials.
 Aspergillus fumigatus -- A fungus that causes respiratory disease in people who have had organ transplants.
Positive results did not stop at the number of species the compound was effective on.
"Normally, fungi make mutations in their genes to become resistant to antifungals and sequester them via their secreted polysaccharide substances," Vediyappan said. "From our preliminary results, we found that the fungi were not able to develop resistance to our compound. We're thinking that may be because the compound is affecting multiple mechanisms or pathways in these fungi rather than a single pathway that can easily be modified for resistance development."
Some of the gene expression studies were conducted in collaboration with a research colleague in Canada.
While the research has focused on human health, Vediyappan said the compound also may be beneficial for food plants. Rice and wheat, for example, are susceptible to fungi such as Magnaporthe oryzae and Fusarium graminearum, which causes rice blast and fusarium head blight.
Researchers hope to work with the National Institutes of Health to conduct more studies with the compound. These tests would build off of ones with a worm model of Candidiasis that showed that the compound killed most of the fungi while not harming the worms.
The patent was issued to the Kansas State University Research Foundation, a nonprofit corporation responsible for managing technology transfer activities at the university.
The discovery was made possible through support from the Kansas IDeA Network of Biomedical Research Excelence, or KINBRE; Kansas State University's Johnson Cancer Research Center; and funding for Hua's research and for Vediyappan's research.
The results were presented in meetings, and the researchers will publish their findings with the compound in the near future.
Story Source:
The above story is based on materials provided by Kansas State University . The original article was written by Greg Tammen. Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="43">
        <date>Thu Apr 30 19:48:41 EEST 2015</date>
        <title>How aspirin fights colorectal cancer</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/Pr9WOBJRnME/150430124841.htm</Link>
        <Description>Taking aspirin reduces a person's risk of colorectal cancer, but the molecular mechanisms involved have remained unknown until a recent discovery. discovered that aspirin might exert its chemopreventive activity against colorectal cancer, at least partially, by normalizing the expression of epidermal growth factor receptor (EGFR) in gastrointestinal precancerous lesions. EGFR is overexpressed in about 80 percent of cases involving colorectal cancer, the third-leading cause of cancer-related death in the United States.</Description>
        <MainBody>How aspirin fights colorectal cancer
Date:
University of Minnesota
Summary:
Taking aspirin reduces a person's risk of colorectal cancer, but the molecular mechanisms involved have remained unknown until a recent discovery. discovered that aspirin might exert its chemopreventive activity against colorectal cancer, at least partially, by normalizing the expression of epidermal growth factor receptor (EGFR) in gastrointestinal precancerous lesions. EGFR is overexpressed in about 80 percent of cases involving colorectal cancer, the third-leading cause of cancer-related death in the United States.
Share:
Total shares: 
FULL STORY
Taking aspirin reduces a person's risk of colorectal cancer, but the molecular mechanisms involved have remained unknown until a recent discovery by The Hormel Institute, University of Minnesota.
Researchers led by The Hormel Institute's Executive Director Dr. Zigang Dong and Associate Director Dr. Ann M. Bode, who co-lead the Cellular &amp; Molecular Biology section, discovered that aspirin might exert its chemopreventive activity against colorectal cancer, at least partially, by normalizing the expression of epidermal growth factor receptor (EGFR) in gastrointestinal precancerous lesions. EGFR is overexpressed in about 80 percent of cases involving colorectal cancer, the third-leading cause of cancer-related death in the United States.
Recently published in the open-access journal EBioMedicine, the Hormel Institute's study revealed a previously unknown functional association between EGFR and COX-2 -- an enzyme associated with pain and inflammation -- during the development of colorectal cancer. The study also provides an explanation as to how taking aspirin can lower the risk of colorectal cancer in patients with familial adenomatous polyposis (FAP), a rare, inherited condition that causes extra tissue, or polyps, to form in the large intestine. Polyps left untreated almost always become cancerous by age 40.
For this study, The Hormel Institute partnered with Mayo Clinic researchers who provided tissue sections from recruited FAP patients who were classified as regular aspirin users or nonusers. Consistent clinical trial data strongly suggests that regular use of aspirin and other non-steroidal, anti-inflammatory drugs lowers a person's lifetime risk of developing colorectal cancer.
Institute researchers found that COX-2 might drive the formation of tumors, at least in part, through the upregulation of EGFR. Given that, researchers believe EGFR might be a novel target for preventing colorectal cancer.
"We found that EGFR overexpression is an early event in the formation of colorectal cancer that can be greatly reduced by regular use of aspirin," Dr. Zigang Dong said. "Our findings are highly interesting, but more research is needed."
A short commentary also published on EBioMedicine by Dr. Paola Patrignani of Italy's "G. d'Annunzio" University of Chieti, highlighting The Hormel Institute's latest paper while discussing the potential use of aspirin for cancer prevention.
"The accumulating data from randomized clinical trials provide the rationale to consider the potential role of daily aspirin use in colorectal cancer prevention and possibly other types of cancer," Patrignani wrote in her commentary titled "COX-2 and EGFR: partners in crime split by aspirin."
Some questions, however, need to be addressed, Patrignani wrote, before recommending the prophylactic use of aspirin for cancer prevention, such as whether the chemopreventive effect is dose-dependent and whether daily, low-dose aspirin affect other types of cancers in addition to colorectal cancer. Clinical studies should be performed, she added, to verify whether the coadministration of low-dose aspirin and possibly other antiplatelet agents may lead to overcoming the resistance to EGFR inhibitors in cancer treatment.
This latest research follows another paper published last December in EBioMedicine by The Hormel Institute's Dong/Bode team related to colorectal cancer that provided a promising strategy for preventing and treating the disease. That study showed evidence that the TXA2 pathway also plays an important role in the processes leading to colorectal cancer, and laid the groundwork for introducing a strategy to target TXA2 for colorectal cancer prevention, early detection and management.
While reliable biomarkers remain a serious issue for the early detection of colorectal cancer, The Hormel Institute's findings in that study suggest that circulating TXA2 levels might have a potential prognostic or predictive value for detecting colorectal cancer early. Work is underway to further confirm the biomarker's clinical performance.
This month, Dr. Zigang Dong also received a grant for more than $1.7 million over five years from the National Cancer Institute to continue his team's supercomputer-assisted development of agents that are more effective and less toxic in preventing and treating colorectal cancer.
Through the use of The Hormel Institute's two IBM supercomputers, Drs. Dong and Bode have discovered three small molecules highly effective at suppressing colon cancer cell growth by inhibiting -catenin, an enzyme strongly expressed in many cancer cell types that promotes growth and tumor formation.
Story Source:
The above story is based on materials provided by University of Minnesota . Note: Materials may be edited for content and length.
Journal Reference:
Haitao Li, Feng Zhu, Lisa A. Boardman, Lei Wang, Naomi Oi, Kangdong Liu, Xiang Li, Yang Fu, Paul J. Limburg, Ann M. Bode, Zigang Dong. Aspirin Prevents Colorectal Cancer by Normalizing EGFR Expression. EBioMedicine, 2015; DOI: 10.1016/j.ebiom.2015.03.019
Cite This Page:
</MainBody>
    </Article>
    <Article id="44">
        <date>Thu Apr 30 19:48:39 EEST 2015</date>
        <title>First global review of Arctic marine mammals</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/ybJMJnMo3lg/150430124839.htm</Link>
        <Description>A multinational study attempted to gauge the population trends of Arctic marine mammals and changes in their habitat, identify missing scientific information, and provide recommendations for the conservation of Arctic marine mammals over the next decades.</Description>
        <MainBody>First global review of Arctic marine mammals
Date:
NASA/Goddard Space Flight Center
Summary:
A multinational study attempted to gauge the population trends of Arctic marine mammals and changes in their habitat, identify missing scientific information, and provide recommendations for the conservation of Arctic marine mammals over the next decades.
Share:
Total shares: 
FULL STORY
Kristin Laidre, lead author of the new review on the status and future of Arctic marine mammals, is doing field work in Greenland in 2013.
Credit: Erik Born / Greenland Institute of Natural Resource
Kristin Laidre, lead author of the new review on the status and future of Arctic marine mammals, is doing field work in Greenland in 2013.
Credit: Erik Born / Greenland Institute of Natural Resource
Close
Many human communities want answers about the current status and future of Arctic marine mammals, including scientists who dedicate their lives to study them and indigenous people whose traditional ways of subsistence are intertwined with the fate of species such as ice seals, narwhals, walruses and polar bears.
But there are many unknowns about the current status of 11 species of marine mammals who depend on Arctic sea ice to live, feed and breed, and about how their fragile habitat will evolve in a warming world.
A recently published multinational study attempted to gauge the population trends of Arctic marine mammals and changes in their habitat, identify missing scientific information, and provide recommendations for the conservation of Arctic marine mammals over the next decades.
The Arctic sea ice cover, made of frozen seawater floating on top of the Arctic Ocean and its neighboring seas, naturally grows in the fall and winter and melts during the spring and summer every year. But over the past decades, the melt season has grown longer and the average extent of Arctic sea ice has diminished, changing the game for many Arctic marine mammals -- namely beluga, narwhal and bowhead whales; ringed, bearded, spotted, ribbon, harp and hooded seals; walruses; and polar bears.
"This research would not have been possible without support from NASA," said Kristin Laidre, lead author of the new study and a polar scientist with University of Washington in Seattle. "NASA backed us on research related to the biodiversity and ecology of Arctic marine mammals, as well as the development of metrics for the loss of sea ice, their habitat."
Laidre's team used the Arctic sea ice record derived from microwave measurements taken by NASA and Department of Defense satellites. This record began in late 1978, is uninterrupted, and relies on NASA-developed methods for processing the microwave data.
"It's really our best global view of the Arctic sea ice," said Harry Stern, author of the paper with Laidre and a mathematician specializing in sea ice and climate at University of Washington.
Stern divided the Arctic Ocean into 12 regions. Using daily sea ice concentration data from the satellite record, he calculated changes in the dates of the beginning of the melt season in spring and the start of the fall freeze-up from 1979 to 2013. He found that, in all regions but one, the melt season had grown longer (mostly by 5 to 10 weeks, and by 20 weeks in one region).
"Sea ice is critical for Arctic marine mammals because events such as feeding, giving birth, molting, and resting are closely timed with the availability of their ice platform," Laidre said. "It is especially critical for the ice-dependent species -- seals and polar bears. Ice seals use the sea ice platform to give birth and nurse pups during very specific weeks of the spring, and polar bears use sea ice for feeding, starting in late winter and continuing until the ice breaks up."
Pacific walrus use the floating pack ice both as a platform on which to rest between feeding bouts and as a passive transport around their habitat.
"Loss of sea ice has resulted in walrus hauling out on land in Alaska and Russia in massive numbers -- these land haul outs result in trampling of their young," Laidre said. "Also, now walrus must travel a longer way to reach their feeding areas, which is energetically costly."
In the case of Arctic whales, the changes in sea ice might benefit their populations, at least in the short term: the loss and earlier retreat of sea ice opens up new habitats and, in some areas of the Arctic, has also led to an increase in food production and the length of their feeding season.
In the future, Stern said higher-resolution satellite microwave data might come in handy when studying the interactions of Arctic marine mammals with their icy habitat.
"For example, we know that narwhals congregate in specific areas of the Arctic in the wintertime, so maybe a higher spatial resolution in these areas might help us better understand their relationship with the ice," Stern said. "But mainly, just continuing daily coverage is what's important for the long-term monitoring of habitat changes."
Story Source:
The above story is based on materials provided by NASA/Goddard Space Flight Center . Note: Materials may be edited for content and length.
Journal Reference:
Kristin L. Laidre, Harry Stern, Kit M. Kovacs, Lloyd Lowry, Sue E. Moore, Eric V. Regehr, Steven H. Ferguson, ystein Wiig, Peter Boveng, Robyn P. Angliss, Erik W. Born, Dennis Litovka, Lori Quakenbush, Christian Lydersen, Dag Vongraven, Fernando Ugarte. Arctic marine mammal population status, sea ice habitat loss, and conservation recommendations for the 21st century. Conservation Biology, 2015; DOI: 10.1111/cobi.12474
Cite This Page:
</MainBody>
    </Article>
    <Article id="45">
        <date>Thu Apr 30 19:48:37 EEST 2015</date>
        <title>Percentage of Texans without health insurance drops dramatically</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/guZQbFY-E-8/150430124837.htm</Link>
        <Description>The percentage of Texans without health insurance dropped 31 percent since enrollment began in the Affordable Care Act's Health Insurance Marketplace, according to a new report. Despite this improvement, Texas remains the state with the highest percentage of people without health insurance, and for the first time, Texas now has the largest number of uninsured residents in the country.</Description>
        <MainBody>Percentage of Texans without health insurance drops dramatically
Date:
Rice University
Summary:
The percentage of Texans without health insurance dropped 31 percent since enrollment began in the Affordable Care Act's Health Insurance Marketplace, according to a new report. Despite this improvement, Texas remains the state with the highest percentage of people without health insurance, and for the first time, Texas now has the largest number of uninsured residents in the country.
Share:
Total shares: 
FULL STORY
The percentage of Texans without health insurance dropped 31 percent since enrollment began in the Affordable Care Act's (ACA) Health Insurance Marketplace, according to a new report released by the Episcopal Health Foundation and Rice University's Baker Institute for Public Policy.
The report found that from September 2013 to March 2015, the percentage of uninsured adult Texans ages 18-64 dropped from 25 to 17 percent.
"This is a dramatic drop that's unprecedented in Texas," said Elena Marks, president and CEO of the Episcopal Health Foundation and a health policy scholar at the Baker Institute. "It's almost entirely attributable to newly insured individuals who purchased their own health insurance plans. The drop in the uninsured rate occurred across all income levels and age groups, including younger adults."
Despite this improvement, Texas remains the state with the highest percentage of people without health insurance, and for the first time, Texas now has the largest number of uninsured residents in the country.
The report found that the poorest Texans had the lowest drop in the uninsured rate. While there was a 45 percent drop in the uninsured rate for individuals earning more than $16,000 a year, the decrease was only 20 percent for those with lower incomes.
"The lowest-income Texans are almost four times more likely to be uninsured than those with higher incomes," said Vivian Ho, the chair in health economics at Rice's Baker Institute, a professor of economics at Rice and a professor of medicine at Baylor College of Medicine. "This coverage gap has grown since 2013 primarily because the ACA Marketplace allows households above the federal poverty level to buy health insurance using subsidies. Those same levels of discounts are not available to Texans with incomes below the federal poverty level."
Individuals who earn below $16,000 are able to enroll in Medicaid in the 31 states that opted to expand Medicaid. In Texas, those same individuals are not eligible for Medicaid coverage because state leaders chose not to accept federal funding for Medicaid expansion.
"Unless Texas participates in an expanded Medicaid program or develops some other mechanism for covering the lowest-income Texans, the number who remain uninsured is not likely to change," Ho said. "Right now, those at the lowest incomes must rely on health care that is highly subsidized by county and state tax dollars, or get by without needed health care."
The 31 percent decrease in the rate of uninsured Texans was similar to drops in other states that did not expand Medicaid coverage, but much lower than the 53 percent average decrease in states that did expand Medicaid coverage, the report found.
The report is the 11th in a series on the implementation of the ACA in Texas co-authored by Marks and Ho.
The Health Reform Monitoring Survey (HRMS) is a quarterly survey of adults ages 18-64 that began in 2013. Today's report is a summary of data extracted from the HRMS Surveys in Texas administered between September 2013 and March 2015.
The full report can be found at: http://bakerinstitute.org/research/effects-affordable-care-act-health-insurance-coverage-texas-march-2015/
Story Source:
The above story is based on materials provided by Rice University . Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="46">
        <date>Thu Apr 30 19:48:35 EEST 2015</date>
        <title>Busy Americans can reap health benefits by balancing protein intake throughout the day</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/EYJDN-bVXXU/150430124835.htm</Link>
        <Description>Researchers conducted a review of the current scientific literature on protein consumption and found that a moderate increase in protein consumption at each meal, balanced throughout the day, can lead to significant improvements.</Description>
        <MainBody>Busy Americans can reap health benefits by balancing protein intake throughout the day
Date:
University of Missouri-Columbia
Summary:
Researchers conducted a review of the current scientific literature on protein consumption and found that a moderate increase in protein consumption at each meal, balanced throughout the day, can lead to significant improvements.
Share:
Total shares: 
FULL STORY
Research has shown that eating more protein can support weight loss and prevent weight gain by boosting metabolism, increasing feelings of fullness and helping the body retain muscle while losing fat. However, many Americans are not consuming enough protein in a balanced way to achieve these effects. University of Missouri researcher Heather Leidy and her colleagues conducted a review of the current scientific literature on protein consumption and found that a moderate increase in protein consumption at each meal, balanced throughout the day, can lead to significant improvements.
To help individuals integrate more protein into their diets, Leidy, an assistant professor in the MU Department of Nutrition and Exercise Physiology, provides several recommendations based on her and others' research:
Eat breakfast.
"Breakfast, in general, provides benefits for appetite control and satiety, or feelings of fullness," Leidy said. "Eating a protein-rich breakfast containing about 30 grams of protein leads to even greater satiety throughout the day and can reduce unhealthy snacking by improving appetite control."
Evenly distribute protein intake throughout the day.
Leidy said individuals should aim for a diet that contains 1.2 -- 1.6 grams of protein per kilogram of body weight. For example, a 150-pound woman who wants to lose weight or prevent weight gain should eat approximately 90-100 grams of protein a day.
"This amount of protein has been shown to promote weight and fat losses while preserving lean mass," Leidy said. "Additionally, new evidence also indicates that spreading this amount evenly throughout the day is important. Thus, eating approximately 30 grams of high-quality protein at each meal appears to be necessary for these benefits."
Plan ahead.
Leidy said that individuals may think eating 30 grams of protein for breakfast sounds too difficult, but planning ahead can make it easier to accomplish.
"Most people eat enough protein in the evening," Leidy said. "Take whatever source of protein you ate for dinner -- whether that's a steak or a pork chop -- and eat it for breakfast along with Greek yogurt or include it in a pre-made breakfast casserole with eggs, which can easily get you to 30 grams of protein in the morning."
Add a little protein to every meal, especially at breakfast and lunch.
"We want people to know that they don't have to consume impractical amounts of protein," Leidy said. "Although most Americans don't consume the amount of protein necessary to achieve benefits, such as increased feelings of fullness, the research suggests that individuals only need to add an additional 10-15 grams of high-quality protein, such as eggs, beef, pork or dairy, at breakfast and lunch to achieve the recommended amount."
Consume high-quality protein.
Not all proteins are created equal. High-quality, or "complete," proteins found in animal-based foods such as beef, pork, poultry, fish, eggs and dairy products contain all the essential amino acids and are easily digestible. Most plant-based proteins found in vegetables and grains are considered lower quality, or "incomplete," proteins because they lack one or more essential amino acids and are less digestible.
Story Source:
The above story is based on materials provided by University of Missouri-Columbia . The original article was written by Sarah Clinton. Note: Materials may be edited for content and length.
Journal Reference:
H. J. Leidy, P. M. Clifton, A. Astrup, T. P. Wycherley, M. S. Westerterp-Plantenga, N. D. Luscombe-Marsh, S. C. Woods, R. D. Mattes. The role of protein in weight loss and maintenance. American Journal of Clinical Nutrition, 2015; DOI: 10.3945/ajcn.114.084038
Cite This Page:
</MainBody>
    </Article>
    <Article id="47">
        <date>Thu Apr 30 19:41:16 EEST 2015</date>
        <title>First embryonic stem cell therapy safety trial in Asian patients</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/L_1p5SMjL9g/150430124116.htm</Link>
        <Description>A clinical trial for patients with degenerative eye diseases is the first to test the safety of an embryonic stem cell therapy for people of Asian descent. The study, which followed four individuals for a year after they were treated with embryonic stem cell-derived retinal pigment epithelial cells for macular degeneration, observed no serious side effects (tumor growth or other unexpected effects) related to the therapy.</Description>
        <MainBody>First embryonic stem cell therapy safety trial in Asian patients
Date:
Cell Press
Summary:
A clinical trial for patients with degenerative eye diseases is the first to test the safety of an embryonic stem cell therapy for people of Asian descent. The study, which followed four individuals for a year after they were treated with embryonic stem cell-derived retinal pigment epithelial cells for macular degeneration, observed no serious side effects (tumor growth or other unexpected effects) related to the therapy.
Share:
Total shares: 
FULL STORY
The cells at the edge of the pigmented cluster displayed typical morphology of hRPE cells with light, medium, dark pigmentation status.
Credit: CHA Biotech Co., Ltd.
The cells at the edge of the pigmented cluster displayed typical morphology of hRPE cells with light, medium, dark pigmentation status.
Credit: CHA Biotech Co., Ltd.
Close
A clinical trial in the Republic of Korea for patients with degenerative eye diseases is the first to test the safety of an embryonic stem cell therapy for people of Asian descent. The study, which followed four individuals for a year after they were treated with embryonic stem cell-derived retinal pigment epithelial cells for macular degeneration, observed no serious side effects (tumor growth or other unexpected effects) related to the therapy. The researchers report the results on April 30 in Stem Cell Reports, the journal of the International Society for Stem Cell Research.
"This is mainly a safety study, and the goal is to prevent the progress of disease. So we were pleasantly surprised to see an actual improvement in visual acuity in the patients," says lead author Won Kyung Song of CHA University's Department of Ophthalmology. "However, this is a preliminary result. The positive responses from the patients need to be interpreted cautiously until controlled phase II studies are carried out."
The Korean trial was a collaborative effort between scientists at CHA University and stem cell pioneer Robert Lanza at Ocata Therapeutics (formerly known as Advanced Cell Technology). Lanza previously led a clinical trial in the United States--published November 2014 in the Lancet --that demonstrated embryonic stem cells could be used safely for patients with degenerative eye diseases, but the patient sample was Caucasian with the exception of one African-American.
The patients in both trials either had age-related macular degeneration or Stargardt's macular dystrophy, the leading forms of adult and juvenile blindness in the developed world. Both are currently incurable. An embryonic stem cell-derived retinal cell therapy is an attractive option because they can be used to regrow the retina cells that are lost in both diseases.
"Embryonic stem cells are among the most complex/dynamic clinical therapies ever proposed," Song says. "It is important that the clinical trials are carried out in a safe and responsible fashion."
CHA Biotech Co., Ltd., the sponsor company of this clinical study, is planning to get approval from the Korean Ministry of Food and Drug Safety to carry out phase II clinical trials with Stargardt's macular dystrophy this year and to continue dose escalation with the age-related macular degeneration trial. CHA Biotech Co., Ltd., hopes to get approval to commercialize the therapy in Korea within the next four years.
Story Source:
The above story is based on materials provided by Cell Press . Note: Materials may be edited for content and length.
Journal Reference:
Stem Cell Reports, Song et al. Treatment of Macular Degeneration Using Embryonic Stem Cell-Derived Retinal Pigment Epithelium: Preliminary Results in Asian Patients. Stem Cell Reports, April 2015 DOI: 10.1016/j.stemcr.2015.04.005
Cite This Page:
</MainBody>
    </Article>
    <Article id="48">
        <date>Thu Apr 30 19:41:13 EEST 2015</date>
        <title>Role of telomeres in plant stem cells discovered</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/Krji4L1413Q/150430124113.htm</Link>
        <Description>The development of an innovative technology that enables the monitoring of telomeres at the cellular level in plants has been described in a new article. The technique allows to demonstrate, for the first time, the role played by these structures in plant development and longevity of plants.</Description>
        <MainBody>Role of telomeres in plant stem cells discovered
Date:
Centro Nacional de Investigaciones Oncologicas (CNIO)
Summary:
The development of an innovative technology that enables the monitoring of telomeres at the cellular level in plants has been described in a new article. The technique allows to demonstrate, for the first time, the role played by these structures in plant development and longevity of plants.
Share:
Total shares: 
FULL STORY
The role played by telomeres in mammalian cells has been known for several years. It is also known that these non-coding DNA sequences, which are found at the ends of the chromosomes, protect them and are necessary to ensure correct cell division. What is more, the "youngest" cells have longer telomeres, and as these cells divide, the telomeres get shorter until they no longer permit new cell divisions. This telomere shortening process has also been associated with cancer, which emphasises the important implications of these structures, not only in the aging process, but also in the oncology field or other age-associated illnesses.
However, to date, little is known about the role played by telomeres in essential plant physiology processes such as growth. The reason is simple: the technologies that are necessary to be able to detect and measure the telomeres at the cellular level in these organisms had not been developed until now.
Now, a study led by CSIC researcher Ana Cao-Delgado, carried out at the CRAG, describes the development of an innovative technology that enables the monitoring of telomeres at the cellular level in plants. The technique allows to demonstrate, for the first time, the role played by these structures in plant development and longevity of plants.
The study, published in Cell Reports, is the result of a five-year interdisciplinary effort that has brought together CRAG plant experts and international leaders in the field of mammalian telomeres, such as the Telomeres and Telomerase Group led by Mara Blasco at the CNIO. Computational engineers from La Salle (Universitat Ramon Llull) and physicists from the Universidad de Barcelona (UB) and the University of Texas (ATM, USA) have also participated in this study.
LIFELONG GROWTH
Unlike animals, plants can grow throughout their entire lives. This is possible thanks to the meristems, namely, the tissues of undifferentiated cells found at the ends of stalks and in the stem cell niche located at the root apex. It is these two organs that provide the plant with new cells and enable it to grow throughout its entire life.
According to the researchers participating in the study, the cells with longer telomeres are precisely those of the stem cells and of the meristems. It is also in these areas where the telomerase enzyme (which enables the lengthening of the telomeres) is active, while there is no activity of this type in differentiated tissues such as leaves or branches.
"We see that telomeres and telomerase are essential for the renewal of the stem cells in the meristems and for the growth of the plant," explains Ana Cao, who goes on to add that: "those plants without active telomerase and in which the telomeres are abnormally shortened rapidly exhaust all of their stem cell resources and stop growing." The results show a vital relationship between the length of the telomeres, the stem cells and the longevity of these organisms.
USING PLANTS TO SEARCH FOR NOVEL THERAPEUTIC DRUGS
Mara Blasco indicates: "the fact that telomeres can now be measured at cell level in plants is extremely important as it shows the universal nature of both the technology and of what it is that the telomeres do. It also presents the possibility of using plants to look for compounds capable of regulating the telomeres in animal cells that could be useful in the treatment of illnesses associated with aging, including cancer."
The research project, which was carried out using the root of Arabidopsois thaliana, a laboratory model plant, included the use of an innovative technology for displaying the telomeres present in individual cells. This is an adaptation of telomapping, a technique developed by Mara Blasco's team at the CNIO for studying telomeres in mammalian cells.
The La Salle engineers and the UB physicists collaborated at a computational level in the development of algorithms for the analysis of images of intact plants in which the telomeres within the nucleus of the cells are observed.
Story Source:
The above story is based on materials provided by Centro Nacional de Investigaciones Oncologicas (CNIO) . Note: Materials may be edited for content and length.
Journal Reference:
Mary-Paz Gonzlez-Garca, Irina Pavelescu, Andres Canela, Xavier Sevillano, Katherine A. Leehy, Andrew D.L. Nelson, Marta Ibaes, Dorothy E. Shippen, Maria A. Blasco, Ana I. Cao-Delgado. Single-Cell Telomere-Length Quantification Couples Telomere Length to Meristem Activity and Stem Cell Development in Arabidopsis. Cell Reports, April 2015 DOI: 10.1016/j.celrep.2015.04.013
Cite This Page:
</MainBody>
    </Article>
    <Article id="49">
        <date>Thu Apr 30 19:41:07 EEST 2015</date>
        <title>Brain scan reveals out-of-body illusion</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/Tdv85Zmr1dQ/150430124107.htm</Link>
        <Description>Neuroscientists have created an out-of-body illusion in participants placed inside a brain scanner. They then used the illusion to perceptually 'teleport' the participants to different locations in a room and show that the perceived location of the bodily self can be decoded from activity patterns in specific brain regions.</Description>
        <MainBody>Karolinska Institutet
Summary:
Neuroscientists have created an out-of-body illusion in participants placed inside a brain scanner. They then used the illusion to perceptually 'teleport' the participants to different locations in a room and show that the perceived location of the bodily self can be decoded from activity patterns in specific brain regions.
Share:
Total shares: 
FULL STORY
This is the visual perspective from one of the out-of-body positions. Illustration of study published in Current Biology April 2015.
Credit: Arvid Guterstam
This is the visual perspective from one of the out-of-body positions. Illustration of study published in Current Biology April 2015.
Credit: Arvid Guterstam
Close
The feeling of being inside one's own body is not as self-evident as one might think. In a new study from Sweden's Karolinska Institutet, neuroscientists created an out-of-body illusion in participants placed inside a brain scanner. They then used the illusion to perceptually 'teleport' the participants to different locations in a room and show that the perceived location of the bodily self can be decoded from activity patterns in specific brain regions.
The sense of owning one's body and being located somewhere in space is so fundamental that we usually take it for granted. To the brain, however, this is an enormously complex task that requires continuous integration of information from our different senses in order to maintain an accurate sense of where the body is located with respect to the external world. Studies in rats have shown that specific regions of the brain contain GPS-like 'place cells' that signal the rat's position in the room -- a discovery that was awarded the 2014 Nobel Prize in Physiology or Medicine. To date, however, it remains unknown how the human brain shapes our perceptual experience of being a body somewhere in space, and whether the regions that have been identified in rats are involved in this process.
In a new study, published in the scientific journal Current Biology, the scientists created an out-of-body illusion in fifteen healthy participants placed inside a brain scanner. In the experiment, the participants wore head-mounted displays and viewed themselves and the brain scanner from another part of the room. From the new visual perspective, the participant observes the body of a stranger in the foreground while their physical body is visible in the background, protruding from the bore of the brain scanner. To elicit the illusion, the scientist touches the participant's body with an object in synchrony with identical touches being delivered to the stranger's body, in full view of the participant.
"In a matter of seconds, the brain merges the sensation of touch and visual input from the new perspective, resulting in the illusion of owning the stranger's body and being located in that body's position in the room, outside the participant's physical body," says Arvid Guterstam, lead author of the present study.
In the most important part of the study, the scientists used the out-of-body illusion to perceptually 'teleport' the participants between different places in the scanner room. They then employed pattern recognition techniques to analyze the brain activity and show that the perceived self-location can be decoded from activity patterns in specific areas in the temporal and parietal lobes. Furthermore, the scientists could demonstrate a systematic relationship between the information content in these patterns and the participants' perceived vividness of the illusion of being located in a specific out-of-body position.
"The sense of being a body located somewhere in space is essential for our interactions with the outside world and constitutes a fundamental aspect of human self-consciousness," says Arvid Guterstam. "Our results are important because they represent the first characterization of the brain areas that are involved in shaping the perceptual experience of the bodily self in space."
One of the brain regions from which the participants' perceived self-location could be decoded was the hippocampus -- the structure in which the Nobel Prize awarded 'place cells' have been identified.
"This finding is particularly interesting because it indicates that place cells are not only involved in navigation and memory encoding, but are also important for generating the conscious experience of one's body in space," says principal investigator Henrik Ehrsson, professor at the Department of Neuroscience.
Story Source:
The above story is based on materials provided by Karolinska Institutet . Note: Materials may be edited for content and length.
Journal Reference:
Arvid Guterstam, Malin Bjrnsdotter, Giovanni Gentile &amp; Henrik Ehrsson. Posterior Cingulate Cortex Integrates the Senses of Self-location and Body Ownership. Current Biology,, 30 April 2015 DOI: 10.1016/j.cub.2015.03.059
Cite This Page:
</MainBody>
    </Article>
    <Article id="50">
        <date>Thu Apr 30 19:41:03 EEST 2015</date>
        <title>Keen sense of touch allows bats to fly with breathtaking precision</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/bLDLjAuObBc/150430124103.htm</Link>
        <Description>Bats fly with breathtaking precision because their wings are equipped with highly sensitive touch sensors, cells that respond to even slight changes in airflow, researchers demonstrated.</Description>
        <MainBody>Keen sense of touch allows bats to fly with breathtaking precision
Date:
Johns Hopkins University
Summary:
Bats fly with breathtaking precision because their wings are equipped with highly sensitive touch sensors, cells that respond to even slight changes in airflow, researchers demonstrated.
Share:
A brown bat in flight maneuvers in search of prey.
Credit: Johns Hopkins University
A brown bat in flight maneuvers in search of prey.
Credit: Johns Hopkins University
Close
Bats fly with breathtaking precision because their wings are equipped with highly sensitive touch sensors, cells that respond to even slight changes in airflow, researchers have demonstrated for the first time.
Scientists from Johns Hopkins University, as well as Columbia University and the University of Maryland, determined how the sense of touch plays a key role in powered flight. In a paper published April 30 in the journal Cell Reports, they show how sensory receptors in bat wings send information about airflow to neurons in the brain, enabling the bat to make split-second flight control adjustments.
"Until now no one had investigated the sensors on the bat's wing, which allow it to serve as more than a propeller, a flipper, an airplane wing or any simple airfoil," said Johns Hopkins neuroscientist Cynthia F. Moss, one of the senior authors and a professor in the Department of Psychological and Brain Sciences in the Krieger School of Arts and Sciences. "These findings can inform more broadly how organisms use touch to guide movement."
Moss and the team studied the big brown bat, a common species found throughout North America. Bats are the only mammals capable of true powered flight, able to reach speeds of 7 to 20 mph with the sort of aerial maneuverability humans only wish they could engineer.
The team found that the evolutionary process that allowed bats to form wings resulted in unusual tactile circuitry that not only enhances control during flight, but also allows bats to use their wings to climb, cradle their young and capture insects.
First, they discovered an array of sensory receptors in bat wings -- a significant number of which are clustered at the base of tiny hairs that cover the appendages. That placement of these touch cells, both lanceolate endings and Merkel cells, allows the bat, while flying, to sense changes in airflow as air ruffles the hairs.
When the team stimulated these hairs with brief air puffs, neurons in the bat's primary somatosensory cortex responded with precisely timed but sparse bursts of activity, suggesting this circuitry helped guide bats during fast, dynamic flight.
The team also found that the innervation of bat wings -- the distribution in and supply of nerves to the wings -- is unlike that of other mammalian forelimbs, a clue into how wings grew in bats during evolution. The researchers were surprised to discover that neurons in the wing skin connected not only to the higher parts of the spinal cord where forelimbs typically connect, but also to lower parts of the spinal cord that would normally only innervate an animal's trunk.
These findings lay the groundwork for understanding how bats use sensory information to fly with precision in the dark and catch prey midair. The information, researchers say, could eventually help people design air vehicles that better negotiate obstacles by sensing and adjusting to air turbulence.
Video: https://www.youtube.com/watch?v=d9m-ERCYAqI&amp;feature=youtu.be
Story Source:
The above story is based on materials provided by Johns Hopkins University . The original article was written by Jill Rosen. Note: Materials may be edited for content and length.
Journal Reference:
Marshall et al. Somatosensory Substrates of Flight Control in Bats. Cell Reports, 2015 DOI: 10.1016/j.celrep.2015.04.001
Cite This Page:
</MainBody>
    </Article>
    <Article id="51">
        <date>Thu Apr 30 19:40:59 EEST 2015</date>
        <title>Wild bearded capuchin monkeys really know how to crack a nut</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/bx_UHT1VfJs/150430124059.htm</Link>
        <Description>When it comes to cracking nuts, wild bearded capuchin monkeys are more skilled than anyone had given them credit for, according to researchers.</Description>
        <MainBody>Wild bearded capuchin monkeys really know how to crack a nut
Date:
Cell Press
Summary:
When it comes to cracking nuts, wild bearded capuchin monkeys are more skilled than anyone had given them credit for, according to researchers.
Share:
This image shows the male capuchin monkey, Jatob, cracking a tucum nut.
Credit: Dorothy Fragaszy
This image shows the male capuchin monkey, Jatob, cracking a tucum nut.
Credit: Dorothy Fragaszy
Close
When it comes to cracking nuts, wild bearded capuchin monkeys are more skilled than anyone had given them credit for, according to researchers who report new findings in the Cell Press journal Current Biology on April 30.
The monkeys are known to use stone "hammers" to crack nuts. The new study shows that the monkeys are quite careful about the amount of force delivered to those nuts. They adjust the force applied with each strike based on the condition of the nutshell, making it less likely that they'll end up smashing the tasty kernel inside.
"Wild bearded capuchin monkeys dynamically modulate their strikes based on the outcome of the preceding strike while using stone hammers to crack nuts," says Madhur Mangalam of the University of Georgia at Athens. "Until now, this level of dexterity was not suspected of any monkey."
Mangalam's graduate advisor, Dorothy Fragaszy, and her colleagues have studied nut-cracking in wild bearded capuchin monkeys since 2005, when they established the EthoCebus research project. They were especially curious how the monkeys managed to crack such hard nuts. They also wondered whether the monkeys might change their nut-cracking approach with nuts that are softer.
In the new study, the researchers videotaped 14 capuchin moneys cracking nuts. They carefully analyzed the tapes to determine the height and velocity of each and every strike. It typically takes several strikes with a stone to reach the nut inside.
And what they discovered came as quite a surprise.
"It was a 'eureka' moment when we realized that the monkeys modulated the strikes systematically according to the condition of the nut following the preceding strike," Mangalam says.
They had expected the monkeys to maintain the force of their strikes within a certain range, or possibly to increase it until the nuts cracked. It never crossed their minds that the monkeys might show such a sophisticated ability to match their action to the physical state of the nut. But that's exactly what they did.
"Our finding opens our eyes to the fact that non-human primates modulate their actions with a tool to accommodate the rapidly changing requirements of the task, which is a cognitive accomplishment," Mangalam says.
The researchers now plan to examine whether other species make adjustments in tool use on the fly. They'll also explore how this kind of dexterity influences each species' tool-use repertoire.
Story Source:
The above story is based on materials provided by Cell Press . Note: Materials may be edited for content and length.
Journal Reference:
Mangalam et al. Wild Bearded Capuchin Monkeys Crack Nuts Dexterously. Current Biology, 2015 DOI: 10.1016/j.cub.2015.03.035
Cite This Page:
</MainBody>
    </Article>
    <Article id="52">
        <date>Thu Apr 30 19:40:19 EEST 2015</date>
        <title>The regulating hand in ribosome formation</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/kacVzKY4Lus/150430124019.htm</Link>
        <Description>Biochemists have discovered a protein that regulates the hierarchical organization of ribosome development. Ribosomes are complexly structured cellular nanomachines consisting of four ribonucleic acids and approximately 80 different ribosomal proteins (r-proteins). They are responsible for synthesising protein chains.</Description>
        <MainBody>The regulating hand in ribosome formation
Date:
Heidelberg, Universitt
Summary:
Biochemists have discovered a protein that regulates the hierarchical organization of ribosome development. Ribosomes are complexly structured cellular nanomachines consisting of four ribonucleic acids and approximately 80 different ribosomal proteins (r-proteins). They are responsible for synthesising protein chains.
Share:
Model of assembly of the ribosomal protein L4 into the pre-ribosome.
Credit: Image courtesy of Heidelberg, Universitt
Model of assembly of the ribosomal protein L4 into the pre-ribosome.
Credit: Image courtesy of Heidelberg, Universitt
Close
Ribosomes, which use a fixed genetic programme to manufacture cell proteins, also form according to a strict hierarchical plan. In an interdisciplinary approach, the research teams of Prof. Dr. Ed Hurt of the Heidelberg University Biochemistry Center (BZH) and Prof. Dr. Andr Hoelz of the California Institute of Technology (Caltech) in Pasadena (USA) have decoded the mechanism that regulates this process. They discovered a previously unknown protein that regulates the processes in the cell nucleus that permit the cell to incorporate ribosomal proteins into the developing pre-ribosome in the correct order. The results of their research were published online in "Molecular Cell."
Ribosomes are complexly structured cellular nanomachines consisting of four ribonucleic acids and approximately 80 different ribosomal proteins (r-proteins). They are responsible for synthesising protein chains. "Correct ribosomal formation is of elementary importance in cell division and propagation. Their structure is highly complicated because all ribosomal proteins are added to the developing pre-ribosome in a strict sequence, with approximately 200 helper proteins assisting in the process," says Ed Hurt.
In eukaryotes, new ribosomes are formed primarily in the cell nucleus. The r-proteins needed for their formation must travel from the cell plasma to the site in the nucleus where the ribosomes are manufactured, called the nucleolus. Until now, scientists knew only that r-proteins were built into the newly forming ribosome following a strict hierarchy -- r-protein B comes after r-protein A and so on. "But the question of how the strict sequence is ensured and who is responsible remained largely unanswered," explains Prof. Hurt.
The researchers have now been able to demonstrate that the newly discovered protein, called the assembly chaperone of L4 or Acl4, regulates the orderly integration of ribosomal protein L4 into the early pre-ribosome. "This employs a well-known everyday concept, like an usher holding a seat open until the correct occupant arrives," explains the researcher.
Using new investigative procedures, the two primary authors of the publication, Dr. Philipp Stelter of the BZH and Ferdinand Huber of Caltech, were able to decode the detection mechanism between the L4 r-protein and the developing ribosome. According to the researchers, the underlying basis is a eukaryote-specific extension of the L4 ribosomal protein that comes into contact with the surface of the ribosome and is released for assembly by the Acl4 helper protein. If these interactions are hindered by insufficient production of the r-protein or an error in the growing ribosome, the helper protein remains bound and prevents the development of a faulty ribosome.
The collaboration between the researchers of the Heidelberg University Biochemistry Center and the California Institute of Technology offered an opportunity to combine traditional and newly developed methods in cellular biology, biochemistry and biophysics. "This was pivotal for the detailed characterisation of the newly discovered mechanisms and the participating components," emphasises Ed Hurt.
Story Source:
The above story is based on materials provided by Heidelberg, Universitt . Note: Materials may be edited for content and length.
Journal Reference:
P. Stelter, F. M. Huber, R. Kunze, D. Flemming, A. Hoelz, E. Hurt. Coordinated ribosomal L4 protein assembly into the pre-ribosome is regulated by its eukaryote-specific extension. Molecular Cell, April 2015 DOI: 10.1016/j.molcel.2015.03.029
Cite This Page:
</MainBody>
    </Article>
    <Article id="53">
        <date>Thu Apr 30 19:40:05 EEST 2015</date>
        <title>Light -- not pain-killing drugs -- used to activate brain's opioid receptors</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/OQ6c-9sEPBw/150430124005.htm</Link>
        <Description>Neuroscientists have attached the light-sensing protein rhodopsin to opioid receptor parts to activate the receptor pathways using light from a laser fiber-optic device. They also influenced the behavior of mice using light, rather than drugs, to activate the reward response. When an opioid receptor is exposed to a pain-killing drug, it initiates activity in specific chemical pathways in the brain and spinal cord. And when the researchers shone light on the receptors that contained rhodopsin, the same cellular pathways were activated. Neurons in that part of the brain release chemicals such as dopamine that create feelings of euphoria.</Description>
        <MainBody>Washington University in St. Louis
Summary:
Neuroscientists have attached the light-sensing protein rhodopsin to opioid receptor parts to activate the receptor pathways using light from a laser fiber-optic device. They also influenced the behavior of mice using light, rather than drugs, to activate the reward response. When an opioid receptor is exposed to a pain-killing drug, it initiates activity in specific chemical pathways in the brain and spinal cord. And when the researchers shone light on the receptors that contained rhodopsin, the same cellular pathways were activated. Neurons in that part of the brain release chemicals such as dopamine that create feelings of euphoria.
Share:
Total shares: 
FULL STORY
New research at Washington University in St. Louis shows that it's possible to activate opioid receptors with light instead of pain-killing drugs. The discovery eventually may lead to new ways to relieve severe pain without the addictive properties and side effects posed by opiates.
Credit: Robert Boston
New research at Washington University in St. Louis shows that it's possible to activate opioid receptors with light instead of pain-killing drugs. The discovery eventually may lead to new ways to relieve severe pain without the addictive properties and side effects posed by opiates.
Credit: Robert Boston
Close
Despite the abuse potential of opioid drugs, they have long been the best option for patients suffering from severe pain. The drugs interact with receptors on brain cells to tamp down the body's pain response. But now, neuroscientists at Washington University School of Medicine in St. Louis have found a way to activate opioid receptors with light.
In a test tube, the scientists melded the light-sensing protein rhodopsin to key parts of opioid receptors to activate receptor pathways using light. They also influenced the behavior of mice by injecting the receptors into the brain, using light instead of drugs to stimulate a reward response.
Their findings are published online April 30 in the journal Neuron.
The eventual hope is to develop ways to use light to relieve pain, a line of discovery that also could lead to better pain-killing drugs with fewer side effects.
"It's conceivable that with much more research we could develop ways to use light to relieve pain without a patient needing to take a pain-killing drug with side effects," said first author Edward R. Siuda, a graduate student in the laboratory of Michael R. Bruchas, PhD, an assistant professor of anesthesiology and of neurobiology.
But before that's possible, the researchers are attempting to learn the most effective ways to activate and deactivate the opioid receptor's pathways in brain cells. Bruchas, the study's principal investigator, explained that working with light rather than pain-killing drugs makes it much easier to understand how the receptors function within the complex array of cells and circuits in the brain and spinal cord.
"It's been difficult to determine exactly how opioid receptors work because they have multiple functions in the body," Bruchas explained. "These receptors interact with pain-killing drugs called opiates, but they also are involved in breathing, are found in the gastrointestinal tract and play a role in the reward response."
So the researchers sought a way to limit opioid receptors to performing a single task at a time, and it turned out to be almost as easy as flipping on a light switch, according to Bruchas, Siuda and their collaborators, including co-first author Bryan A. Copits, PhD, a postdoctoral research scholar in the laboratory of Robert W. Gereau, IV, PhD, the Dr. Seymour and Rose T. Brown Professor of Anesthesiology.
By combining the rhodopsin protein, which senses light in the eye's retina, with a specific type of opioid receptor called a Mu opioid receptor, the researchers were able to build a receptor that responds to light in exactly the same way that standard opioid receptors respond to pain-killing drugs.
When an opioid receptor is exposed to a pain-killing drug, it initiates activity in specific chemical pathways in the brain and spinal cord. And when the researchers shone light on the receptors that contained rhodopsin, the same cellular pathways were activated.
In a test tube and in cells, Siuda exposed the receptors to light and then watched as they released the same chemicals that standard opioid receptors release. Then, in mice, the researchers implanted a light-emitting diode (LED) device the size of a human hair into a brain region linked to the reward response. They injected the light-sensing receptors they had genetically manufactured into the same brain region. Neurons in that part of the brain release chemicals such as dopamine that create feelings of euphoria.
In decades of past opioid studies, researchers have observed mice and rats to press a lever to receive a dose of morphine, for example. The morphine would activate opioid receptors and the release of dopamine, and the animals would enjoy the response and press the lever again to continue feeling that reward sensation. This is one of the reasons opiates are so often abused in patients being treated for pain -- people like the way the drugs make them feel as much as the pain relief they provide -- and rates of abuse have skyrocketed over the past ten years.
Working to deliver a similar reward sensation using light, the researchers put the mice into an enclosed chamber. In one part of the chamber, the lighted laser fiber-optic device stimulated the release of dopamine in the brain. When the animals left that part of the chamber, the light in the brain turned off. Soon after, the mice returned to the part of the chamber that activated the fiber-optic device so that the brain could receive more light stimulation.
"By activating the receptors with light, we are presumably causing the brain to release more dopamine," Bruchas explained. "Rather than a drug such as morphine activating an opioid receptor, the light provides the reward."
The researchers were able to vary the animals' response depending on the amount and type of light emitted by the LED. Different colors of light, longer and shorter exposure to light, and whether the light pulsed or was constant all produced slightly different effects.
When a person takes an opioid drug such as Vicodin or OxyContin to relieve pain, such drugs interact with receptors in the brain to blunt pain sensations. But over time, patients develop tolerance and sometimes addiction. Opioids also can dramatically slow a person's breathing, too, and typically cause constipation.
In theory, receptors tuned to light may not present the same danger. Siuda said it someday may be possible to activate, or deactivate, nerve cells without affecting any of the other receptors that pain-killing drugs trigger, although achieving that goal will be difficult.
Bruchas' team is planning future studies that will use these receptors to test ways to control the brain cells that mediate pain and reward behavior with light rather than drugs.
The research was supported by a EUREKA award from the National Institute on Drug Abuse, the National Institute of Mental Health and the National Institute of General Medical Sciences of the National Institutes of Health (NIH); grant numbers R01 DA037152, F31 MH101956, K99 DA038725, TR32 GM108539 and NSTR01 NS081707. Additional funding from a W.M. Keck Fellowship in Molecular Medicine; and the Howard Hughes Medical Institute.
Siuda ER, Copits BA, Schmidt MJ, Baird MA, Al-Hasani R, Planer WJ, Funderburk SC, McCall JG, Gereau RW, Bruchas M. Spatiotemporal control of opioid signaling and behavior. Neuron, published online April 30, 2015.
Story Source:
The above story is based on materials provided by Washington University in St. Louis . The original article was written by Jim Dryden. Note: Materials may be edited for content and length.
Journal Reference:
Edward R. Siuda, Bryan A. Copits, Martin J. Schmidt, Madison A. Baird, Ream Al-Hasani, William J. Planer, Samuel C. Funderburk, Jordan G. Mccall, Robert W. Gereau Iv, Michael R. Bruchas. Spatiotemporal Control of Opioid Signaling and Behavior. Neuron, 2015 DOI: 10.1016/j.neuron.2015.03.066
Cite This Page:
</MainBody>
    </Article>
    <Article id="54">
        <date>Thu Apr 30 19:40:03 EEST 2015</date>
        <title>Vital step in stem cell growth revealed</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/05fgc-t3BR0/150430124003.htm</Link>
        <Description>Stem cells, which have the potential to turn into any kind of cell, offer the tantalizing possibility of generating new tissues for organ replacements, stroke victims and patients of many other diseases. Now, scientist have uncovered details about stem cell growth that could help improve regenerative therapies.</Description>
        <MainBody>Vital step in stem cell growth revealed
Date:
Salk Institute for Biological Studies
Summary:
Stem cells, which have the potential to turn into any kind of cell, offer the tantalizing possibility of generating new tissues for organ replacements, stroke victims and patients of many other diseases. Now, scientist have uncovered details about stem cell growth that could help improve regenerative therapies.
Share:
Total shares: 
FULL STORY
In early-stage cell nuclei (blue), developmental genes (green) must be turned on for the cell to develop. When the two cellular processes, Wnt and Activin, work together (upper left), genes are activated to a much greater degree than when neither process is active (upper right). When just a single pathway is active (Wnt, lower left, Activin, lower right), only a few genes are turned on.
Credit: Salk Institute
In early-stage cell nuclei (blue), developmental genes (green) must be turned on for the cell to develop. When the two cellular processes, Wnt and Activin, work together (upper left), genes are activated to a much greater degree than when neither process is active (upper right). When just a single pathway is active (Wnt, lower left, Activin, lower right), only a few genes are turned on.
Credit: Salk Institute
Close
Stem cells, which have the potential to turn into any kind of cell, offer the tantalizing possibility of generating new tissues for organ replacements, stroke victims and patients of many other diseases. Now, scientists at the Salk Institute have uncovered details about stem cell growth that could help improve regenerative therapies.
While it was known that two key cellular processes -- called Wnt and Activin -- were needed for stem cells to grow into specific mature cells, no one knew exactly how these pathways worked together. The details of how Wnt and Activin influence each other, published April 30, 2015 in Molecular Cell, offer guidance for improving stem cell therapies. The new work also reveals more about certain cancers that arise when these processes go astray, for example, when the Wnt signaling step becomes inappropriately reactivated, as happens in most colon cancers.
"We found that the mechanisms of these two pathways are complementary and activate the transcription, or turning on, of about 200 genes essential for stem cells to differentiate," says Kathy Jones, senior author of the paper and a Salk professor. These genes are among the first steps that prompt stem cells to begin to change, or differentiate, into specific tissues, particularly ones that will eventually form the digestive and respiratory tracts, including intestines, lung, pancreas, thyroid and liver.
The researchers found that Wnt loads up the cellular machinery needed to begin the copying and activation of genes. Activin, meanwhile, boosts the process further: it increases the speed and efficiency by which the cellular machinery moves to copy the gene. Whereas Wnt treatment alone enhances the expression of developmental genes by a factor of 20-fold, further treatment with Activin boosts the signal to 150-fold or higher, says Jones. The team also found that the order of the signaling is equally important, because Activin could not turn on these genes unless the cells were first exposed to the Wnt signal.
"Wnt gets the ball rolling and Activin amplifies the signal," says Conchi Estars, first author of the paper. "This is a particularly clear example of how two different pathways, working through two different mechanisms, can cooperate to activate the same genes." The new finding adds to a growing picture that the transcription process is much more dynamic than previous thought.
"Now we understand stem cell differentiation at a much finer level by seeing how these cellular signals transmit their effects in the cells," adds Jones. "Understanding these details is important for developing more robust stem cell protocols and optimizing the efficiency of stem cell therapies."
When they looked closer at the genes that both pathways activated, researchers were surprised to find that the pathways were further connected to a third process, which is known to control tissue growth and organ size. The central protein in this new pathway, called Yap, acted specifically at these genes to counteract the effects of the Activin.
"The opposing effects of Activin and Yap are exerted at a late step in transcription, the elongation phase," says Jones. We don't know very much about the signaling networks in normal or cancer cells specifically affect the elongation stage of transcription, so it was a real bonus to find that it is targeted by two pathways in stem cells."
Both the Wnt and Activin signaling processes operate differently in cancer, compared to stem cells. Wnt, in particular, is turned on very early in human colon cancer in nearly 90 percent of cases. The aberrant behavior of the Activin process, meanwhile, is tied to the metastasis of many cancers.
"There is great interest in developing transcription-based inhibitors of the Wnt pathway, because these would have strong anticancer activity for many tumor types," says Estars. "Because the environment of stem cells and cancer cells are quite distinct, and different target genes are involved, it will be interesting to see how the synergy and regulation that we have defined in stem cells operates in the cells of a tumor."
Story Source:
The above story is based on materials provided by Salk Institute for Biological Studies . Note: Materials may be edited for content and length.
Journal Reference:
Conchi Estars, Chris Benner, Katherine A. Jones. SMADs and YAP Compete to Control Elongation of -Catenin:LEF-1-Recruited RNAPII during hESC Differentiation. Molecular Cell, April 2015 DOI: 10.1016/j.molcel.2015.04.001
Cite This Page:
</MainBody>
    </Article>
    <Article id="55">
        <date>Thu Apr 30 19:40:00 EEST 2015</date>
        <title>New tool can switch behavior -- such as voracious eating -- 'on' and 'off'</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/vMwuA2k5h48/150430124000.htm</Link>
        <Description>Researchers have perfected a noninvasive "chemogenetic" technique that allows them to switch off a specific behavior in mice -- such as voracious eating -- and then switch it back on. The method works by targeting two different cell surface receptors.</Description>
        <MainBody>University of North Carolina School of Medicine
Summary:
Researchers have perfected a noninvasive "chemogenetic" technique that allows them to switch off a specific behavior in mice -- such as voracious eating -- and then switch it back on. The method works by targeting two different cell surface receptors.
Share:
Total shares: 
FULL STORY
Neuroscientists have perfected a chemical-genetic remote control for brain circuitry and behavior. This evolving technology can now sequentially switch the same neurons - and the behaviors they mediate - on-and-off in mice.
Credit: Image courtesy of University of North Carolina School of Medicine
Neuroscientists have perfected a chemical-genetic remote control for brain circuitry and behavior. This evolving technology can now sequentially switch the same neurons - and the behaviors they mediate - on-and-off in mice.
Credit: Image courtesy of University of North Carolina School of Medicine
Close
Researchers at the University of North Carolina School of Medicine and the National Institutes of Health (NIH) have perfected a noninvasive "chemogenetic" technique that allows them to switch off a specific behavior in mice -- such as voracious eating -- and then switch it back on. The method works by targeting two different cell surface receptors of neurons that are responsible for triggering the specific chemical signals that control brain function and complex behaviors.
When this complex signaling system goes awry, the results can lead to a plethora of diseases, including schizophrenia, depression, Alzheimer's Disease, Parkinson's Disease, eating disorders, and epilepsy. Cell surface receptors also play roles in cancers, diabetes, digestive conditions, and other diseases. This new technique could be modified to study them, as well.
This is the first technology to stem from the initial set of NIH BRAIN Initiative grants to create new cutting-edge research tools to improve our understanding of the brain.
"This new chemogenetic tool will show us how brain circuits can be more effectively targeted to treat human disease, " said Bryan L. Roth, MD, PhD, the Michael Hooker Distinguished Professor of Protein Therapeutics and Translational Proteomics at the UNC School of Medicine. "The problem facing medical science is that although most approved drugs target these brain receptors, it remains unclear how to selectively modulate specific kinds of receptors to effectively treat disease."
Roth addressed this problem by inventing a technology he dubbed "DREADDs" -- Designer Receptor Exclusively Activated by a Designer Drug.
The first-generation DREADD technology was developed in 2007.
Essentially, in lab experiments, Roth's team altered the chemical structure of G protein-coupled receptors so that the receptors expressed synthetic proteins when reintroduced into a mouse. This way, the mutated receptor could only be activated or inhibited by a specific synthesized drug-like compound. The receptor became like a lock; the synthetic drug became the only key that fit the lock. Depending on what Roth's team wanted to study, they could lock or unlock the specific brain circuits and behaviors associated with that one receptor.
This DREADD technology -- also known as chemogenetics -- is now used by hundreds of labs worldwide. It helped revolutionize our understanding of how brain circuits control normal and abnormal behavior, emotions, perception, pain sensation, memory, and many other processes. DREADDs have been used to improve the function of insulin-producing cells in mice as a way of treating diabetes. DREADD technology has also helped scientists treat epileptic seizures in mice.
But scientists could use this first DREADD to only manipulate a single receptor in one direction -- excite the receptor or inhibit it.
Last year, Roth and UNC colleagues Thomas Kash, PhD, and Jian Jin, PhD, received a $2.84-million NIH BRAIN Initiative grant to develop the next generation of DREADDs.
Today in the journal Neuron, UNC and NIH researchers revealed the first fruit of that grant -- a new chemogenetic technology they have named KORD (k-opioid receptor DREADD). This new tool, co-invented by Roth and Eyal Vardy, PhD, a former UNC postdoctoral fellow, can target two different kinds of receptors on the same neuron sequentially. This allowed them to study the function of two kinds of receptors as they relate to each other.
In the Neuron paper, Roth's team explain how they modified the receptors in the lab, packaged the receptors in an viral vector, and injected them into mice so that the synthetic receptors were expressed only in certain kinds of neurons in specific parts of the brain.
Then they administered the synthetic drug-like compound to demonstrate how neuronal signaling could be manipulated to turn the same neurons 'on' and 'off' and thereby turning 'on' and 'off' specific behaviors in mice.
In one type of experiment, the NIH lab of Michael Krashes, PhD, was able to turn 'on' and 'off' voracious feeding behavior in mice. In another type of experiment, UNC researchers were able to turn 'on' and 'off' behaviors similar to those induced by drugs such as cocaine and amphetamines.
Elliot Robinson, an MD/PhD student at UNC and co-first author of the Neuron paper, said, "These experiments have validated KORD as a new tool for researchers interested in controlling the function of specific populations of cells while also highlighting their therapeutic potential."
Reid Johnson, UNC graduate student and paper co-author, said, "Using genetically modified mice, we can now tease apart the interactions between seemingly disparate neuronal systems in a logical fashion."
Roth added, "We are now sharing KORD and other DREADD technology freely with other scientists, and it is likely that new uses for these technologies will appear in the near future."
Vardy, co-first author of the Neuron paper, is now a senior scientist at Merck Pharmaceuticals. Robinson conducted his experiments while in the lab of CJ Malanga, PhD, associate professor of neurology at the UNC School of Medicine and paper co-author. Johnson is part of the lab headed by Juan Song, assistant professor of pharmacology at UNC and paper co-author. Thomas Kash was also an author on this paper.
Story Source:
The above story is based on materials provided by University of North Carolina School of Medicine . Note: Materials may be edited for content and length.
Journal Reference:
Eyal Vardy, J. Elliott Robinson, Chia Li, Reid H.j. Olsen, Jeffrey F. Diberto, Patrick M. Giguere, Flori M. Sassano, Xi-Ping Huang, Hu Zhu, Daniel J. Urban, Kate L. White, Joseph E. Rittiner, Nicole A. Crowley, Kristen E. Pleil, Christopher M. Mazzone, Philip D. Mosier, Juan Song, Thomas L. Kash, C.j. Malanga, Michael J. Krashes, Bryan L. Roth. A New DREADD Facilitates the Multiplexed Chemogenetic Interrogation of Behavior. Neuron, 2015 DOI: 10.1016/j.neuron.2015.03.065
Cite This Page:
</MainBody>
    </Article>
    <Article id="56">
        <date>Thu Apr 30 19:39:30 EEST 2015</date>
        <title>Spinal cord axon injury location determines neuron's regenerative fate</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/xCACW1XApvY/150430123930.htm</Link>
        <Description>A previously unappreciated phenomenon has been reported in which the location of injury to a neuron's communication wire in the spinal cord -- the axon -- determines whether the neuron simply stabilizes or attempts to regenerate. The study demonstrates how advances in live-imaging techniques are revealing new insights into the body's ability to respond to spinal cord injuries.</Description>
        <MainBody>Spinal cord axon injury location determines neuron's regenerative fate
Date:
University of California, San Diego Health Sciences
Summary:
A previously unappreciated phenomenon has been reported in which the location of injury to a neuron's communication wire in the spinal cord -- the axon -- determines whether the neuron simply stabilizes or attempts to regenerate. The study demonstrates how advances in live-imaging techniques are revealing new insights into the body's ability to respond to spinal cord injuries.
Share:
Injury to a spinal cord axon after a major branch point.
Credit: Image courtesy of University of California, San Diego Health Sciences
Injury to a spinal cord axon after a major branch point.
Credit: Image courtesy of University of California, San Diego Health Sciences
Close
Researchers at University of California, San Diego School of Medicine report a previously unappreciated phenomenon in which the location of injury to a neuron's communication wire in the spinal cord -- the axon -- determines whether the neuron simply stabilizes or attempts to regenerate. The study, published April 30 by Neuron, demonstrates how advances in live-imaging techniques are revealing new insights into the body's ability to respond to spinal cord injuries.
While the body of a neuron is small, its axon can extend far up or down the spinal cord, which is about one and half feet long in humans. Along that distance, the axon branches out to make hundreds of connections with other cells, sending out signals that allow us to sense and respond to the world around us. Unless something happens to disrupt the axon's reach, that is. Adult human axons in the brain and spinal cord are very limited in their ability to regenerate after injury -- a hurdle that many researchers are trying to overcome in the treatment of spinal cord injuries and neurodegenerative diseases of the brain.
In this study, senior author Binhai Zheng, PhD, associate professor of neurosciences, first author Ariana O. Lorenzana, PhD, and colleagues used a sophisticated optical imaging technique that allows them to directly visualize the spinal cord in living mouse models. With this approach, the researchers were able to systematically examine the effects of axon injury location on degeneration and regeneration of the injured branch. The injury locations they compared were just before an axon's major branch point (where a single axon branches into two) and just after it. The injuries just after the branch point cut off one branch, leaving the other intact, or cut both branches.
The researchers found that injury to the main axon, before a branch point, resulted in regeneration in 89 percent of the cases. Axons with both branches cut after a branch point regenerated in 67 percent of cases. Regeneration occurred in the form of axon elongation, branching or both for at least five days after injury. In contrast, regeneration occurred only 12 percent of cases following cuts to just one of two axon branches after a major branch point. In this case, the injured branch trims itself all the way back to the base, preserving the function of the other, uninjured branch.
"What we think is happening is that if an axon is injured in such a way that it still has some kind of connection, is still transmitting signals, the neuron can justify stabilization, but not the energy it would take to either regenerate axon length or just kill the whole thing off," Zheng said. "On the other hand, once both branches of an axon are cut and there's no longer any connection or output, the neuron can justify the energy and resources to regenerate, even though that effort is largely futile in the central nervous system of an adult mammal."
This is a new, yet very fundamental, understanding of neuron behavior -- one that will be important to keep in mind as new therapeutic approaches are proposed for spinal cord injuries, the researchers say.
Story Source:
The above story is based on materials provided by University of California, San Diego Health Sciences . The original article was written by Heather Buschman. Note: Materials may be edited for content and length.
Journal Reference:
Binhai Zheng, PhD et al. A Surviving Intact Branch Stabilizes Remaining Axon Architecture after Injury as Revealed by In Vivo Imaging in the Mouse Spinal Cord. Neuron, April 2015 DOI: 10.1016/j.neuron.2015.03.061
Cite This Page:
</MainBody>
    </Article>
    <Article id="57">
        <date>Thu Apr 30 19:39:28 EEST 2015</date>
        <title>Protein 'brake' in metabolic reprogramming restrains senescent cells from becoming cancerous</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/soBU3SeONPY/150430123928.htm</Link>
        <Description>In recent years, research has shown that cancerous cells have a different metabolism -- essential chemical and nutritional changes needed for supporting the unlimited growth observed in cancer-- than normal cells. Now, scientists have identified a way that cells can reprogram their metabolism to overcome a tumor-suppressing mechanism known as senescence, solidifying the notion that altered metabolism is a hallmark of cancer progression.</Description>
        <MainBody>Protein 'brake' in metabolic reprogramming restrains senescent cells from becoming cancerous
Date:
Wistar Institute
Summary:
In recent years, research has shown that cancerous cells have a different metabolism -- essential chemical and nutritional changes needed for supporting the unlimited growth observed in cancer-- than normal cells. Now, scientists have identified a way that cells can reprogram their metabolism to overcome a tumor-suppressing mechanism known as senescence, solidifying the notion that altered metabolism is a hallmark of cancer progression.
Share:
Total shares: 
FULL STORY
Shown on the left is an ATM positive cell that has undergone senescence (depicted by blue color). On the right are cells with low ATM that are not senescent due to a cancer-like change in metabolism.
Credit: The Wistar Institute
Shown on the left is an ATM positive cell that has undergone senescence (depicted by blue color). On the right are cells with low ATM that are not senescent due to a cancer-like change in metabolism.
Credit: The Wistar Institute
Close
In recent years, research has shown that cancerous cells have a different metabolism -- essential chemical and nutritional changes needed for supporting the unlimited growth observed in cancer- than normal cells. Now, scientists at The Wistar Institute have identified a way that cells can reprogram their metabolism to overcome a tumor-suppressing mechanism known as senescence, solidifying the notion that altered metabolism is a hallmark of cancer progression.
The findings were published online by the journal Cell Reports.
Normal cells become senescent as an automatic way to halt growth in the presence of potential cancer-causing changes. In this study, researchers found that metabolic reprogramming allows for the proliferation of cells that should have become senescent, and these proliferative cells have the potential to lead to tumor formation. "Senescence is an important suppressor of tumorigenesis," said Katherine Aird, Ph.D., a postdoctoral fellow in the laboratory of Rugang Zhang, Ph.D. and lead author of the study. "When the cell finds a way to get around senescence, like the mechanism we described, there is the potential for cells to become cancerous."
At the center of these findings is a protein kinase called ataxia telangiectasia mutated, or ATM. ATM is mutated in approximately 1 in 50,000 people, and these patients have a 25% lifetime incidence of cancer. In this study, researchers found that when ATM is suppressed, it both inhibits p53 -- which is highly involved in tumor suppressive metabolism -- and increases the expression of the oncogene c-MYC, a gene known to play an important role in tumor-promoting metabolism. Essentially, the loss of ATM reprograms a cell's metabolism in such a way that it may promote cancer. This could explain why many cancers have lost ATM and provide possible additional insight into why people with ATM mutations have a higher than normal incidence of cancer.
"A better understanding of the basic regulatory processes that control cancer metabolism is critical for eventually targeting this process for the development of novel cancer therapeutics," said Rugang Zhang Ph.D., associate professor in Wistar's Gene Expression and Regulation Program and corresponding author of the study. "With this study, we have found that specific metabolic changes can overcome senescence. We may be able to exploit this for cancer therapy by reversing these senescence-overcoming metabolic changes."
"This is an extremely exciting and timely set of observations," said Dario C. Altieri, M.D., President and CEO of the Wistar Institute and director of The Wistar Institute Cancer Center. "The work that Katherine has just completed gives a completely novel perspective on how tumors can bypass a fundamental barrier that protects us against cancer. At the same time, the results point the way on how it may be possible to target the unique metabolism of tumor cells for novel therapies."
Story Source:
The above story is based on materials provided by Wistar Institute . Note: Materials may be edited for content and length.
Journal Reference:
Katherine M. Aird, Andrew J. Worth, Nathaniel W. Snyder, Joyce V. Lee, Sharanya Sivanand, Qin Liu, Ian A. Blair, Kathryn E. Wellen, Rugang Zhang. ATM Couples Replication Stress and Metabolic Reprogramming during Cellular Senescence. Cell Reports, April 2015 DOI: 10.1016/j.celrep.2015.04.014
Cite This Page:
</MainBody>
    </Article>
    <Article id="58">
        <date>Thu Apr 30 18:36:06 EEST 2015</date>
        <title>Observing the solar eclipse over the Arctic</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/GicWBknEvyw/150430113606.htm</Link>
        <Description>Scientists braved Arctic weather to successfully observe the total solar eclipse of March 20 from Longyearbyen on the island of Spitsbergen in the Svalbard archipelago east of northern Greenland.</Description>
        <MainBody>Observing the solar eclipse over the Arctic
Date:
University of Hawaii at Manoa
Summary:
Scientists braved Arctic weather to successfully observe the total solar eclipse of March 20 from Longyearbyen on the island of Spitsbergen in the Svalbard archipelago east of northern Greenland.
Share:
Corona from Svalbard. Image composed of 29 eclipse images.
Credit: Miloslav Druckmller, Shadia Habbal, Peter Aniol, and Pavel Starha. University of Hawaii at Manoa
Corona from Svalbard. Image composed of 29 eclipse images.
Credit: Miloslav Druckmller, Shadia Habbal, Peter Aniol, and Pavel Starha. University of Hawaii at Manoa
Close
The international Solar Wind Sherpas team, led by Dr. Shadia Habbal of the University of Hawaii at Manoa Institute for Astronomy, braved Arctic weather to successfully observe the total solar eclipse of March 20 from Longyearbyen on the island of Spitsbergen in the Svalbard archipelago east of northern Greenland. Their preliminary results are being presented Thursday at the Triennial Earth-Sun Summit in Indianapolis, IN.
It was no easy feat. Ever-changing weather predictions, subfreezing temperatures of -4 degrees F (-20 C) and the danger from polar bears were some of the challenges the team faced, but their years of preparation paid off. The sky over the snow-covered landscape was crystal clear before, during and after totality, so they were able to capture a beautiful solar corona.
Because the Svalbard archipelago, like the Hawaiian Islands, has microclimates, the team observed at two locations to increase its chances of seeing the eclipse. With local support, the team was able to set up its equipment inside the old Northern Light Observatory and observe the event through specially designed doors that replaced the old windows, and to use an airport hangar located 10 miles away.
Identical sets of imaging instruments were set up at both locations, with six digital SLR cameras fitted with different focal length lenses, and four astrophotography cameras with special filters to observe the colors of light given off by ionized iron atoms, stripped of 10 and 13 electrons. These highly ionized atoms probe the high temperature outer layers, or corona, of the sun. In addition, a special instrument, called a dual-channel imaging spectrograph was used at the observatory to measure the motions of these ions in the sun's corona. At the airport, Dr. Haosheng Lin (IfA) used a spectropolarimeter that he designed and constructed to measure the sun's magnetic fields.
The shadow bands, thin bands of light and dark observed prior to and during totality, were remarkable as the snow-covered landscape offered ideal conditions for seeing them. The corona of the eclipsed sun, which was at an altitude of 12 degrees, was shimmering throughout the 2 minutes and 20 seconds of totality, with one large prominence clearly visible to the naked eye.
To further maximize the likelihood of observing the corona during this eclipse, the other members of the Solar Wind Sherpas team observed from three other sites: the Faroe Islands, located between Iceland and Norway; a Falcon Dassault flying at 49,000 feet (15,000 m) over the Faroe Islands, and an Irish Marine Corps DC-3 flying out of Dublin. All were successful except for the group on the Faroe Islands, where rain prevented them from observing totality.
Members of the team are currently working on the calibration and analysis of the data and will be presenting preliminary results at the upcoming Triennial Earth-Sun Summit (TESS) meeting in Indianapolis. Their results will be published once the analysis is completed.
The 2015 members of the Solar Wind Sherpas and their respective institutions and corporations are Shadia Habbal, Haosheng Lin and Garry Nitta, Institute for Astronomy, University of Hawaii at Manoa; Adalbert Ding, Technische Universitat and Institute for Technical Physics, Berlin; Judd Johnson, Electricon, Boulder, Colorado; Miloslav Druckmller, Pavel Starha, Jana Hoderova and Jan Malec, Brno University of Technology, Czech Republic; Petr Starha, Brno, Czech Republic; Peter Aniol, Astelco Corporation, Germany; Martin Dietzel, Zeiss Corporation, Germany; Feras Habbal, University of Texas, Austin; Yaseen Almleaky, King Abdullah University, Saudi Arabia; Huw Morgan, Duraid Al-Shakarshi, Nathalia Alzate and Joe Hutton, Aberystwyth University, Wales; Martina Arndt, Bridgewater State University, Massachusetts; and Scott Gregoire Granite Mountain Research, Boulder. In addition, Peter Gallagher of Trinity College Dublin collaborated on the flight of the DC-3. The Northern Light Observatory was made available by the director of University Centre in Svalbard, Prof. Fred Sigernes, and Dr. Sebastian Sikora. The airport hangar was made available by airport manager Morten Ulsnes.
The dual-channel imaging spectrograph was designed and constructed by Prof. Adalbert Ding (Technische Universitat and Institute for Technical Physics, Berlin).
Story Source:
The above story is based on materials provided by University of Hawaii at Manoa . Note: Materials may be edited for content and length.
Cite This Page:
</MainBody>
    </Article>
    <Article id="59">
        <date>Thu Apr 30 18:35:40 EEST 2015</date>
        <title>Swine farming a risk factor for drug-resistant staph infections, study finds</title>
        <Link>http://feeds.sciencedaily.com/~r/sciencedaily/~3/Z7WUYSnMlyo/150430113540.htm</Link>
        <Description>Swine farmers are six times more likely to be carriers of staph bacteria, including the MRSA strain, than others, new research shows. S. aureus is a type of bacteria commonly found on the skin as well as in the noses and throats of people and animals. About 30 percent of the U.S. population carries these bacteria, which can cause a range of skin and soft tissue infections. Although most infections are minor, S. aureus can sometimes cause serious infections.</Description>
        <MainBody>University of Iowa
Summary:
Swine farmers are six times more likely to be carriers of staph bacteria, including the MRSA strain, than others, new research shows. S. aureus is a type of bacteria commonly found on the skin as well as in the noses and throats of people and animals. About 30 percent of the U.S. population carries these bacteria, which can cause a range of skin and soft tissue infections. Although most infections are minor, S. aureus can sometimes cause serious infections.
Share:
Total shares: 
FULL STORY
A new Univ. of Iowa study reports swine farmers are six times more likely to have staph bacteria than others.
Credit: Lynn Betts, USDA Natural Resources Conservation Service
A new Univ. of Iowa study reports swine farmers are six times more likely to have staph bacteria than others.
Credit: Lynn Betts, USDA Natural Resources Conservation Service
Close
Swine farmers are more likely to carry multidrug-resistant Staphylococcus aureus (S. aureus or "staph") than people without current swine exposure, according to a study conducted by a team of researchers from the University of Iowa, Kent State University, and the National Cancer Institute.
The study, published online in the journal Clinical Infectious Diseases, is the largest prospective examination of S. aureus infection in a group of livestock workers worldwide, and the first such study in the United States.
S. aureus is a type of bacteria commonly found on the skin as well as in the noses and throats of people and animals. About 30 percent of the U.S. population carries these bacteria, which can cause a range of skin and soft tissue infections. Although most infections are minor, S. aureus can sometimes cause serious infections.
Increasingly, drug-resistant strains of S. aureus are emerging, including methicillin-resistant (MRSA), tetracycline-resistant (TRSA), or multidrug-resistant (MDRSA) strains. And while previous studies have shown that certain strains of S. aureus are often associated with swine, cattle, and poultry exposure, little is known about livestock-associated staph carriage and infection in the United States.
The study authors note the research helps keep farmers safe by raising awareness about a potential health issue in swine operations. S. aureus does not present an economic concern for swine farmers since pigs generally are unaffected by staph infections.
"S. aureus does not typically make pigs sick, but they can act as carriers and transmit the bacterium to farmers," says Tara Smith, corresponding author on the study. "While carriage of S. aureus isn't itself harmful, individuals who harbor the bacterium in their nose, throat, or on their skin are at risk of developing an active staph infection, and they can also pass the bacterium to other family or community members. Individuals who may be immunocompromised, or have existing conditions such as diabetes, are especially at risk from staph infections."
For the study, the researchers followed a group of 1,342 Iowans, including individuals with livestock contact and a community-based comparison group, for 17 months. The participants were recruited from 53 of Iowa's 99 counties and lived in rural areas or small towns. Nose and throat swabs were collected from participants at the beginning of the study to determine if they carried S. aureus. Participants who experienced skin infections during the study period also were assessed for S. aureus.
Overall, 26 percent of the participants carried S. aureus. However, the investigators found that farmers with livestock exposure, particularly swine exposure, were more likely to carry MDRSA, TRSA, and livestock-associated S. aureus than those who weren't exposed to livestock.
"Current swine workers were six times more likely to carry multidrug-resistant S. aureus than those study participants without current swine exposure," says Smith. The study is based on research that Smith, currently an associate professor at Kent State University, conducted while she was a faculty member at the UI College of Public Health.
"Swine workers are also at risk of becoming infected with these organisms," Smith adds. "One hundred and three potential S. aureus infections were reported, and included infections with livestock-associated strains of this bacterium."
There currently is no method to prevent or eliminate carriage of S. aureus in animals or their human caretakers, meaning constant re-exposure and possibly transmission can occur between livestock and farm workers. Those workers can then pass staph to their family or community members.
"Iowa ranks third nationally in overall livestock production and first in swine production," notes Smith. "Transmission of staph between pigs and farmers and into the broader community could complicate efforts to control S. aureus transmission statewide, and have effects nationally due to the travel of pigs and people carrying these bacteria."
Story Source:
The above story is based on materials provided by University of Iowa . The original article was written by Debra Venzke. Note: Materials may be edited for content and length.
Journal Reference:
Shylo E. Wardyn, Brett M. Forshey, Sarah A. Farina, Ashley E. Kates, Rajeshwari Nair, Megan K. Quick, James Y. Wu, Blake M. Hanson, Sean M. O'malley, Hannah W. Shows, Ellen M. Heywood, Laura E. Beanefreeman, Charles F. Lynch, Margaret Carrel, and Tara C. Smith. Swine Farming Is a Risk Factor for Infection With and High Prevalence of Carriage of Multidrug-Resistant Staphylococcus aureus. Clinical Infectious Diseases, April 2015 DOI: 10.1093/cid/civ234
Cite This Page:
</MainBody>
    </Article>
</site>
